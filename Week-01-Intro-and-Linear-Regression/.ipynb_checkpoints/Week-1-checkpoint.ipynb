{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 1 (Aug 23)\n",
    "\n",
    "See the PDF notes in Canvas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 2 (Aug 25)\n",
    "\n",
    "## Data Matrix\n",
    "\n",
    "In this section, we will cover how data is frequently stored such that it can be used by machine learning methods. It should not be thought that this is the only way or always the best way to store data, but it will be a series of conventions used in many fields.\n",
    "\n",
    "For many problems, we will have a **dataset** consisting of some number $n$ points in $\\mathbb{R}^d$ that we will store in a matrix\n",
    "\n",
    "$$\n",
    "X = \\begin{pmatrix}\n",
    "x_{11} & x_{12} & \\cdots & x_{1d}\\\\\n",
    "x_{21} & x_{22} & \\cdots & x_{2d}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "x_{n1} & x_{n2} & \\cdots & x_{nd}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "The $i$th *row* $x_i = (x_{i1}, x_{i2}, ..., x_{id})\\in\\mathbb{R}^d$ is the $i$th point in the dataset. These points have many names in different fields: **points, datapoints, examples, vectors, records, feature-vectors**. In some sources, these points $x_i$ are denoted $\\mathbf{x}_i$, but it should be clear that $x$ with a single subscripts indicates a point while $x$ with two subscripts is the component of a point.\n",
    "\n",
    "The $j$th *column* $X_j=(x_{1j}, x_{2j}, ..., x_{nj})\\in\\mathbb{R}^n$ is the $j$th component of each point in the dataset. These are likewise called by many names dependent on field: **features, attributes, variables, dimensions, properties, fields**. In some cases, each column can be considered a random sample of a random variable, or the rows of the matrix $X$ can be considered a random sample of vector-valued random variables.\n",
    "\n",
    "The number of points $n$ is the **size** of the dataset and the length of the points $d$ is called the **dimensionality** of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Problems\n",
    "\n",
    "**Regression problems** are problems where we try to predict a numerical output value given some input datapoint based on some examples ($x_i$, $y_i$) of input datapoints with known outputs or **targets**. This will be the task of the machine learning problem.\n",
    "\n",
    "### Examples\n",
    "\n",
    "1. If we have a dataset of workers in a certain position of numbers of years of experience as an input and salary, we might want to predict salaries we don't know based on the years of experience a person has.\n",
    "\n",
    "1. If we have a dataset of variables about houses (floorspace, number of bedrooms, number of bathrooms, number of stories, age of the house) along with their selling prices, we might want to predict selling prices of homes not in the dataset based on the other variables about the house.\n",
    "\n",
    "1. If we have a dataset of variables about countries (average salary, average education of citizens, death rate, birth rate, infant mortality rate, etc.) along with their GDP, we might want to predict the GDP of countires not in the dataset based on the other variables about the country.\n",
    "\n",
    "1. If we have a dataset of seasonal variables about NBA basketball teams (points per game, turnovers per game, point differential, rebounds per game, blocks per game, etc.) and the numbers of wins they had in different seasons, we might want to take the statistics of a team early in a season to try to predict the number of wins they will have and average number of points per game.\n",
    "\n",
    "### Types of Regression Problems\n",
    "\n",
    "There are different types of regression problems based on the numbers of input variables and output variables.\n",
    "\n",
    "* A **simple** regression problem predicts an output variable with just one input variable like Example 1.\n",
    "\n",
    "* A **multiple** regression problem predicts an output variable with more than one input variable like Examples 2 and 3.\n",
    "\n",
    "* A **multivariate** regression problem predicts more than one output variables like Example 4.\n",
    "\n",
    "### The Math of a Regression Problem\n",
    "\n",
    "All of these regression problems have some things in common: there are example datapoints with outputs and we want to predict the outputs for new datapoints. Consider a $d$-dimensional point, or vector, $x_i\\in\\mathbb{R}^d$ and denote $x_i=(x_{i1},x_{i2},...,x_{id})$. $x_i$ maps to an output $y_i\\in\\mathbb{R}^m$. (In statistics, the components of the vector $x_1$ are more frequently called **predictors** or **independent variables** and the $y_i$ values are more frequently called **responses** or **dependent variables**.)\n",
    "\n",
    "In a perfect world, a solution to a (univariate) regression problem will find a function $f:\\mathbb{R}^d\\to\\mathbb{R}$ that can do two things:\n",
    "\n",
    "1. Mapping each example $x_i$ in a dataset to its output $y_i=f(x_i)$\n",
    "1. Generalize to successfully predict outputs of new datapoints\n",
    "\n",
    "In reality, $f$ will not always map each input $x_i$ values to each $y_i$ value perfectly or generalize to new inputs well, but we try to get as close to these ideals as possible.\n",
    "\n",
    "### Regression Algorithms\n",
    "\n",
    "There are a number of popular approaches to regression problems, including the following.\n",
    "\n",
    "* linear regression\n",
    "* lasso regression\n",
    "* ridge regression\n",
    "* decision trees\n",
    "* support vector machines\n",
    "* neural networks\n",
    "\n",
    "In the near term, we will consider the first three approaches as they are quite similar. As an added bonus, they all use a numerical optimization scheme called gradient descent, which is one of the main algorithms of machine learning.\n",
    "\n",
    "## Linear Regression\n",
    "\n",
    "We will assume the function $f$ we aim to predict is linear in some parameters $w_0,...,w_d$ so that our predicted function will be\n",
    "\n",
    "$$\n",
    "f(x_i)=w_0 + \\sum\\limits_{k=1}^d w_k x_{ik}=w_0+w_1 x_{i1}+\\cdots+w_d x_{id}\n",
    "$$\n",
    "\n",
    "for some parameters $w_0,...,w_d$. Here, we assume the output $Y$ is a random variable of the form\n",
    "\n",
    "$$Y = f(X_1, ..., X_d) + \\varepsilon$$\n",
    "\n",
    "where $\\varepsilon$ is a random error term that is assumed to account for the effect of unknown latent variables and intrinsic randomness of $y$\n",
    "\n",
    "We will aim to choose the values of $w_0,...,w_d$ that will minimize a loss function on a training dataset $(x_1,y_1),...,(x_n,y_n)$. This loss function will be small if each $f(x_i)$ is near each $y_i$, which is what we want.\n",
    "\n",
    "Note that this model may be represented as a neural network with one node, $d$ inputs, and 1 output (see more details in the lecture notes).\n",
    "\n",
    "### Note\n",
    "\n",
    "It is a common misconception that \"linear regression\" must fit linear functions to data, but the \"linear\" part of linear regression refers to the fact that $f$ is linear with respect to $w_0,...,w_d$, not with respect to $x_i$, so it is certainly possible to apply some preprocessing to the datapoints, which in effect, fits a nonlinear surface.\n",
    "\n",
    "For example, if each $x_i\\in\\mathbb{R}^1$, we can fit a parabola by manipulating each $x_i$ into $x_i^*=(x_i,x_i^2)$ so that our predicted function would be\n",
    "\n",
    "$$f(x_i)=w_0+w_1 x_i+w_2 x_i^2$$\n",
    "\n",
    "While we will discuss only points in the form $x_i=(x_{i1},...,x_{id})$ for now, keep in mind that we can always create new variables from the data, preprocess the data into different forms, and so on, to consider some (kernel) function of the data $g(x_i)$ as the inputs, as long as the function is differentiable (or at least piecewise differentiable). The main point here is that linear regression can learn to represent functions far beyond simply lines and planes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Least Squares\n",
    "\n",
    "Recall that we have a labeled dataset $(x_1,y_1)$, ..., $(x_n,y_n)$ with each $x_i\\in\\mathbb{R}^d$ and each $y_i\\in\\mathbb{R}$ and our goal is to find a function $\\hat{f}$ that maps the $x_i$'s to the $y_i$'s as well as possible, and, we hope, is effective at mapping unknown datapoints to appropriate outputs.\n",
    "\n",
    "In the **linear least squares** method, we try to minimize a the **sum of squared differences** between the real outputs $y_1, ..., y_n$ and the predicted outputs $\\hat{f}(x_1)$, ..., $\\hat{f}(x_n)$. In other words, the loss function in this method is\n",
    "\n",
    "$$L(w)=\\sum\\limits_{i=1}^n \\left(\\hat{f}(x_i)-y_i\\right)^2 = \\sum\\limits_{i=1}^n \\left(x_i^Tw-y_i\\right)^2,$$\n",
    "\n",
    "which we will call a **loss function**, where\n",
    "\n",
    "$$\n",
    "X=\\begin{pmatrix}\n",
    "1 & x_{11} & x_{12} & \\cdots & x_{1d}\\\\\n",
    "1 & x_{21} & x_{22} & \\cdots & x_{2d}\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "1 & x_{n1} & x_{n2} & \\cdots & x_{nd}\n",
    "\\end{pmatrix}\n",
    "\\hspace{2cm}y=\\begin{pmatrix}\n",
    "y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n\n",
    "\\end{pmatrix}\n",
    "\\hspace{2cm}w=\\begin{pmatrix}\n",
    "w_0 \\\\ w_1 \\\\ \\vdots \\\\ w_d\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "This matrix is the data matrix, except we added a column of ones to the left for convenience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Norms and Distance Metrics\n",
    "\n",
    "Let $d(x_1,x_2)=\\|x_1-x_2\\|$ be the distance from $x_1$ to $x_2$. The specific formula we use is called the **distance metric** and the function $\\|\\cdot\\|:\\mathbb{R}^d\\to[0,\\infty)$ is called a **norm**. The most-used norms are from the family of $L^p$ distances for each number $p>0$. (Note that mathematicians would likely refer to this as simply the $p$-norm for a finite dimensional space.) Distances between points are computed with the $L^p$-norm as\n",
    "\n",
    "$$\\left\\|x_1-x_2\\right\\|_p =\\left(\\sum\\limits_{i=1}^d |x_{1i}-x_{2i}|^p\\right)^{1/p}$$\n",
    "\n",
    "* If $p=1$, we get what is called the **taxi-cab** or **Manhattan distance** because, geometrically, the distance from (4, 6) to (8, 15) is the distance to drive along city blocks from 4th street and 6th avenue to 8th street and 15th avenue. It is computed as\n",
    "\n",
    "$$\\left\\|x_1-x_2\\right\\|_1= \\left|x_{11}-x_{21}\\right|+\\cdots+\\left|x_{1d}-x_{2d}\\right|$$\n",
    "\n",
    "* If $p=2$, we get the familiar **Euclidean distance**, the straight-line distance in flat space you have certainly encountered in elementary algebra and calculus courses,\n",
    "\n",
    "$$\n",
    "\\left\\|x_1-x_2\\right\\|_2=\\sqrt{\\left(x_{11}-x_{21}\\right)^2+\\cdots+\\left(x_{1d}-x_{2d}\\right)^2}$$\n",
    "\n",
    "* If $p=\\infty$, we get what is called the **supremum** or **maximum distance** and it is calculated as\n",
    "\n",
    "$$\\left\\|x_1-x_2\\right\\|_\\infty= \\max\\limits_{i=1,...,d}\\left|x_{1i}-x_{2i}\\right|=\\max\\{|x_{11}-x_{21}|, ..., |x_{1d}-x_{2d}|\\}$$\n",
    "\n",
    "For now, let's we will use the Euclidean distance, but in other areas, different $L^p$ norms may be used as may other <a href=\"https://en.wikipedia.org/wiki/Norm_(mathematics)#Definition\">norms</a> (see 2.5 in Goodfellow et. al.).\n",
    "\n",
    "## Back to Linear Least Squares\n",
    "\n",
    "Given this, we can rewrite the loss function as\n",
    "\n",
    "$$L(w)=\\|Xw-y\\|^2_2=(Xw-y)^T(Xw-y),$$\n",
    "\n",
    "where the $T$ subscript represents the **transpose** of a matrix. Here, $X$ and $y$ are constants derived from the dataset, so the $w$ values are the only unknowns, so we need to find the $w$ values that minimize the loss function $L$. In other words, we need to solve a minimization problem\n",
    "\n",
    "$$\\min\\limits_w\\,L(w)=\\min\\limits_w\\,(Xw - y)^T(Xw - y).$$\n",
    "\n",
    "We should note that these $w$ values are called **weights** or **parameters** of the model, which are values that are estimated by the model automatically from the data.\n",
    "\n",
    "In multivariate calculus, the approach to minimize a differentiable function with unbounded domain is to take derivatives with respect to $w_0, ..., w_d$, set them all equal to zero, solve, and compare the function at these critical values.\n",
    "\n",
    "While this can be done simply for this least squares problem, it is infeasible for most optimization problems we will see with neural networks, so we will use a gradient descent approach to approximate the solution instead, but more on this next week!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
