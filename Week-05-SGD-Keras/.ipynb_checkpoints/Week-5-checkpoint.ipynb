{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee069992-255e-4480-9980-0dbf9c811e11",
   "metadata": {},
   "source": [
    "# Week 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34295868-2395-416d-a812-fa2b4afc4a3b",
   "metadata": {},
   "source": [
    "# Lecture 9 - Speeding Up Learning (SGD, cross-entropy)\n",
    "\n",
    "We already know what gradient descent is: we run our model on training data, compute the loss function, find the gradient of the loss function, and update our parameters in the opposite direction of the gradient by a small amount; and repeat this until the model converges. The same general approach is used in nearly all neural networks.\n",
    "\n",
    "In linear regression and logistic classification, we were computing the outputs and loss function for the entire dataset (usually multiple times) per iteration of gradient descent, making a weight update based on the gradient, and repeating.\n",
    "\n",
    "The typical approach used in modern neural networks is **stochastic** gradient descent (SGD). SGD updates weights after processing a random sample, called a **mini-batch**, of datapoints: enough points to reduce the variance of using just one for more stable convergence of parameters but not so much that the computation is too expensive. In this way, we process a mini-batch for outputs, compute an approximate loss function and approximate gradients, update weights, and repeat.\n",
    "\n",
    "## How large should our mini-batches be?\n",
    "\n",
    "Typically, using $2^n$ for something like $n=5, 6, 7, 8$ because these values tend to be ideal for the linear algebra optimization libraries we use (NumPy, TensorFlow, etc). The mini-batch size is a hyperparameter, but it generally isn't one you need to tweak too much.\n",
    "\n",
    "One exception is if you are using GPUs for computing: then, it's typically best to choose the largest power of 2 that allows a whole mini-batch to fit into GPU memory, which will result in the underlying linear algebra libraries working optimally.\n",
    "\n",
    "## Implementing SGD\n",
    "\n",
    "Once we implement SGD with backpropagation, we will have constructed a \"vanilla\" neural network, which is probably the simplest neural network that is of practical use.\n",
    "\n",
    "First, let's import some libraries we will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67f3912a-92c2-4c20-a643-a7b95b236f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load some libraries\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import datasets\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2ef614-4193-42c3-ad5c-b41df2074aad",
   "metadata": {},
   "source": [
    "Next, we will write a class similar to the `FeedforwardNeuralNetwork` class we wrote previously but upgraded to use SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4e9311c-af50-4342-a4af-655793709742",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedforwardNeuralNetworkSGD:\n",
    "    \n",
    "    # input a vector [a, b, c, ...] with the number of nodes in each layer\n",
    "    def __init__(self, layers, alpha = 0.1, batchSize = 32):\n",
    "        # list of weight matrices between layers\n",
    "        self.W = []\n",
    "        \n",
    "        # network architecture will be a vector of numbers of nodes for each layer\n",
    "        self.layers = layers\n",
    "        \n",
    "        # learning rate\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        # batch size\n",
    "        self.batchSize = batchSize\n",
    "        \n",
    "        # initialize the weights (randomly) -- this is our initial guess for gradient descent\n",
    "        \n",
    "        # initialize the weights between layers (up to the next-to-last one) as normal random variables\n",
    "        for i in np.arange(0, len(layers) - 2):\n",
    "            self.W.append(np.random.randn(layers[i] + 1, layers[i + 1] + 1))\n",
    "            \n",
    "        # initialize weights between the last two layers (we don't want bias for the last one)\n",
    "        self.W.append(np.random.randn(layers[-2] + 1, layers[-1]))\n",
    "        \n",
    "    # define the sigmoid activation\n",
    "    def sigmoid(self, x):\n",
    "        return 1.0 / (1 + np.exp(-x))\n",
    "    \n",
    "    # define the sigmoid derivative (where z is the output of a sigmoid)\n",
    "    def sigmoidDerivative(self, z):\n",
    "        return z * (1 - z)\n",
    "    \n",
    "    # get a new mini-batch of data from the dataset\n",
    "    def getNextBatch(self, X, y, batchSize):\n",
    "        for i in np.arange(0, X.shape[0], batchSize):\n",
    "            # yield returns a generator, which can continue where it left off on later calls\n",
    "            # of the function\n",
    "            yield (X[i:i + batchSize], y[i:i + batchSize])\n",
    "    \n",
    "    # fit the model\n",
    "    def fit(self, X, y, epochs = 10000, update = 1000):\n",
    "        # add a column of ones to the end of X\n",
    "        X = np.hstack((X, np.ones([X.shape[0],1])))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            # randomize the examples\n",
    "            p = np.arange(0, X.shape[0])\n",
    "            np.random.shuffle(p)\n",
    "            X = X[p]\n",
    "            y = y[p]\n",
    "\n",
    "            # feed forward, backprop, and weight update\n",
    "            for (x, target) in self.getNextBatch(X, y, self.batchSize):\n",
    "                \n",
    "                # make a list of output activations from the first layer\n",
    "                # (just the original x values)\n",
    "                A = [np.atleast_2d(x)]\n",
    "                \n",
    "                # feed forward\n",
    "                for layer in range(len(self.W)):\n",
    "                    \n",
    "                    # feed through one layer and apply sigmoid activation\n",
    "                    net = A[layer].dot(self.W[layer])\n",
    "                    out = self.sigmoid(net)\n",
    "                    \n",
    "                    # add our network output to the list of activations\n",
    "                    A.append(out)\n",
    "                    \n",
    "                # backpropagation (coming soon!)\n",
    "                error = A[-1] - target\n",
    "                \n",
    "                D = [error * self.sigmoidDerivative(A[-1])]\n",
    "                \n",
    "                # loop backwards over the layers to build up deltas\n",
    "                for layer in np.arange(len(A) - 2, 0, -1):\n",
    "                    delta = D[-1].dot(self.W[layer].T)\n",
    "                    delta = delta * self.sigmoidDerivative(A[layer])\n",
    "                    D.append(delta)\n",
    "                    \n",
    "                # reverse the deltas since we looped in reverse\n",
    "                D = D[::-1]\n",
    "                \n",
    "                # weight update in each layer\n",
    "                for layer in range(len(self.W)):\n",
    "                    self.W[layer] -= self.alpha * A[layer].T.dot(D[layer])\n",
    "                    \n",
    "            # print an \n",
    "            if (epoch + 1) % update == 0:\n",
    "                loss = self.computeLoss(X,y)\n",
    "                print('Epoch =', epoch + 1, '\\t loss =', loss)\n",
    "                \n",
    "    def predict(self, X, addOnes = True):\n",
    "        # initialize data, be sure it's the right dimension\n",
    "        p = np.atleast_2d(X)\n",
    "        \n",
    "        # add a column of 1s for bias\n",
    "        if addOnes:\n",
    "            p = np.hstack((p, np.ones([X.shape[0],1])))\n",
    "        \n",
    "        # feed forward!\n",
    "        for layer in np.arange(0, len(self.W)):\n",
    "            p = self.sigmoid(np.dot(p, self.W[layer]))\n",
    "            \n",
    "        return p\n",
    "    \n",
    "    def computeLoss(self, X, y):\n",
    "        # initialize data, be sure it's the right dimension\n",
    "        y = np.atleast_2d(y)\n",
    "        \n",
    "        # feed the datapoints through the network to get predicted outputs\n",
    "        predictions = self.predict(X, addOnes = False)\n",
    "        loss = np.sum((predictions - y)**2) / 2.0\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9292d6e2-4c7e-45ef-8f30-329a23d98c87",
   "metadata": {},
   "source": [
    "## Example: MNIST\n",
    "\n",
    "As promised, this SGD neural net should run faster, so let's try to use the full 60,000 training images available in MNIST and 10,000 test images. (This is still a LOT of computation, using 70000 total 28-by-28 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "763deb18-9667-41f9-b108-106c27c68eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 1 \t loss = 4181.568523748957\n",
      "Epoch = 2 \t loss = 3577.5316798156996\n",
      "Epoch = 3 \t loss = 3087.4106358539325\n",
      "Epoch = 4 \t loss = 2831.066907637149\n",
      "Epoch = 5 \t loss = 2811.1728792574854\n",
      "Epoch = 6 \t loss = 3284.17154733417\n",
      "Epoch = 7 \t loss = 2569.969146241502\n",
      "Epoch = 8 \t loss = 2317.841675484064\n",
      "Epoch = 9 \t loss = 2322.351701446411\n",
      "Epoch = 10 \t loss = 2212.8389394980545\n",
      "Epoch = 11 \t loss = 2144.6290770266423\n",
      "Epoch = 12 \t loss = 2231.341620738748\n",
      "Epoch = 13 \t loss = 2175.7692359996713\n",
      "Epoch = 14 \t loss = 2160.6623280922627\n",
      "Epoch = 15 \t loss = 2224.4949355867166\n",
      "Epoch = 16 \t loss = 2050.887116062982\n",
      "Epoch = 17 \t loss = 1967.1981252693054\n",
      "Epoch = 18 \t loss = 1928.7549269655324\n",
      "Epoch = 19 \t loss = 2227.9452491800503\n",
      "Epoch = 20 \t loss = 1999.053300507481\n",
      "Epoch = 21 \t loss = 2129.1787303961733\n",
      "Epoch = 22 \t loss = 2122.308416396405\n",
      "Epoch = 23 \t loss = 1797.635091033949\n",
      "Epoch = 24 \t loss = 2081.659567379195\n",
      "Epoch = 25 \t loss = 1925.5539488117793\n",
      "Epoch = 26 \t loss = 1874.1280539553534\n",
      "Epoch = 27 \t loss = 1815.3031032605168\n",
      "Epoch = 28 \t loss = 1771.1417108879475\n",
      "Epoch = 29 \t loss = 1816.250503136037\n",
      "Epoch = 30 \t loss = 1762.3563519721945\n",
      "Epoch = 31 \t loss = 1899.6049828999728\n",
      "Epoch = 32 \t loss = 1880.9828707071642\n",
      "Epoch = 33 \t loss = 1797.2544449845443\n",
      "Epoch = 34 \t loss = 2049.4327939593372\n",
      "Epoch = 35 \t loss = 1846.8256774394736\n",
      "Epoch = 36 \t loss = 1682.2159054193653\n",
      "Epoch = 37 \t loss = 1765.4959955720099\n",
      "Epoch = 38 \t loss = 1574.916271974402\n",
      "Epoch = 39 \t loss = 1804.1566377631068\n",
      "Epoch = 40 \t loss = 1866.6366306378652\n",
      "Epoch = 41 \t loss = 1986.0123948491148\n",
      "Epoch = 42 \t loss = 1712.6244306644885\n",
      "Epoch = 43 \t loss = 1631.7956317975406\n",
      "Epoch = 44 \t loss = 1603.6061622311922\n",
      "Epoch = 45 \t loss = 1473.9772045049183\n",
      "Epoch = 46 \t loss = 1526.935886566514\n",
      "Epoch = 47 \t loss = 1607.7894845568449\n",
      "Epoch = 48 \t loss = 1511.5703920518126\n",
      "Epoch = 49 \t loss = 1634.7397778236593\n",
      "Epoch = 50 \t loss = 1500.7238567019795\n",
      "Epoch = 51 \t loss = 1474.0241736444293\n",
      "Epoch = 52 \t loss = 1595.3564412542203\n",
      "Epoch = 53 \t loss = 1618.7589822626805\n",
      "Epoch = 54 \t loss = 2000.4086931544095\n",
      "Epoch = 55 \t loss = 1435.2338711948753\n",
      "Epoch = 56 \t loss = 1391.1868722581637\n",
      "Epoch = 57 \t loss = 1626.7034166635476\n",
      "Epoch = 58 \t loss = 1567.7736828957334\n",
      "Epoch = 59 \t loss = 1446.600920344404\n",
      "Epoch = 60 \t loss = 1529.6345643015607\n",
      "Epoch = 61 \t loss = 1464.1424019746519\n",
      "Epoch = 62 \t loss = 1580.6359641687304\n",
      "Epoch = 63 \t loss = 1422.877563505137\n",
      "Epoch = 64 \t loss = 1572.4190586409384\n",
      "Epoch = 65 \t loss = 1419.2018156712181\n",
      "Epoch = 66 \t loss = 1321.2443403598065\n",
      "Epoch = 67 \t loss = 1552.6148204816056\n",
      "Epoch = 68 \t loss = 1401.2344233288297\n",
      "Epoch = 69 \t loss = 1290.6440612254833\n",
      "Epoch = 70 \t loss = 1435.0866692816467\n",
      "Epoch = 71 \t loss = 1464.1352639250256\n",
      "Epoch = 72 \t loss = 1515.2393798933151\n",
      "Epoch = 73 \t loss = 1448.515901201413\n",
      "Epoch = 74 \t loss = 1449.128751803187\n",
      "Epoch = 75 \t loss = 1190.6412581741063\n",
      "Epoch = 76 \t loss = 1483.4294239535748\n",
      "Epoch = 77 \t loss = 1262.3876918849592\n",
      "Epoch = 78 \t loss = 1269.5347695876173\n",
      "Epoch = 79 \t loss = 1334.0744238491457\n",
      "Epoch = 80 \t loss = 1201.75107446095\n",
      "Epoch = 81 \t loss = 1162.0261624139368\n",
      "Epoch = 82 \t loss = 1382.3471843177151\n",
      "Epoch = 83 \t loss = 1352.598384614586\n",
      "Epoch = 84 \t loss = 1303.0171022736859\n",
      "Epoch = 85 \t loss = 1128.3762810477772\n",
      "Epoch = 86 \t loss = 1161.460187845619\n",
      "Epoch = 87 \t loss = 1400.3629913934333\n",
      "Epoch = 88 \t loss = 1344.2319267024034\n",
      "Epoch = 89 \t loss = 1369.3646547109265\n",
      "Epoch = 90 \t loss = 1250.0011079556516\n",
      "Epoch = 91 \t loss = 1316.372961782233\n",
      "Epoch = 92 \t loss = 1387.0117709883323\n",
      "Epoch = 93 \t loss = 1314.795954685649\n",
      "Epoch = 94 \t loss = 1337.3796140372335\n",
      "Epoch = 95 \t loss = 1303.8042147678173\n",
      "Epoch = 96 \t loss = 1424.7397007209775\n",
      "Epoch = 97 \t loss = 1336.9865891343857\n",
      "Epoch = 98 \t loss = 1471.8149186320504\n",
      "Epoch = 99 \t loss = 1168.455113135149\n",
      "Epoch = 100 \t loss = 1292.3890618415805\n",
      "Training set accuracy\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98      5923\n",
      "           1       0.99      0.99      0.99      6742\n",
      "           2       0.97      0.98      0.98      5958\n",
      "           3       0.97      0.97      0.97      6131\n",
      "           4       0.98      0.98      0.98      5842\n",
      "           5       0.97      0.93      0.95      5421\n",
      "           6       0.98      0.99      0.98      5918\n",
      "           7       0.98      0.98      0.98      6265\n",
      "           8       0.95      0.97      0.96      5851\n",
      "           9       0.96      0.97      0.97      5949\n",
      "\n",
      "    accuracy                           0.97     60000\n",
      "   macro avg       0.97      0.97      0.97     60000\n",
      "weighted avg       0.97      0.97      0.97     60000\n",
      "\n",
      "Test set accuracy\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97       980\n",
      "           1       0.98      0.98      0.98      1135\n",
      "           2       0.95      0.95      0.95      1032\n",
      "           3       0.94      0.95      0.95      1010\n",
      "           4       0.96      0.96      0.96       982\n",
      "           5       0.94      0.91      0.92       892\n",
      "           6       0.96      0.96      0.96       958\n",
      "           7       0.96      0.95      0.95      1028\n",
      "           8       0.93      0.95      0.94       974\n",
      "           9       0.93      0.94      0.93      1009\n",
      "\n",
      "    accuracy                           0.95     10000\n",
      "   macro avg       0.95      0.95      0.95     10000\n",
      "weighted avg       0.95      0.95      0.95     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### CLASSIFY MNIST PICTURES\n",
    "\n",
    "# load the full MNIST dataset: both data and labels\n",
    "((trainX, trainY), (testX, testY)) = mnist.load_data()\n",
    "\n",
    "# scale the data to values in [0,1]\n",
    "trainX = trainX.astype('float32')/255.0\n",
    "testX = testX.astype('float32')/255.0\n",
    "\n",
    "# reshape the data\n",
    "trainX = trainX.reshape([60000, 28*28])\n",
    "testX = testX.reshape([10000, 28*28])\n",
    "\n",
    "# convert the digits to one-hot vectors\n",
    "trainY = to_categorical(trainY, 10)\n",
    "testY = to_categorical(testY, 10)\n",
    "\n",
    "# fit the model to the training data\n",
    "model = FeedforwardNeuralNetworkSGD([784, 32, 16, 10], 0.5, 32)\n",
    "model.fit(trainX, trainY, 100, 1)\n",
    "\n",
    "# print the classification performance\n",
    "print(\"Training set accuracy\")\n",
    "predictedY = model.predict(trainX)\n",
    "predictedY = predictedY.argmax(axis=1)\n",
    "\n",
    "trainY = trainY.argmax(axis=1)\n",
    "print(classification_report(trainY, predictedY))\n",
    "\n",
    "print(\"Test set accuracy\")\n",
    "predictedY = model.predict(testX)\n",
    "predictedY = predictedY.argmax(axis=1)\n",
    "\n",
    "testY = testY.argmax(axis=1)\n",
    "print(classification_report(testY, predictedY))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766ef2d6-5f86-48b5-ab6c-9fbb64d693f0",
   "metadata": {},
   "source": [
    "Our test accuracy on MNIST jumped from mid-80\\% previously to 95\\% with our implementation using SGD and the full dataset!\n",
    "\n",
    "### Cross-Entropy Loss Function\n",
    "\n",
    "We will discuss in class, but the cross-entropy loss function can lead to faster training than the SSE we have used before, so we add it to the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fbe147c2-bfdc-40ba-b704-887c9171b4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedforwardNeuralNetworkSGD:\n",
    "    \n",
    "    # input a vector [a, b, c, ...] with the number of nodes in each layer\n",
    "    def __init__(self, layers, alpha = 0.1, batchSize = 32, loss = 'sum-of-squares'):\n",
    "        # list of weight matrices between layers\n",
    "        self.W = []\n",
    "        \n",
    "        # network architecture will be a vector of numbers of nodes for each layer\n",
    "        self.layers = layers\n",
    "        \n",
    "        # learning rate\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        # batch size\n",
    "        self.batchSize = batchSize\n",
    "        \n",
    "        # loss function\n",
    "        self.loss = loss\n",
    "        \n",
    "        # initialize the weights (randomly) -- this is our initial guess for gradient descent\n",
    "        \n",
    "        # initialize the weights between layers (up to the next-to-last one) as normal random variables\n",
    "        for i in np.arange(0, len(layers) - 2):\n",
    "            self.W.append(np.random.randn(layers[i] + 1, layers[i + 1] + 1))\n",
    "            \n",
    "        # initialize weights between the last two layers (we don't want bias for the last one)\n",
    "        self.W.append(np.random.randn(layers[-2] + 1, layers[-1]))\n",
    "        \n",
    "    # define the sigmoid activation\n",
    "    def sigmoid(self, x):\n",
    "        return 1.0 / (1 + np.exp(-x))\n",
    "    \n",
    "    # define the sigmoid derivative (where z is the output of a sigmoid)\n",
    "    def sigmoidDerivative(self, z):\n",
    "        return z * (1 - z)\n",
    "    \n",
    "    # get a new mini-batch of data from the dataset\n",
    "    def getNextBatch(self, X, y, batchSize):\n",
    "        for i in np.arange(0, X.shape[0], batchSize):\n",
    "            # yield returns a generator, which can continue where it left off on later calls\n",
    "            # of the function\n",
    "            yield (X[i:i + batchSize], y[i:i + batchSize])\n",
    "    \n",
    "    # fit the model\n",
    "    def fit(self, X, y, epochs = 10000, update = 1000):\n",
    "        # add a column of ones to the end of X\n",
    "        X = np.hstack((X, np.ones([X.shape[0],1])))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            # randomize the examples\n",
    "            p = np.arange(0, X.shape[0])\n",
    "            np.random.shuffle(p)\n",
    "            X = X[p]\n",
    "            y = y[p]\n",
    "\n",
    "            # feed forward, backprop, and weight update\n",
    "            for (x, target) in self.getNextBatch(X, y, self.batchSize):\n",
    "                \n",
    "                # make a list of output activations from the first layer\n",
    "                # (just the original x values)\n",
    "                A = [np.atleast_2d(x)]\n",
    "                \n",
    "                # feed forward\n",
    "                for layer in range(len(self.W)):\n",
    "                    \n",
    "                    # feed through one layer and apply sigmoid activation\n",
    "                    net = A[layer].dot(self.W[layer])\n",
    "                    out = self.sigmoid(net)\n",
    "                    \n",
    "                    # add our network output to the list of activations\n",
    "                    A.append(out)\n",
    "                    \n",
    "                # backpropagation (coming soon!)\n",
    "                error = A[-1] - target\n",
    "                \n",
    "                if self.loss == 'sum-of-squares':\n",
    "                    D = [error * self.sigmoidDerivative(A[-1])]\n",
    "                    \n",
    "                if self.loss == 'cross-entropy':\n",
    "                    D = [error]\n",
    "                    \n",
    "                # loop backwards over the layers to build up deltas\n",
    "                for layer in np.arange(len(A) - 2, 0, -1):\n",
    "                    delta = D[-1].dot(self.W[layer].T)\n",
    "                    delta = delta * self.sigmoidDerivative(A[layer])\n",
    "                    D.append(delta)\n",
    "                    \n",
    "                # reverse the deltas since we looped in reverse\n",
    "                D = D[::-1]\n",
    "                \n",
    "                # weight update in each layer\n",
    "                for layer in range(len(self.W)):\n",
    "                    self.W[layer] -= self.alpha * A[layer].T.dot(D[layer])\n",
    "                    \n",
    "            # print an \n",
    "            if (epoch + 1) % update == 0:\n",
    "                loss = self.computeLoss(X,y)\n",
    "                print('Epoch =', epoch + 1, '\\t loss =', loss)\n",
    "                \n",
    "    def predict(self, X, addOnes = True):\n",
    "        # initialize data, be sure it's the right dimension\n",
    "        p = np.atleast_2d(X)\n",
    "        \n",
    "        # add a column of 1s for bias\n",
    "        if addOnes:\n",
    "            p = np.hstack((p, np.ones([X.shape[0],1])))\n",
    "        \n",
    "        # feed forward!\n",
    "        for layer in np.arange(0, len(self.W)):\n",
    "            p = self.sigmoid(np.dot(p, self.W[layer]))\n",
    "            \n",
    "        return p\n",
    "    \n",
    "    def computeLoss(self, X, y):\n",
    "        # initialize data, be sure it's the right dimension\n",
    "        y = np.atleast_2d(y)\n",
    "        \n",
    "        # feed the datapoints through the network to get predicted outputs\n",
    "        predictions = self.predict(X, addOnes = False)\n",
    "        \n",
    "        # if the loss function is sum of squares, compute it\n",
    "        if self.loss == 'sum-of-squares':\n",
    "            loss = np.sum((predictions - y)**2) / 2.0\n",
    "            \n",
    "        # if the loss function is cross-entropy, compute it\n",
    "        if self.loss == 'cross-entropy':\n",
    "            loss = np.sum(np.nan_to_num(-y * np.log(predictions) - (1 - y) * np.log(1 - predictions)))\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0547ac8f-65eb-4786-baee-497d0e03afaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 1 \t loss = 40122.71844849206\n",
      "Epoch = 2 \t loss = 32331.92919555894\n",
      "Epoch = 3 \t loss = 29315.209593694643\n",
      "Epoch = 4 \t loss = 23977.553682819387\n",
      "Epoch = 5 \t loss = 21971.151884070645\n",
      "Epoch = 6 \t loss = 25500.665681209626\n",
      "Epoch = 7 \t loss = 22840.328942089633\n",
      "Epoch = 8 \t loss = 23015.48395805336\n",
      "Epoch = 9 \t loss = 20771.926410321987\n",
      "Epoch = 10 \t loss = 19549.419980649505\n",
      "Epoch = 11 \t loss = 18994.35086963409\n",
      "Epoch = 12 \t loss = 17600.895697610365\n",
      "Epoch = 13 \t loss = 18547.376584929814\n",
      "Epoch = 14 \t loss = 19212.014202816572\n",
      "Epoch = 15 \t loss = 18938.94621936713\n",
      "Epoch = 16 \t loss = 17062.575130269648\n",
      "Epoch = 17 \t loss = 17571.909032503\n",
      "Epoch = 18 \t loss = 15517.32153607155\n",
      "Epoch = 19 \t loss = 15750.82834391362\n",
      "Epoch = 20 \t loss = 15478.15822738616\n",
      "Epoch = 21 \t loss = 15986.494216126219\n",
      "Epoch = 22 \t loss = 16599.109506475495\n",
      "Epoch = 23 \t loss = 14605.050218198267\n",
      "Epoch = 24 \t loss = 14121.688908332279\n",
      "Epoch = 25 \t loss = 16275.73831164797\n",
      "Epoch = 26 \t loss = 14950.041996830387\n",
      "Epoch = 27 \t loss = 14676.86578867796\n",
      "Epoch = 28 \t loss = 13626.346261658635\n",
      "Epoch = 29 \t loss = 13864.947439616397\n",
      "Epoch = 30 \t loss = 14739.767027894646\n",
      "Epoch = 31 \t loss = 13942.513828038807\n",
      "Epoch = 32 \t loss = 13541.061315427205\n",
      "Epoch = 33 \t loss = 16313.097829075214\n",
      "Epoch = 34 \t loss = 13001.927088037944\n",
      "Epoch = 35 \t loss = 12649.07676255359\n",
      "Epoch = 36 \t loss = 11791.167220515475\n",
      "Epoch = 37 \t loss = 12418.56280021595\n",
      "Epoch = 38 \t loss = 15099.941333838848\n",
      "Epoch = 39 \t loss = 12563.647152675789\n",
      "Epoch = 40 \t loss = 12847.160265875887\n",
      "Epoch = 41 \t loss = 12457.136025077165\n",
      "Epoch = 42 \t loss = 12409.727568161134\n",
      "Epoch = 43 \t loss = 12959.27647939511\n",
      "Epoch = 44 \t loss = 14075.22731430229\n",
      "Epoch = 45 \t loss = 12280.354041629871\n",
      "Epoch = 46 \t loss = 12386.405913546922\n",
      "Epoch = 47 \t loss = 13192.767381482865\n",
      "Epoch = 48 \t loss = 13102.840742449098\n",
      "Epoch = 49 \t loss = 12983.809352468214\n",
      "Epoch = 50 \t loss = 13794.459317607118\n",
      "Epoch = 51 \t loss = 11824.830312881691\n",
      "Epoch = 52 \t loss = 13232.656373908732\n",
      "Epoch = 53 \t loss = 12536.489631094353\n",
      "Epoch = 54 \t loss = 11876.835150395393\n",
      "Epoch = 55 \t loss = 14103.213945369554\n",
      "Epoch = 56 \t loss = 13490.220675950071\n",
      "Epoch = 57 \t loss = 11393.794720207565\n",
      "Epoch = 58 \t loss = 11007.80512544315\n",
      "Epoch = 59 \t loss = 11144.119311112308\n",
      "Epoch = 60 \t loss = 11316.766228550261\n",
      "Epoch = 61 \t loss = 10107.214021469019\n",
      "Epoch = 62 \t loss = 12103.410175018122\n",
      "Epoch = 63 \t loss = 11021.555633944981\n",
      "Epoch = 64 \t loss = 11391.171785430017\n",
      "Epoch = 65 \t loss = 10928.570101799822\n",
      "Epoch = 66 \t loss = 10887.538256163405\n",
      "Epoch = 67 \t loss = 9634.9379893994\n",
      "Epoch = 68 \t loss = 11689.894481876994\n",
      "Epoch = 69 \t loss = 10543.833053919492\n",
      "Epoch = 70 \t loss = 11609.007964520013\n",
      "Epoch = 71 \t loss = 10857.820294733508\n",
      "Epoch = 72 \t loss = 11645.582675457346\n",
      "Epoch = 73 \t loss = 10522.766246800047\n",
      "Epoch = 74 \t loss = 11062.414784152936\n",
      "Epoch = 75 \t loss = 11120.670030777868\n",
      "Epoch = 76 \t loss = 9274.034946024061\n",
      "Epoch = 77 \t loss = 11411.88782018744\n",
      "Epoch = 78 \t loss = 10577.285040297182\n",
      "Epoch = 79 \t loss = 11867.144027083736\n",
      "Epoch = 80 \t loss = 10164.824925410405\n",
      "Epoch = 81 \t loss = 11112.244603566185\n",
      "Epoch = 82 \t loss = 10060.80249892781\n",
      "Epoch = 83 \t loss = 8855.75168952277\n",
      "Epoch = 84 \t loss = 10073.281581242403\n",
      "Epoch = 85 \t loss = 9504.595868536324\n",
      "Epoch = 86 \t loss = 9949.301612893736\n",
      "Epoch = 87 \t loss = 9158.913603412886\n",
      "Epoch = 88 \t loss = 9362.587718559475\n",
      "Epoch = 89 \t loss = 9679.825568803271\n",
      "Epoch = 90 \t loss = 9861.941696201515\n",
      "Epoch = 91 \t loss = 9874.282305203094\n",
      "Epoch = 92 \t loss = 9990.626376511082\n",
      "Epoch = 93 \t loss = 9847.15899863163\n",
      "Epoch = 94 \t loss = 11852.56744946186\n",
      "Epoch = 95 \t loss = 10044.18985610732\n",
      "Epoch = 96 \t loss = 9548.792095310147\n",
      "Epoch = 97 \t loss = 12307.449316333114\n",
      "Epoch = 98 \t loss = 10016.691834784087\n",
      "Epoch = 99 \t loss = 9572.091348551648\n",
      "Epoch = 100 \t loss = 9813.099198668277\n",
      "Training set accuracy\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99      5923\n",
      "           1       0.99      0.98      0.99      6742\n",
      "           2       0.96      0.98      0.97      5958\n",
      "           3       0.97      0.96      0.97      6131\n",
      "           4       0.96      0.98      0.97      5842\n",
      "           5       0.97      0.97      0.97      5421\n",
      "           6       0.99      0.98      0.99      5918\n",
      "           7       0.97      0.99      0.98      6265\n",
      "           8       0.96      0.97      0.97      5851\n",
      "           9       0.99      0.94      0.96      5949\n",
      "\n",
      "    accuracy                           0.98     60000\n",
      "   macro avg       0.98      0.98      0.98     60000\n",
      "weighted avg       0.98      0.98      0.98     60000\n",
      "\n",
      "Test set accuracy\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97       980\n",
      "           1       0.98      0.98      0.98      1135\n",
      "           2       0.94      0.95      0.95      1032\n",
      "           3       0.94      0.95      0.94      1010\n",
      "           4       0.94      0.97      0.96       982\n",
      "           5       0.94      0.92      0.93       892\n",
      "           6       0.98      0.96      0.97       958\n",
      "           7       0.94      0.96      0.95      1028\n",
      "           8       0.92      0.94      0.93       974\n",
      "           9       0.98      0.90      0.94      1009\n",
      "\n",
      "    accuracy                           0.95     10000\n",
      "   macro avg       0.95      0.95      0.95     10000\n",
      "weighted avg       0.95      0.95      0.95     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### CLASSIFY MNIST PICTURES\n",
    "\n",
    "# load the full MNIST dataset: both data and labels\n",
    "((trainX, trainY), (testX, testY)) = mnist.load_data()\n",
    "\n",
    "# scale the data to values in [0,1]\n",
    "trainX = trainX.astype('float32')/255.0\n",
    "testX = testX.astype('float32')/255.0\n",
    "\n",
    "# reshape the data\n",
    "trainX = trainX.reshape([60000, 28*28])\n",
    "testX = testX.reshape([10000, 28*28])\n",
    "\n",
    "# convert the digits to one-hot vectors\n",
    "trainY = to_categorical(trainY, 10)\n",
    "testY = to_categorical(testY, 10)\n",
    "\n",
    "# fit the model to the training data\n",
    "model = FeedforwardNeuralNetworkSGD([784, 32, 16, 10], 0.1, 32, 'cross-entropy')\n",
    "model.fit(trainX, trainY, 100, 1)\n",
    "\n",
    "# print the classification performance\n",
    "print(\"Training set accuracy\")\n",
    "predictedY = model.predict(trainX)\n",
    "predictedY = predictedY.argmax(axis=1)\n",
    "\n",
    "trainY = trainY.argmax(axis=1)\n",
    "print(classification_report(trainY, predictedY))\n",
    "\n",
    "print(\"Test set accuracy\")\n",
    "predictedY = model.predict(testX)\n",
    "predictedY = predictedY.argmax(axis=1)\n",
    "\n",
    "testY = testY.argmax(axis=1)\n",
    "print(classification_report(testY, predictedY))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d79680-ff3c-48da-b5a3-43ad1b28e1b6",
   "metadata": {
    "id": "ecc74b6f-96a5-4027-84e8-ea4179f6a945"
   },
   "source": [
    "# Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd5503b-2134-452b-9c2f-cd3bed668386",
   "metadata": {
    "id": "a68317bd"
   },
   "source": [
    "Since computation is a bottleneck for neural nets, it is worthwhile to learn to construct neural nets with a dedicated deep learning framework rather than simply raw Python with NumPy, as we have used so far, because they allow highly optimized computation accelerated by graphics processing units (GPUs) and let us create nets quickly and easily.\n",
    "\n",
    "As of now, the most popular solutions are Meta's PyTorch and Google's TensorFlow with Keras. (There are some other solutions like Theano, Caffe, and MXNet as well.) Both have two main parts: (1) highly optimized tensor computing, including matrix multiplication and (2) simple functionality for creating neural networks with optimized backpropagation.\n",
    "\n",
    "We will use Keras/TensorFlow today."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d345c820-1a5e-4a9b-9776-f37bd422c0a1",
   "metadata": {
    "id": "a736be8c-b4f5-422e-85aa-746244363240"
   },
   "source": [
    "## GPU Computing\n",
    "\n",
    "Neural networks benefit from parallelization with GPUs, and TensorFlow can handle this for us. A GPU can be in your local device or in a cloud. The following code can check if TensorFlow sees a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ee060ee-4048-4f36-8072-70e69be2f5e1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2c356c4a-0af2-4680-9642-5aa9e09be909",
    "outputId": "9f44f6a6-907b-4a1c-eaf4-12b4adf9c1c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "/device:GPU:0\n",
      "device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "numGPUs = len(tf.config.experimental.list_physical_devices('GPU'))\n",
    "\n",
    "print('Num GPUs Available: ', numGPUs)\n",
    "\n",
    "if numGPUs > 0:\n",
    "    print(tf.test.gpu_device_name())\n",
    "    print(device_lib.list_local_devices()[1].physical_device_desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3872d86-baf7-41b1-b072-bec124737b8a",
   "metadata": {
    "id": "d27bc86e-3fb6-46c6-ac64-1768cd7bf533"
   },
   "source": [
    "If there is a GPU on the device running the notebook, this prints the name of it. Otherwise, it prints that there are no GPUs available. (Note that, for TensorFlow to see and use a GPU in your local device, you must install NVIDIA's CUDA parallel computing platform and some drivers.)\n",
    "\n",
    "TensorFlow, on which Keras is built, automatically exploits GPU resources quite efficiently, so it's not something that requires extra effort on our side. This means we have several layers of abstraction: Keras makes calls to TensorFlow, which makes calls to CUDA, which runs on C/C++. So, it is unrealistic to expect the whole path makes *everything* run optimally with absolute maximum efficiency, but, in practice, it is pretty close! However, I did want to point out that it can be worthwhile to customize lower-level code with TensorFlow or manage parallelization with raw CUDA code. None of this is particularly difficult, and CUDA customization is usually unnecessary, but it can be done."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fb3048-e6f1-403a-a545-9c8655c28bd4",
   "metadata": {
    "id": "1b599cea-57ea-4422-ae1a-ed40a486d7f8"
   },
   "source": [
    "### Writing a Fully-connected Feedforward Neural Net with Keras\n",
    "\n",
    "We will aim to write a neural net similar to what we have constructed through the course so far. That is, it should feed data forward through a sequence of layers, the layers should be fully connected (dense), and we should use SGD to optimize it. We can import these things directly from Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4863b14-83ac-4f8b-86f4-c04121aced93",
   "metadata": {
    "id": "0b6e49e4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30984c0e-73d2-4961-a959-684f5a6f1b9f",
   "metadata": {
    "id": "15faaaa6"
   },
   "source": [
    "Let's construct the net to classify MNIST (our beloved benchmarking dataset) on a vanilla neural network (SGD, sigmoid, SSE loss) with two hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ffc8863-9bba-48c5-b550-a44a8fe41fd3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 682
    },
    "id": "5a0e00c2",
    "outputId": "d2e20edf-143d-43b1-c9d6-be4baa85d7ae",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               200960    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 235,146\n",
      "Trainable params: 235,146\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "# Create a feedforward neural net\n",
    "model = Sequential()\n",
    "\n",
    "# Create the layers\n",
    "model.add(Dense(256, activation = 'sigmoid', input_shape=(784,))) # hidden layer 1\n",
    "model.add(Dense(128, activation = 'sigmoid')) # hidden layer 2\n",
    "model.add(Dense(10, activation = 'sigmoid')) # output layer\n",
    "\n",
    "# compile the model by choosing how the optimizer works\n",
    "model.compile(loss = 'mean_squared_error', optimizer = SGD(0.5), metrics = ['accuracy'])\n",
    "\n",
    "# print a summary of the model\n",
    "model.summary()\n",
    "\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427071a0-b8d6-4f2f-a4cd-637e80008fa2",
   "metadata": {
    "id": "3aa7ba7b"
   },
   "source": [
    "Now, let's read in MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9011d9a5-5227-49ae-a0b6-4f752fcb52b0",
   "metadata": {
    "id": "cd33de7a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read in full MNIST\n",
    "(trainX, trainY), (testX, testY) = mnist.load_data()\n",
    "\n",
    "# reshape into vectors\n",
    "trainX = trainX.reshape((trainX.shape[0], 28 * 28))\n",
    "testX = testX.reshape((testX.shape[0], 28 * 28))\n",
    "\n",
    "# scale the data to [0,1]\n",
    "trainX = trainX/255.0\n",
    "testX = testX/255.0\n",
    "\n",
    "# convert the labels to one-hot form\n",
    "trainY = to_categorical(trainY)\n",
    "testY = to_categorical(testY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768c4759-bd87-48de-b796-3ea29a492724",
   "metadata": {
    "id": "d7e66abe"
   },
   "source": [
    "Next, let's classify MNIST with our vanilla net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a77cef9d-e948-4dbf-a8f0-1d331de54f41",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "f53b31b6",
    "outputId": "7c088e60-b6c2-4670-d365-57674558dfd6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "469/469 [==============================] - 5s 8ms/step - loss: 0.0901 - accuracy: 0.1808 - val_loss: 0.0887 - val_accuracy: 0.2372\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0879 - accuracy: 0.2612 - val_loss: 0.0868 - val_accuracy: 0.3296\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.0850 - accuracy: 0.2924 - val_loss: 0.0823 - val_accuracy: 0.3264\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.0786 - accuracy: 0.3994 - val_loss: 0.0741 - val_accuracy: 0.5206\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.0702 - accuracy: 0.5319 - val_loss: 0.0654 - val_accuracy: 0.5886\n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.0612 - accuracy: 0.6219 - val_loss: 0.0564 - val_accuracy: 0.6741\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.0532 - accuracy: 0.6980 - val_loss: 0.0494 - val_accuracy: 0.7416\n",
      "Epoch 8/20\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.0472 - accuracy: 0.7502 - val_loss: 0.0443 - val_accuracy: 0.7708\n",
      "Epoch 9/20\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.0427 - accuracy: 0.7890 - val_loss: 0.0401 - val_accuracy: 0.8113\n",
      "Epoch 10/20\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.0389 - accuracy: 0.8180 - val_loss: 0.0366 - val_accuracy: 0.8372\n",
      "Epoch 11/20\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.0357 - accuracy: 0.8372 - val_loss: 0.0335 - val_accuracy: 0.8527\n",
      "Epoch 12/20\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0330 - accuracy: 0.8495 - val_loss: 0.0309 - val_accuracy: 0.8646\n",
      "Epoch 13/20\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0307 - accuracy: 0.8586 - val_loss: 0.0288 - val_accuracy: 0.8717\n",
      "Epoch 14/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0288 - accuracy: 0.8655 - val_loss: 0.0270 - val_accuracy: 0.8769\n",
      "Epoch 15/20\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.0271 - accuracy: 0.8720 - val_loss: 0.0255 - val_accuracy: 0.8814\n",
      "Epoch 16/20\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.0258 - accuracy: 0.8763 - val_loss: 0.0242 - val_accuracy: 0.8844\n",
      "Epoch 17/20\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0246 - accuracy: 0.8799 - val_loss: 0.0231 - val_accuracy: 0.8868\n",
      "Epoch 18/20\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0236 - accuracy: 0.8831 - val_loss: 0.0222 - val_accuracy: 0.8893\n",
      "Epoch 19/20\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0228 - accuracy: 0.8855 - val_loss: 0.0215 - val_accuracy: 0.8914\n",
      "Epoch 20/20\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0221 - accuracy: 0.8876 - val_loss: 0.0208 - val_accuracy: 0.8928\n",
      "Test accuracy\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.98      0.94       980\n",
      "           1       0.94      0.97      0.96      1135\n",
      "           2       0.89      0.86      0.88      1032\n",
      "           3       0.89      0.89      0.89      1010\n",
      "           4       0.86      0.92      0.89       982\n",
      "           5       0.86      0.78      0.82       892\n",
      "           6       0.91      0.93      0.92       958\n",
      "           7       0.91      0.90      0.90      1028\n",
      "           8       0.86      0.84      0.85       974\n",
      "           9       0.89      0.85      0.87      1009\n",
      "\n",
      "    accuracy                           0.89     10000\n",
      "   macro avg       0.89      0.89      0.89     10000\n",
      "weighted avg       0.89      0.89      0.89     10000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1b58f48d1b0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# fit the model to the training data\n",
    "maxEpoch = 20\n",
    "H = model.fit(trainX, trainY, validation_data = (testX, testY), epochs = maxEpoch, batch_size = 128)\n",
    "\n",
    "print('Test accuracy')\n",
    "\n",
    "# predict posterior probability distribution for labels of the test set\n",
    "predictedY = model.predict(testX)\n",
    "\n",
    "# convert posterior probabilities to labels\n",
    "predictedY = predictedY.argmax(axis = 1)\n",
    "realY = testY.argmax(axis = 1)\n",
    "\n",
    "# print classification report\n",
    "print(classification_report(realY, predictedY))\n",
    "\n",
    "# plot the loss and accuracy through training\n",
    "plt.style.use('ggplot')\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(range(maxEpoch), H.history['loss'], label = 'training loss')\n",
    "plt.plot(range(maxEpoch), H.history['val_loss'], label = 'dev loss')\n",
    "plt.plot(range(maxEpoch), H.history['accuracy'], label = 'training accuracy')\n",
    "plt.plot(range(maxEpoch), H.history['val_accuracy'], label = 'dev accuracy')\n",
    "\n",
    "plt.title('Loss and Accuracy')\n",
    "plt.xlabel('Epoch #')\n",
    "plt.ylabel('Loss/Accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025d8f59-6034-40d4-9e9e-a13dd7de921e",
   "metadata": {
    "id": "9946bdeb"
   },
   "source": [
    "Let's try a more modern version using ReLU and softmax layers with categorical crossentropy loss, the ADAM optimizer, and both $L^1$ and $L^2$ regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "749768f7-43eb-4794-80d0-67ee44aa1090",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8f8cc0b8",
    "outputId": "8101be1e-c446-448e-c6ae-36b18f070504"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_15 (Dense)            (None, 256)               200960    \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 235,146\n",
      "Trainable params: 235,146\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create a feedforward neural net\n",
    "model = Sequential()\n",
    "\n",
    "# Create the layers\n",
    "model.add(Dense(256, input_shape = (784,), activation = 'relu', kernel_regularizer = l1_l2(l1 = 0.0, l2 = 0.0001)))\n",
    "model.add(Dense(128, activation = 'relu', kernel_regularizer = l1_l2(l1 = 0.0, l2 = 0.0001)))\n",
    "model.add(Dense(10, activation = 'softmax', kernel_regularizer = l1_l2(l1 = 0.0, l2 = 0.0001)))\n",
    "\n",
    "# compile the model by choosing how the optimizer works\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n",
    "# print a summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53696de-35d4-46f9-849f-dab3694a96c0",
   "metadata": {
    "id": "76008c07"
   },
   "source": [
    "Let's use this more modern network to classify MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a7d1174b-b421-4d11-a8ea-aa7d57c93880",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "baaf8baa",
    "outputId": "128d9b86-18cd-486a-b108-9e70c3acaa23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.3224 - accuracy: 0.9239 - val_loss: 0.1693 - val_accuracy: 0.9630\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1543 - accuracy: 0.9691 - val_loss: 0.1396 - val_accuracy: 0.9720\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.1216 - accuracy: 0.9779 - val_loss: 0.1201 - val_accuracy: 0.9784\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.1046 - accuracy: 0.9831 - val_loss: 0.1161 - val_accuracy: 0.9797\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0942 - accuracy: 0.9864 - val_loss: 0.1325 - val_accuracy: 0.9725\n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0889 - accuracy: 0.9878 - val_loss: 0.1227 - val_accuracy: 0.9757\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0829 - accuracy: 0.9898 - val_loss: 0.1175 - val_accuracy: 0.9789\n",
      "Epoch 8/20\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0792 - accuracy: 0.9906 - val_loss: 0.1256 - val_accuracy: 0.9743\n",
      "Epoch 9/20\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0774 - accuracy: 0.9906 - val_loss: 0.1143 - val_accuracy: 0.9789\n",
      "Epoch 10/20\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0716 - accuracy: 0.9932 - val_loss: 0.1088 - val_accuracy: 0.9808\n",
      "Epoch 11/20\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0699 - accuracy: 0.9933 - val_loss: 0.1131 - val_accuracy: 0.9791\n",
      "Epoch 12/20\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0693 - accuracy: 0.9929 - val_loss: 0.1192 - val_accuracy: 0.9757\n",
      "Epoch 13/20\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0662 - accuracy: 0.9939 - val_loss: 0.1035 - val_accuracy: 0.9813\n",
      "Epoch 14/20\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0670 - accuracy: 0.9932 - val_loss: 0.1067 - val_accuracy: 0.9806\n",
      "Epoch 15/20\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0653 - accuracy: 0.9940 - val_loss: 0.1060 - val_accuracy: 0.9808\n",
      "Epoch 16/20\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0636 - accuracy: 0.9940 - val_loss: 0.0995 - val_accuracy: 0.9832\n",
      "Epoch 17/20\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0616 - accuracy: 0.9946 - val_loss: 0.1119 - val_accuracy: 0.9812\n",
      "Epoch 18/20\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0612 - accuracy: 0.9951 - val_loss: 0.1020 - val_accuracy: 0.9813\n",
      "Epoch 19/20\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0596 - accuracy: 0.9949 - val_loss: 0.1026 - val_accuracy: 0.9820\n",
      "Epoch 20/20\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0589 - accuracy: 0.9952 - val_loss: 0.1443 - val_accuracy: 0.9668\n",
      "Test accuracy\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       980\n",
      "           1       0.99      0.98      0.99      1135\n",
      "           2       1.00      0.88      0.94      1032\n",
      "           3       0.87      1.00      0.93      1010\n",
      "           4       0.97      0.98      0.97       982\n",
      "           5       0.98      0.96      0.97       892\n",
      "           6       0.99      0.98      0.98       958\n",
      "           7       0.97      0.98      0.97      1028\n",
      "           8       0.97      0.94      0.96       974\n",
      "           9       0.96      0.98      0.97      1009\n",
      "\n",
      "    accuracy                           0.97     10000\n",
      "   macro avg       0.97      0.97      0.97     10000\n",
      "weighted avg       0.97      0.97      0.97     10000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f82309dd910>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# fit the model to the training data\n",
    "maxEpoch = 20\n",
    "H = model.fit(trainX, trainY, validation_data = (testX, testY), epochs = maxEpoch, batch_size = 128)\n",
    "\n",
    "print('Test accuracy')\n",
    "\n",
    "# predict posterior probability distribution for labels of the test set\n",
    "predictedY = model.predict(testX)\n",
    "\n",
    "# convert posterior probabilities to labels\n",
    "predictedY = predictedY.argmax(axis = 1)\n",
    "realY = testY.argmax(axis = 1)\n",
    "\n",
    "# print classification report\n",
    "print(classification_report(realY, predictedY))\n",
    "\n",
    "# plot the loss and accuracy through training\n",
    "plt.style.use('ggplot')\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(range(maxEpoch), H.history['loss'], label = 'training loss')\n",
    "plt.plot(range(maxEpoch), H.history['val_loss'], label = 'dev loss')\n",
    "plt.plot(range(maxEpoch), H.history['accuracy'], label = 'training accuracy')\n",
    "plt.plot(range(maxEpoch), H.history['val_accuracy'], label = 'dev accuracy')\n",
    "\n",
    "plt.title('Loss and Accuracy')\n",
    "plt.xlabel('Epoch #')\n",
    "plt.ylabel('Loss/Accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addb78f3-307b-429b-b7ab-64accc542976",
   "metadata": {
    "id": "7e0aa227-4f4b-46d5-b00e-c137e282361e"
   },
   "source": [
    "As expected, this is a little better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff44ecb-0fd3-47a4-8880-8c536039de75",
   "metadata": {
    "id": "899a3e8d-8c4d-40e7-84ce-2668f5a26857",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Writing a Fully-connected Feedforward Neural Net with Keras\n",
    "\n",
    "We will aim to write a neural net similar to what we have constructed through the course so far. That is, it should feed data forward through a sequence of layers, the layers should be fully connected (dense), and we should use SGD to optimize it. We can import these things directly from Keras."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
