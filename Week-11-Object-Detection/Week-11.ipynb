{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "726d6a86-0599-45bd-8915-fe9a43ffed5b",
   "metadata": {},
   "source": [
    "# Week 11\n",
    "\n",
    "This week, we will be moving beyond image classification and focus on a new task: object detection. This requires us to localize and classify objects from multiple classes in an image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edaadf5d-d81a-4d62-babe-95a4dbb2c13a",
   "metadata": {},
   "source": [
    "Let's do a GPU check before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1b1f5aa-af52-42f4-ba20-64a4faedb5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "numGPUs = len(tf.config.experimental.list_physical_devices('GPU'))\n",
    "\n",
    "print('Num GPUs Available: ', numGPUs)\n",
    "\n",
    "if numGPUs > 0:\n",
    "    print(tf.test.gpu_device_name())\n",
    "    print(device_lib.list_local_devices()[1].physical_device_desc)\n",
    "    print(device_lib.list_local_devices()[2].physical_device_desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286b0916-2289-4d09-8c9b-2fb76769f2fe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Lecture 18 - Classical Object Detection\n",
    "\n",
    "Below we import some libraries and write some utility functions.\n",
    "\n",
    "**Note**: Some code here is adapted from https://www.pyimagesearch.com/2020/06/22/turning-any-cnn-image-classifier-into-an-object-detector-with-keras-tensorflow-and-opencv/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7cf6b68-0038-487f-9f0c-592a665c2433",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imutils\n",
    "import numpy as np\n",
    "\n",
    "def sliding_window(image, stride, wh):\n",
    "    # slide window vertically\n",
    "    for y in range(0, image.shape[0] - wh[1], stride):\n",
    "        \n",
    "        # slide window horizontally\n",
    "        for x in range(0, image.shape[1] - wh[0], stride):\n",
    "            \n",
    "            # yield the lower left corner of the window and the window\n",
    "            yield (x, y, image[y:y + wh[1], x:x + wh[0]])\n",
    "            \n",
    "def image_pyramid(image, scale = 2, minSize = (224, 224)):\n",
    "    # yield the original window\n",
    "    yield image\n",
    "    \n",
    "    while True:\n",
    "        # find the dimensions of the next image in the pydamid\n",
    "        w = int(image.shape[1] / scale)\n",
    "        \n",
    "        # resize the image while maintaining aspect ratio\n",
    "        image = imutils.resize(image, width = int(image.shape[1] / scale))\n",
    "        \n",
    "        # if the image is below the min size, stop\n",
    "        if image.shape[0] < minSize[1] or image.shape[1] < minSize[0]:\n",
    "            break\n",
    "            \n",
    "        # yield the next image in the pyramid\n",
    "        yield image\n",
    "        \n",
    "#  Felzenszwalb et al.\n",
    "def non_max_suppression_slow(boxes, overlapThresh):\n",
    "    # if there are no boxes, return an empty list\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "    \n",
    "    # initialize the list of picked indexes\n",
    "    pick = []\n",
    "    \n",
    "    # get coordinates of the bounding boxes\n",
    "    x1 = boxes[:,0]\n",
    "    y1 = boxes[:,1]\n",
    "    x2 = boxes[:,2]\n",
    "    y2 = boxes[:,3]\n",
    "    \n",
    "    # compute the area of the bounding boxes and sort by the bottom-right y-coordinate\n",
    "    area = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    idxs = np.argsort(y2)\n",
    "    \n",
    "    # loop over the indexes\n",
    "    while len(idxs) > 0:\n",
    "        # get last index in the indexes list\n",
    "        last = len(idxs) - 1\n",
    "        i = idxs[last]\n",
    "        \n",
    "        # add the index value to the list of picked indexes\n",
    "        pick.append(i)\n",
    "        \n",
    "        # initialize the suppression list using the last index\n",
    "        suppress = [last]\n",
    "        \n",
    "        # loop over all indexes in list\n",
    "        for pos in range(last):\n",
    "            # get the current index\n",
    "            j = idxs[pos]\n",
    "            \n",
    "            # find min and max (x, y) coordinates for the bounding box\n",
    "            xx1 = max(x1[i], x1[j])\n",
    "            yy1 = max(y1[i], y1[j])\n",
    "            xx2 = min(x2[i], x2[j])\n",
    "            yy2 = min(y2[i], y2[j])\n",
    "            \n",
    "            # compute the width and height of the bounding box\n",
    "            w = max(0, xx2 - xx1 + 1)\n",
    "            h = max(0, yy2 - yy1 + 1)\n",
    "            \n",
    "            # compute the ratio of overlap between the computed bounding box and the bounding box in the area list\n",
    "            overlap = float(w * h) / area[j]\n",
    "            \n",
    "            # if there is sufficient overlap, suppress the current bounding box\n",
    "            if overlap > overlapThresh:\n",
    "                suppress.append(pos)\n",
    "        \n",
    "        # delete all indexes in the suppression list\n",
    "        idxs = np.delete(idxs, suppress)\n",
    "        \n",
    "    # return the bounding boxes that were picked\n",
    "    return boxes[pick]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40ce8792-c7fc-4272-b26d-f51e7d554503",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet101\n",
    "from tensorflow.keras.applications.resnet import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.applications import imagenet_utils\n",
    "from imutils.object_detection import non_max_suppression\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aac2e45-f7c5-4e1a-93e9-63774cda79cf",
   "metadata": {},
   "source": [
    "## Transfer Classifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0871cbe7-85dd-42be-9281-62f4b5886991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize variables used for the object detection procedure\n",
    "WIDTH = 800\n",
    "P_SCALE = 1.25\n",
    "WIN_STRIDE = 10\n",
    "#ROI_SIZE = (550, 350)\n",
    "ROI_SIZES = [(200, 100), (150, 100), (100, 100), (100, 150), (100, 200)]\n",
    "INPUT_SIZE = (224, 224)\n",
    "IMAGE = 'cat_dog.jpg'\n",
    "VIZ = False\n",
    "MINCONF = 0.8\n",
    "OVERLAP_THRESHOLD = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aeff1f5e-6c19-4b94-bdaa-cf74d8610a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading network...\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "[INFO] Searching ROI size (200, 100)\n",
      "[INFO] New pyramid image of shape (450, 800, 3)\n",
      "[INFO] New pyramid image of shape (360, 640, 3)\n",
      "[INFO] New pyramid image of shape (288, 512, 3)\n",
      "[INFO] New pyramid image of shape (230, 409, 3)\n",
      "[INFO] New pyramid image of shape (183, 327, 3)\n",
      "[INFO] New pyramid image of shape (146, 261, 3)\n",
      "[INFO] New pyramid image of shape (116, 208, 3)\n",
      "[INFO] Looping over pyramid/windows took 1.311359167098999 s\n",
      "[INFO] Searching ROI size (150, 100)\n",
      "[INFO] New pyramid image of shape (450, 800, 3)\n",
      "[INFO] New pyramid image of shape (360, 640, 3)\n",
      "[INFO] New pyramid image of shape (288, 512, 3)\n",
      "[INFO] New pyramid image of shape (230, 409, 3)\n",
      "[INFO] New pyramid image of shape (183, 327, 3)\n",
      "[INFO] New pyramid image of shape (146, 261, 3)\n",
      "[INFO] New pyramid image of shape (116, 208, 3)\n",
      "[INFO] Looping over pyramid/windows took 1.6097431182861328 s\n",
      "[INFO] Searching ROI size (100, 100)\n",
      "[INFO] New pyramid image of shape (450, 800, 3)\n",
      "[INFO] New pyramid image of shape (360, 640, 3)\n",
      "[INFO] New pyramid image of shape (288, 512, 3)\n",
      "[INFO] New pyramid image of shape (230, 409, 3)\n",
      "[INFO] New pyramid image of shape (183, 327, 3)\n",
      "[INFO] New pyramid image of shape (146, 261, 3)\n",
      "[INFO] New pyramid image of shape (116, 208, 3)\n",
      "[INFO] Looping over pyramid/windows took 1.8161864280700684 s\n",
      "[INFO] Searching ROI size (100, 150)\n",
      "[INFO] New pyramid image of shape (450, 800, 3)\n",
      "[INFO] New pyramid image of shape (360, 640, 3)\n",
      "[INFO] New pyramid image of shape (288, 512, 3)\n",
      "[INFO] New pyramid image of shape (230, 409, 3)\n",
      "[INFO] New pyramid image of shape (183, 327, 3)\n",
      "[INFO] Looping over pyramid/windows took 1.5025999546051025 s\n",
      "[INFO] Searching ROI size (100, 200)\n",
      "[INFO] New pyramid image of shape (450, 800, 3)\n",
      "[INFO] New pyramid image of shape (360, 640, 3)\n",
      "[INFO] New pyramid image of shape (288, 512, 3)\n",
      "[INFO] New pyramid image of shape (230, 409, 3)\n",
      "[INFO] Looping over pyramid/windows took 1.0800023078918457 s\n",
      "[INFO] Classifying ROIs...\n",
      "[INFO] Number of ROIs of aspect ratio 2.0 to 1: 4279\n",
      "34/34 [==============================] - 36s 775ms/step\n",
      "[INFO] Classifying ROIs took 38.030709981918335 s\n",
      "[INFO] Classifying ROIs...\n",
      "[INFO] Number of ROIs of aspect ratio 1.5 to 1: 4824\n",
      "38/38 [==============================] - 33s 835ms/step\n",
      "[INFO] Classifying ROIs took 34.990148305892944 s\n",
      "[INFO] Classifying ROIs...\n",
      "[INFO] Number of ROIs of aspect ratio 1.0 to 1: 5369\n",
      "42/42 [==============================] - 34s 774ms/step\n",
      "[INFO] Classifying ROIs took 36.26236367225647 s\n",
      "[INFO] Classifying ROIs...\n",
      "[INFO] Number of ROIs of aspect ratio 0.67 to 1: 4162\n",
      "33/33 [==============================] - 28s 801ms/step\n",
      "[INFO] Classifying ROIs took 29.78913116455078 s\n",
      "[INFO] Classifying ROIs...\n",
      "[INFO] Number of ROIs of aspect ratio 0.5 to 1: 3085\n",
      "25/25 [==============================] - 21s 783ms/step\n",
      "[INFO] Classifying ROIs took 22.473191261291504 s\n"
     ]
    }
   ],
   "source": [
    "# load ResNet pretrained on ImageNet\n",
    "print('[INFO] Loading network...')\n",
    "\n",
    "# if there are multiple GPUs, use a mirrored strategy to spread batches to all of the GPUs\n",
    "if numGPUs > 1:\n",
    "    strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())\n",
    "\n",
    "    with strategy.scope():\n",
    "        model = ResNet101(weights = 'imagenet')\n",
    "\n",
    "# if there are not multiple GPUs, run it normally\n",
    "else:\n",
    "    model = ResNet101(weights = 'imagenet')\n",
    "\n",
    "# load the input image, resize to specified width, and find the new dimensions\n",
    "original = cv2.imread(IMAGE)\n",
    "original = imutils.resize(original, width = WIDTH)\n",
    "H, W = original.shape[:2]\n",
    "\n",
    "# initialize a list for ROIs from the pyramid and sliding windows\n",
    "rois = []\n",
    "\n",
    "# initialize a list for coordinates in the original image for the ROIs\n",
    "locs = []\n",
    "\n",
    "for ROI_SIZE in ROI_SIZES:\n",
    "    print('[INFO] Searching ROI size', ROI_SIZE)\n",
    "    new_rois = []\n",
    "    new_locs = []\n",
    "\n",
    "    # initialize image pyramid\n",
    "    pyramid = image_pyramid(original, scale = P_SCALE, minSize = ROI_SIZE)\n",
    "\n",
    "    # start a timer\n",
    "    start = time.time()\n",
    "\n",
    "    # preprocess images from pyramid\n",
    "    for image in pyramid:\n",
    "        print('[INFO] New pyramid image of shape', image.shape)\n",
    "        # find the scale between the original image and current layer of pyramid\n",
    "        scale = W / float(image.shape[1])\n",
    "\n",
    "        # loop over sliding window locations\n",
    "        for x, y, roiOriginal in sliding_window(image, WIN_STRIDE, ROI_SIZE):\n",
    "            # scale coordinates of ROI\n",
    "            x = int(x * scale)\n",
    "            y = int(y * scale)\n",
    "            w = int(ROI_SIZE[0] * scale)\n",
    "            h = int(ROI_SIZE[1] * scale)\n",
    "\n",
    "            # preprocess ROI\n",
    "            roi = cv2.resize(roiOriginal, INPUT_SIZE)\n",
    "            roi = img_to_array(roi)\n",
    "            roi = preprocess_input(roi)\n",
    "\n",
    "            # update list of ROIs and coordinates\n",
    "            new_rois.append(roi)\n",
    "            new_locs.append((x, y, x + w, y + h))\n",
    "\n",
    "            if VIZ:\n",
    "                # clone image and draw a bounding box\n",
    "                clone = original.copy()\n",
    "                cv2.rectangle(clone, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "                # show visualization of current ROI\n",
    "                cv2.imshow('Visualization', clone)\n",
    "                cv2.imshow('ROI', roiOriginal)\n",
    "                cv2.waitKey(0)\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    print('[INFO] Looping over pyramid/windows took', end - start, 's')\n",
    "    \n",
    "    new_rois = np.array(new_rois, dtype = 'float32')\n",
    "    \n",
    "    rois.append(new_rois)\n",
    "    locs.append(new_locs)\n",
    "\n",
    "output = open('rois.pkl', 'wb')\n",
    "pickle.dump([rois, locs], output)\n",
    "output.close()\n",
    "    \n",
    "for roi, loc, ROI_SIZE in zip(rois, locs, ROI_SIZES):\n",
    "    \n",
    "    # clear GPU memory\n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    # classify each proposal\n",
    "    print('[INFO] Classifying ROIs...')\n",
    "    print('[INFO] Number of ROIs of aspect ratio', np.round(ROI_SIZE[0]/ROI_SIZE[1], 2), 'to 1:', roi.shape[0])\n",
    "    start = time.time()\n",
    "    preds = model.predict(roi, batch_size = 128, verbose = 1)\n",
    "    end = time.time()\n",
    "    print('[INFO] Classifying ROIs took', end - start, 's')\n",
    "\n",
    "    predictions = imagenet_utils.decode_predictions(preds, top = 1)\n",
    "    labels = {}\n",
    "\n",
    "    # loop over the predictions\n",
    "    for (i, p) in enumerate(predictions):\n",
    "        # get the prediction information for the current ROI\n",
    "        imagenetID, label, prob = p[0]\n",
    "\n",
    "        # filter out weak detections\n",
    "        if prob >= MINCONF:\n",
    "            # get bounding box associated with the prediction and\n",
    "            # convert the coordinates\n",
    "            box = loc[i]\n",
    "\n",
    "            # get predictions for the label and add the bounding box and probability to the list\n",
    "            L = labels.get(label, [])\n",
    "            L.append((box, prob))\n",
    "            labels[label] = L\n",
    "\n",
    "    # loop over the labels for each of detected objects in the image\n",
    "    for label in labels.keys():\n",
    "        # clone the original image\n",
    "        #print('[INFO] Showing results for', label)\n",
    "        clone = original.copy()\n",
    "\n",
    "        # loop over all bounding boxes for the current label\n",
    "        for box, prob in labels[label]:\n",
    "            # draw the bounding box on the image\n",
    "            startX, startY, endX, endY = box\n",
    "            cv2.rectangle(clone, (startX, startY), (endX, endY), (0, 255, 0), 2)\n",
    "\n",
    "        # show the results before NMS\n",
    "        cv2.imshow('Before NMS', clone)\n",
    "        clone = original.copy()\n",
    "\n",
    "        # extract the bounding boxes and prediction probabilities\n",
    "        boxes = np.array([p[0] for p in labels[label]])\n",
    "        proba = np.array([p[1] for p in labels[label]])\n",
    "\n",
    "        # apply NMS\n",
    "        #print('[INFO] Applying NMS...')\n",
    "        boxes = non_max_suppression_slow(boxes, OVERLAP_THRESHOLD)\n",
    "\n",
    "        # loop over all bounding boxes that were kept after applying NMS\n",
    "        for (startX, startY, endX, endY) in boxes:\n",
    "\n",
    "            # draw the bounding box and label on the image\n",
    "            cv2.rectangle(clone, (startX, startY), (endX, endY), (0, 255, 0), 2)\n",
    "\n",
    "            y = startY - 10 if startY - 10 > 10 else startY + 10\n",
    "\n",
    "            cv2.putText(clone, label, (startX, y), cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 255, 0), 2)\n",
    "\n",
    "        # show the output after apply non-maxima suppression\n",
    "        cv2.imshow('After NMS', clone)\n",
    "        cv2.waitKey(0)\n",
    "\n",
    "# close windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15e7a6c-967d-42c7-9fba-b2139fa15443",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Selective Search and R-CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363763c5-8e94-4253-8fc2-c027c9b02e6a",
   "metadata": {},
   "source": [
    "## Selective Search\n",
    "\n",
    "Selective Search is a method for generating region proposals as an alternative to sliding windows and image pyramids. The latter is pretty brute-force, it is very slow, and it is very sensitive to the hyperparameters of the window size, pyramid scaling factor, etc. Beyond that, Selective Search tries to make this process more intelligent and more automatic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e68555e9-e239-4a47-8e6f-276457427a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import random\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "def selective_search(image_path, method = 'fast', viz = False):\n",
    "\n",
    "    # import the image\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # initialize OpenCV's selective search implementation and set the input image\n",
    "    ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n",
    "    ss.setBaseImage(image)\n",
    "\n",
    "    # check to see if we are using the *fast* but *less accurate* version of selective search\n",
    "    if method == 'fast':\n",
    "        print('using *fast* selective search')\n",
    "        ss.switchToSelectiveSearchFast()\n",
    "\n",
    "    # otherwise we are using the *slower* but *more accurate* version\n",
    "    else:\n",
    "        print('using *quality* selective search')\n",
    "        ss.switchToSelectiveSearchQuality()\n",
    "\n",
    "    # run selective search on the input image\n",
    "    start = time.time()\n",
    "    rects = ss.process()\n",
    "    end = time.time()\n",
    "\n",
    "    # show how along selective search took to run along with the total\n",
    "    # number of returned region proposals\n",
    "    print('selective search took', end - start, 'seconds')\n",
    "    print('selective search generated', len(rects), 'region proposals')\n",
    "\n",
    "    if viz:\n",
    "        # loop over the region proposals in chunks (so we can visualize them)\n",
    "        for i in range(0, len(rects), 100):\n",
    "            # clone the original image so we can draw on it\n",
    "            output = image.copy()\n",
    "\n",
    "            # loop over the current subset of region proposals\n",
    "            for (x, y, w, h) in rects[i:100]:\n",
    "                # draw the region proposal bounding box on the image\n",
    "                color = [random.randint(0, 255) for j in range(0, 3)]\n",
    "                cv2.rectangle(output, (x, y), (x + w, y + h), color, 2)\n",
    "\n",
    "            # show the output image\n",
    "            cv2.imshow(\"Output\", output)\n",
    "            key = cv2.waitKey(0) & 0xFF\n",
    "\n",
    "            # if the `q` key was pressed, break from the loop\n",
    "            if key == ord(\"q\"):\n",
    "                break\n",
    "\n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "    return rects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2cd303aa-40ac-4014-b253-c614d10d7cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using *quality* selective search\n",
      "selective search took 34.400309562683105 seconds\n",
      "selective search generated 16930 region proposals\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1253,  391,   28,  105],\n",
       "       [ 247,  563,   14,   37],\n",
       "       [ 450,  230,   56,  116],\n",
       "       ...,\n",
       "       [ 480,    0, 1020,  300],\n",
       "       [ 605,    0,  895,  354],\n",
       "       [  84,    0, 1416,  300]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selective_search('cat_dog.jpg', 'quality', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2aad693-8c1c-463d-b8c2-7d2fafb64d15",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'cv2' has no attribute 'ximgproc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_37696/2925020166.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mselective_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cat_dog.jpg'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'fast'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_37696/2224566901.py\u001b[0m in \u001b[0;36mselective_search\u001b[1;34m(image_path, method, viz)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m# initialize OpenCV's selective search implementation and set the input image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mximgproc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msegmentation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateSelectiveSearchSegmentation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetBaseImage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'cv2' has no attribute 'ximgproc'"
     ]
    }
   ],
   "source": [
    "selective_search('cat_dog.jpg', 'fast', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "77dd1b70-31aa-46e2-9693-a913e582befd",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE = 'dog.jpg'\n",
    "METHOD = 'fast'\n",
    "MINCONF = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8ebb1810-dae0-4535-9f6b-d765548256ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading ResNet...\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "using *fast* selective search\n",
      "selective search took 4.033909559249878 seconds\n",
      "selective search generated 1321 region proposals\n",
      "classifying ROIs...\n",
      "classifying ROIs took 6.421988010406494 s\n",
      "showing results for golden_retriever\n",
      "applying NMS...\n",
      "showing results for Dandie_Dinmont\n",
      "applying NMS...\n",
      "showing results for llama\n",
      "applying NMS...\n",
      "showing results for Brittany_spaniel\n",
      "applying NMS...\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.applications import imagenet_utils\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from imutils.object_detection import non_max_suppression\n",
    "import numpy as np\n",
    "\n",
    "# load ResNet from disk with weights pre-trained on ImageNet\n",
    "print('loading ResNet...')\n",
    "\n",
    "# if there are multiple GPUs, use a mirrored strategy to spread batches to all of the GPUs\n",
    "if numGPUs > 1:\n",
    "    strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())\n",
    "\n",
    "    with strategy.scope():\n",
    "        model = ResNet101(weights = 'imagenet')\n",
    "\n",
    "# if there are not multiple GPUs, run it normally\n",
    "else:\n",
    "    model = ResNet101(weights = 'imagenet')\n",
    "\n",
    "# load the input image from disk and get its dimensions\n",
    "image = cv2.imread(IMAGE)\n",
    "(H, W) = image.shape[:2]\n",
    "\n",
    "# run selective search on the input image\n",
    "rects = selective_search(IMAGE, METHOD)\n",
    "\n",
    "# initialize the list of region proposals to classify and bounding boxes\n",
    "rois = []\n",
    "locs = []\n",
    "\n",
    "# preprocess boxes from selective search\n",
    "for (x, y, w, h) in rects:\n",
    "    # filter out small rois\n",
    "    if w / float(W) < 0.1 or h / float(H) < 0.1:\n",
    "        continue\n",
    "        \n",
    "    # extract the region from the input image\n",
    "    roi = image[y:y + h, x:x + w]\n",
    "    \n",
    "    # convert it from BGR to RGB channel ordering\n",
    "    roi = cv2.cvtColor(roi, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # resize roi to 224x224 (input dimensions required by our pre-trained CNN)\n",
    "    roi = cv2.resize(roi, (224, 224))\n",
    "    \n",
    "    # further preprocess by the ROI\n",
    "    roi = img_to_array(roi)\n",
    "    roi = preprocess_input(roi)\n",
    "    \n",
    "    # update proposals and bounding box lists\n",
    "    rois.append(roi)\n",
    "    locs.append((x, y, w, h))\n",
    "    \n",
    "# convert ROIs to numpy array\n",
    "rois = np.array(rois)\n",
    "\n",
    "# classify each proposal with ResNet\n",
    "print('classifying ROIs...')\n",
    "start = time.time()\n",
    "preds = model.predict(rois)\n",
    "end = time.time()\n",
    "\n",
    "print('classifying ROIs took', end - start, 's')\n",
    "\n",
    "predictions = imagenet_utils.decode_predictions(preds, top = 1)\n",
    "labels = {}\n",
    "\n",
    "# loop over the predictions\n",
    "for (i, p) in enumerate(predictions):\n",
    "    # get the prediction information for the current ROI\n",
    "    imagenetID, label, prob = p[0]\n",
    "\n",
    "    # filter out weak detections\n",
    "    if prob >= MINCONF:\n",
    "        # get bounding box associated with the prediction and\n",
    "        # convert the coordinates\n",
    "        x, y, w, h = locs[i]\n",
    "        box = (x, y, x + w, y + h)\n",
    "        \n",
    "        # get predictions for the label and add the bounding box and probability to the list\n",
    "        L = labels.get(label, [])\n",
    "        L.append((box, prob))\n",
    "        labels[label] = L\n",
    "        \n",
    "# loop over the labels for each of detected objects in the image\n",
    "for label in labels.keys():\n",
    "    # clone the original image\n",
    "    print('showing results for', label)\n",
    "    clone = image.copy()\n",
    "    \n",
    "    # loop over all bounding boxes for the current label\n",
    "    for box, prob in labels[label]:\n",
    "        # draw the bounding box on the image\n",
    "        startX, startY, endX, endY = box\n",
    "        cv2.rectangle(clone, (startX, startY), (endX, endY), (0, 255, 0), 2)\n",
    "    \n",
    "    # show the results before NMS\n",
    "    cv2.imshow('Before NMS', clone)\n",
    "    clone = image.copy()\n",
    "    \n",
    "    # extract the bounding boxes and prediction probabilities\n",
    "    boxes = np.array([p[0] for p in labels[label]])\n",
    "    proba = np.array([p[1] for p in labels[label]])\n",
    "    \n",
    "    # apply NMS\n",
    "    print('applying NMS...')\n",
    "    boxes = non_max_suppression(boxes, proba)\n",
    "    \n",
    "    # loop over all bounding boxes that were kept after applying NMS\n",
    "    for startX, startY, endX, endY in boxes:\n",
    "        \n",
    "        # draw the bounding box and label on the image\n",
    "        cv2.rectangle(clone, (startX, startY), (endX, endY), (0, 255, 0), 2)\n",
    "        \n",
    "        y = startY - 10 if startY - 10 > 10 else startY + 10\n",
    "        \n",
    "        cv2.putText(clone, label, (startX, y), cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 255, 0), 2)\n",
    "        \n",
    "    # show the output after apply non-maxima suppression\n",
    "    cv2.imshow('After NMS', clone)\n",
    "    cv2.waitKey(0)\n",
    "    \n",
    "# close windows\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
