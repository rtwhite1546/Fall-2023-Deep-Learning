{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 10\n",
    "\n",
    "This week, we will cover three new ideas:\n",
    "\n",
    "* The last of the relatively novel CNNs, the ResNet (He et al. 2015, https://arxiv.org/abs/1512.03385), which allows for deeper networks without gradients vanishing by implementing \"skip-connections\" that pass data forward in networks, skipping certain layers.\n",
    "\n",
    "* Ensemble learning -- the idea that we can train several *different* neural networks on a dataset and let them *vote* on inferences.\n",
    "\n",
    "* Transfer learning -- the astounding idea that we can *transfer* a trained neural network's performance on its dataset an *entirely different* dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Lecture 16 - ResNets and Ensemble Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ResNets\n",
    "\n",
    "Let's implement a ResNet here. We will write a function to create residual modules first and then iteratively construct the architecture to save some repetition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications import imagenet_utils\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.layers import add\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet:\n",
    "    def residual_module(data, K, stride, channelsDim, reduce = False, reg = 0.0001, bnEpsilon = 0.00002, bnMomentum = 0.9):\n",
    "        shortcut = data\n",
    "        \n",
    "        # 1x1 CONVs\n",
    "        bn1 = BatchNormalization(axis = channelsDim, epsilon = bnEpsilon, momentum = bnMomentum)(data)\n",
    "        act1 = Activation('relu')(bn1)\n",
    "        conv1 = Conv2D(int(K * 0.25), (1, 1), use_bias = False, kernel_regularizer = l2(reg))(act1)\n",
    "        \n",
    "        # 3x3 CONVs\n",
    "        bn2 = BatchNormalization(axis = channelsDim, epsilon = bnEpsilon, momentum = bnMomentum)(conv1)\n",
    "        act2 = Activation('relu')(bn2)\n",
    "        conv2 = Conv2D(int(K * 0.25), (3, 3), strides = stride, padding = 'same', use_bias = False, kernel_regularizer = l2(reg))(act2)\n",
    "        \n",
    "        # 1x1 CONVs\n",
    "        bn3 = BatchNormalization(axis = channelsDim, epsilon = bnEpsilon, momentum = bnMomentum)(conv2)\n",
    "        act3 = Activation('relu')(bn3)\n",
    "        conv3 = Conv2D(K, (1, 1), use_bias = False, kernel_regularizer = l2(reg))(act3)\n",
    "        \n",
    "        # if we reduce the spatial size, apply a CONV layer to the shortcut\n",
    "        if reduce:\n",
    "            shortcut = Conv2D(K, (1, 1), strides = stride, use_bias = False, kernel_regularizer = l2(reg))(act1)\n",
    "            \n",
    "        # add the shortcut and the final CONV\n",
    "        x = add([conv3, shortcut])\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def build(width, height, depth, classes, stages, filters, reg = 0.0001, bnEpsilon = 0.00002, bnMomentum = 0.9, dataset='cifar'):\n",
    "        inputShape = (height, width, depth)\n",
    "        channelsDim = -1\n",
    "        \n",
    "        if backend.image_data_format() == 'channels_first':\n",
    "            inputShape = (depth, height, width)\n",
    "            channelsDim = 1\n",
    "            \n",
    "        # set the input and apply BN\n",
    "        inputs = Input(shape = inputShape)\n",
    "        x = BatchNormalization(axis = channelsDim, epsilon = bnEpsilon, momentum = bnMomentum)(inputs)\n",
    "        \n",
    "        if dataset == 'cifar':\n",
    "            # apply a single CONV layer\n",
    "            x = Conv2D(filters[0], (3, 3), use_bias = False, padding = 'same',\n",
    "                       kernel_regularizer = l2(reg))(x)\n",
    "        \n",
    "        # loop over the number of stages\n",
    "        for counter in range(0, len(stages)):\n",
    "            # initialize the stride\n",
    "            if counter == 0:\n",
    "                stride = (1, 1)\n",
    "            else:\n",
    "                stride = (2, 2)\n",
    "                    \n",
    "            # apply a residual module to reduce the spatial dimension of the image volume\n",
    "            x = ResNet.residual_module(x, filters[counter + 1], stride, channelsDim, reduce = True, bnEpsilon = bnEpsilon, bnMomentum = bnMomentum)\n",
    "            \n",
    "            # loop over the number of layers in the current stage\n",
    "            for j in range(0, stages[counter] - 1):\n",
    "                # apply a residual module\n",
    "                x = ResNet.residual_module(x, filters[counter + 1], (1, 1), channelsDim, bnEpsilon = bnEpsilon, bnMomentum = bnMomentum)\n",
    "                    \n",
    "        # apply BN -> ACT -> POOL\n",
    "        x = BatchNormalization(axis = channelsDim, epsilon = bnEpsilon, momentum = bnMomentum)(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = AveragePooling2D((8, 8))(x)\n",
    "        \n",
    "        # softmax classifier\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(classes, kernel_regularizer = l2(reg))(x)\n",
    "        x = Activation('softmax')(x)\n",
    "        \n",
    "        # create the model\n",
    "        model = Model(inputs, x, name = 'ResNet')\n",
    "        \n",
    "        # return the model\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet on CIFAR-10\n",
    "\n",
    "Let's test it on CIFAR-10 with data augmentation (width shift, height shift, and horizontal flips)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading CIFAR-10 data...\n",
      "[INFO] compiling model...\n",
      "[INFO] training network...\n",
      "Epoch 1/100\n",
      "390/390 [==============================] - 42s 98ms/step - loss: 1.9638 - accuracy: 0.4437 - val_loss: 1.8061 - val_accuracy: 0.4882\n",
      "Epoch 2/100\n",
      "390/390 [==============================] - 37s 96ms/step - loss: 1.5243 - accuracy: 0.6035 - val_loss: 1.4877 - val_accuracy: 0.6171\n",
      "Epoch 3/100\n",
      "390/390 [==============================] - 37s 96ms/step - loss: 1.2968 - accuracy: 0.6809 - val_loss: 1.2972 - val_accuracy: 0.6824\n",
      "Epoch 4/100\n",
      "390/390 [==============================] - 37s 96ms/step - loss: 1.1377 - accuracy: 0.7348 - val_loss: 1.1144 - val_accuracy: 0.7388\n",
      "Epoch 5/100\n",
      "390/390 [==============================] - 37s 96ms/step - loss: 1.0309 - accuracy: 0.7676 - val_loss: 1.0918 - val_accuracy: 0.7457\n",
      "Epoch 6/100\n",
      "390/390 [==============================] - 37s 96ms/step - loss: 0.9556 - accuracy: 0.7894 - val_loss: 1.1763 - val_accuracy: 0.7397\n",
      "Epoch 7/100\n",
      "390/390 [==============================] - 37s 96ms/step - loss: 0.9075 - accuracy: 0.8065 - val_loss: 1.0274 - val_accuracy: 0.7633\n",
      "Epoch 8/100\n",
      "390/390 [==============================] - 37s 96ms/step - loss: 0.8563 - accuracy: 0.8197 - val_loss: 0.8831 - val_accuracy: 0.8072\n",
      "Epoch 9/100\n",
      "390/390 [==============================] - 37s 96ms/step - loss: 0.8188 - accuracy: 0.8316 - val_loss: 0.9646 - val_accuracy: 0.7859\n",
      "Epoch 10/100\n",
      "390/390 [==============================] - 37s 96ms/step - loss: 0.7890 - accuracy: 0.8382 - val_loss: 0.8580 - val_accuracy: 0.8094\n",
      "Epoch 11/100\n",
      "390/390 [==============================] - 37s 95ms/step - loss: 0.7611 - accuracy: 0.8451 - val_loss: 0.8552 - val_accuracy: 0.8169\n",
      "Epoch 12/100\n",
      "390/390 [==============================] - 37s 95ms/step - loss: 0.7317 - accuracy: 0.8543 - val_loss: 0.9042 - val_accuracy: 0.8014\n",
      "Epoch 13/100\n",
      "390/390 [==============================] - 37s 96ms/step - loss: 0.7089 - accuracy: 0.8590 - val_loss: 0.7920 - val_accuracy: 0.8341\n",
      "Epoch 14/100\n",
      "390/390 [==============================] - 37s 96ms/step - loss: 0.6913 - accuracy: 0.8632 - val_loss: 0.8019 - val_accuracy: 0.8307\n",
      "Epoch 15/100\n",
      "390/390 [==============================] - 37s 96ms/step - loss: 0.6760 - accuracy: 0.8667 - val_loss: 0.7615 - val_accuracy: 0.8424\n",
      "Epoch 16/100\n",
      "390/390 [==============================] - 37s 96ms/step - loss: 0.6597 - accuracy: 0.8729 - val_loss: 0.7833 - val_accuracy: 0.8296\n",
      "Epoch 17/100\n",
      "390/390 [==============================] - 37s 96ms/step - loss: 0.6466 - accuracy: 0.8736 - val_loss: 0.7327 - val_accuracy: 0.8459\n",
      "Epoch 18/100\n",
      "390/390 [==============================] - 37s 96ms/step - loss: 0.6282 - accuracy: 0.8790 - val_loss: 0.6988 - val_accuracy: 0.8581\n",
      "Epoch 19/100\n",
      "390/390 [==============================] - 37s 96ms/step - loss: 0.6195 - accuracy: 0.8829 - val_loss: 0.8093 - val_accuracy: 0.8221\n",
      "Epoch 20/100\n",
      "390/390 [==============================] - 37s 96ms/step - loss: 0.6132 - accuracy: 0.8845 - val_loss: 0.7224 - val_accuracy: 0.8490\n",
      "Epoch 21/100\n",
      "390/390 [==============================] - 37s 96ms/step - loss: 0.6000 - accuracy: 0.8858 - val_loss: 0.7743 - val_accuracy: 0.8336\n",
      "Epoch 22/100\n",
      "390/390 [==============================] - 37s 96ms/step - loss: 0.5928 - accuracy: 0.8893 - val_loss: 0.7327 - val_accuracy: 0.8516\n",
      "Epoch 23/100\n",
      "390/390 [==============================] - 37s 96ms/step - loss: 0.5782 - accuracy: 0.8936 - val_loss: 0.7149 - val_accuracy: 0.8589\n",
      "Epoch 24/100\n",
      "390/390 [==============================] - 37s 96ms/step - loss: 0.5719 - accuracy: 0.8952 - val_loss: 0.7144 - val_accuracy: 0.8608\n",
      "Epoch 25/100\n",
      "390/390 [==============================] - 37s 96ms/step - loss: 0.5625 - accuracy: 0.8985 - val_loss: 0.7383 - val_accuracy: 0.8481\n",
      "Epoch 26/100\n",
      "390/390 [==============================] - 37s 96ms/step - loss: 0.5605 - accuracy: 0.8984 - val_loss: 0.7001 - val_accuracy: 0.8567\n",
      "Epoch 27/100\n",
      "390/390 [==============================] - 37s 96ms/step - loss: 0.5521 - accuracy: 0.9006 - val_loss: 0.6812 - val_accuracy: 0.8586\n",
      "Epoch 28/100\n",
      "390/390 [==============================] - 37s 96ms/step - loss: 0.5442 - accuracy: 0.9026 - val_loss: 0.6610 - val_accuracy: 0.8641\n",
      "Epoch 29/100\n",
      "390/390 [==============================] - 37s 96ms/step - loss: 0.5365 - accuracy: 0.9048 - val_loss: 0.6776 - val_accuracy: 0.8625\n",
      "Epoch 30/100\n",
      "390/390 [==============================] - 37s 96ms/step - loss: 0.5338 - accuracy: 0.9060 - val_loss: 0.6312 - val_accuracy: 0.8731\n",
      "Epoch 31/100\n",
      "390/390 [==============================] - 37s 95ms/step - loss: 0.5282 - accuracy: 0.9072 - val_loss: 0.6566 - val_accuracy: 0.8666\n",
      "Epoch 32/100\n",
      "390/390 [==============================] - 37s 96ms/step - loss: 0.5201 - accuracy: 0.9086 - val_loss: 0.6101 - val_accuracy: 0.8791\n",
      "Epoch 33/100\n",
      "390/390 [==============================] - 37s 96ms/step - loss: 0.5215 - accuracy: 0.9099 - val_loss: 0.6596 - val_accuracy: 0.8685\n",
      "Epoch 34/100\n",
      "390/390 [==============================] - 37s 96ms/step - loss: 0.5180 - accuracy: 0.9085 - val_loss: 0.6282 - val_accuracy: 0.8760\n",
      "Epoch 35/100\n",
      "390/390 [==============================] - 38s 96ms/step - loss: 0.5093 - accuracy: 0.9117 - val_loss: 0.7029 - val_accuracy: 0.8571\n",
      "Epoch 36/100\n",
      "390/390 [==============================] - 37s 96ms/step - loss: 0.5009 - accuracy: 0.9153 - val_loss: 0.6686 - val_accuracy: 0.8654\n",
      "Epoch 37/100\n",
      "390/390 [==============================] - 37s 96ms/step - loss: 0.5028 - accuracy: 0.9149 - val_loss: 0.6939 - val_accuracy: 0.8605\n",
      "Epoch 38/100\n",
      "390/390 [==============================] - 37s 96ms/step - loss: 0.5014 - accuracy: 0.9137 - val_loss: 0.6608 - val_accuracy: 0.8658\n",
      "Epoch 39/100\n",
      "390/390 [==============================] - 37s 96ms/step - loss: 0.4885 - accuracy: 0.9171 - val_loss: 0.6702 - val_accuracy: 0.8611\n",
      "Epoch 40/100\n",
      "390/390 [==============================] - 37s 96ms/step - loss: 0.4912 - accuracy: 0.9163 - val_loss: 0.5860 - val_accuracy: 0.8881\n",
      "Epoch 41/100\n",
      "390/390 [==============================] - 37s 95ms/step - loss: 0.4864 - accuracy: 0.9180 - val_loss: 0.6942 - val_accuracy: 0.8585\n",
      "Epoch 42/100\n",
      "390/390 [==============================] - 37s 96ms/step - loss: 0.4815 - accuracy: 0.9197 - val_loss: 0.6686 - val_accuracy: 0.8660\n",
      "Epoch 43/100\n",
      "390/390 [==============================] - 37s 96ms/step - loss: 0.4812 - accuracy: 0.9210 - val_loss: 0.6063 - val_accuracy: 0.8862\n",
      "Epoch 44/100\n",
      "390/390 [==============================] - 37s 96ms/step - loss: 0.4780 - accuracy: 0.9207 - val_loss: 0.6511 - val_accuracy: 0.8681\n",
      "Epoch 45/100\n",
      "390/390 [==============================] - 37s 96ms/step - loss: 0.4785 - accuracy: 0.9200 - val_loss: 0.5954 - val_accuracy: 0.8841\n",
      "Epoch 46/100\n",
      "390/390 [==============================] - 37s 95ms/step - loss: 0.4742 - accuracy: 0.9208 - val_loss: 0.6055 - val_accuracy: 0.8825\n",
      "Epoch 47/100\n",
      "390/390 [==============================] - 37s 95ms/step - loss: 0.4695 - accuracy: 0.9218 - val_loss: 0.6385 - val_accuracy: 0.8708\n",
      "Epoch 48/100\n",
      "390/390 [==============================] - 37s 95ms/step - loss: 0.4687 - accuracy: 0.9232 - val_loss: 0.6528 - val_accuracy: 0.8701\n",
      "Epoch 49/100\n",
      "390/390 [==============================] - 37s 96ms/step - loss: 0.4622 - accuracy: 0.9257 - val_loss: 0.6620 - val_accuracy: 0.8686\n",
      "Epoch 50/100\n",
      "390/390 [==============================] - 37s 95ms/step - loss: 0.4640 - accuracy: 0.9260 - val_loss: 0.6068 - val_accuracy: 0.8813\n",
      "Epoch 51/100\n",
      "390/390 [==============================] - 37s 96ms/step - loss: 0.4579 - accuracy: 0.9259 - val_loss: 0.6359 - val_accuracy: 0.8786\n",
      "Epoch 52/100\n",
      "390/390 [==============================] - 37s 95ms/step - loss: 0.4601 - accuracy: 0.9255 - val_loss: 0.6292 - val_accuracy: 0.8789\n",
      "Epoch 53/100\n",
      "390/390 [==============================] - 37s 95ms/step - loss: 0.4567 - accuracy: 0.9271 - val_loss: 0.6588 - val_accuracy: 0.8673\n",
      "Epoch 54/100\n",
      "390/390 [==============================] - 37s 95ms/step - loss: 0.4581 - accuracy: 0.9259 - val_loss: 0.6177 - val_accuracy: 0.8799\n",
      "Epoch 55/100\n",
      "390/390 [==============================] - 37s 95ms/step - loss: 0.4504 - accuracy: 0.9290 - val_loss: 0.6241 - val_accuracy: 0.8778\n",
      "Epoch 56/100\n",
      "390/390 [==============================] - 37s 95ms/step - loss: 0.4541 - accuracy: 0.9277 - val_loss: 0.6118 - val_accuracy: 0.8797\n",
      "Epoch 57/100\n",
      "390/390 [==============================] - 37s 95ms/step - loss: 0.4517 - accuracy: 0.9274 - val_loss: 0.6225 - val_accuracy: 0.8806\n",
      "Epoch 58/100\n",
      "390/390 [==============================] - 37s 95ms/step - loss: 0.4476 - accuracy: 0.9298 - val_loss: 0.6232 - val_accuracy: 0.8801\n",
      "Epoch 59/100\n",
      "390/390 [==============================] - 37s 95ms/step - loss: 0.4421 - accuracy: 0.9319 - val_loss: 0.6381 - val_accuracy: 0.8755\n",
      "Epoch 60/100\n",
      "390/390 [==============================] - 37s 95ms/step - loss: 0.4465 - accuracy: 0.9290 - val_loss: 0.6128 - val_accuracy: 0.8821\n",
      "Epoch 61/100\n",
      "390/390 [==============================] - 37s 95ms/step - loss: 0.4386 - accuracy: 0.9313 - val_loss: 0.5663 - val_accuracy: 0.8967\n",
      "Epoch 62/100\n",
      "390/390 [==============================] - 37s 95ms/step - loss: 0.4380 - accuracy: 0.9317 - val_loss: 0.6524 - val_accuracy: 0.8689\n",
      "Epoch 63/100\n",
      "390/390 [==============================] - 37s 95ms/step - loss: 0.4386 - accuracy: 0.9292 - val_loss: 0.5687 - val_accuracy: 0.8928\n",
      "Epoch 64/100\n",
      "390/390 [==============================] - 37s 95ms/step - loss: 0.4334 - accuracy: 0.9329 - val_loss: 0.6357 - val_accuracy: 0.8709\n",
      "Epoch 65/100\n",
      "390/390 [==============================] - 37s 95ms/step - loss: 0.4341 - accuracy: 0.9324 - val_loss: 0.6011 - val_accuracy: 0.8805\n",
      "Epoch 66/100\n",
      "390/390 [==============================] - 37s 95ms/step - loss: 0.4313 - accuracy: 0.9330 - val_loss: 0.5804 - val_accuracy: 0.8914\n",
      "Epoch 67/100\n",
      "390/390 [==============================] - 37s 95ms/step - loss: 0.4289 - accuracy: 0.9334 - val_loss: 0.5818 - val_accuracy: 0.8912\n",
      "Epoch 68/100\n",
      "390/390 [==============================] - 37s 95ms/step - loss: 0.4303 - accuracy: 0.9335 - val_loss: 0.5765 - val_accuracy: 0.8908\n",
      "Epoch 69/100\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 0.4262 - accuracy: 0.9344 - val_loss: 0.6035 - val_accuracy: 0.8833\n",
      "Epoch 70/100\n",
      "390/390 [==============================] - 37s 95ms/step - loss: 0.4271 - accuracy: 0.9342 - val_loss: 0.6209 - val_accuracy: 0.8820\n",
      "Epoch 71/100\n",
      "390/390 [==============================] - 37s 95ms/step - loss: 0.4228 - accuracy: 0.9353 - val_loss: 0.5658 - val_accuracy: 0.8971\n",
      "Epoch 72/100\n",
      "390/390 [==============================] - 37s 95ms/step - loss: 0.4228 - accuracy: 0.9353 - val_loss: 0.5717 - val_accuracy: 0.8927\n",
      "Epoch 73/100\n",
      "390/390 [==============================] - 37s 95ms/step - loss: 0.4200 - accuracy: 0.9367 - val_loss: 0.6212 - val_accuracy: 0.8808\n",
      "Epoch 74/100\n",
      "390/390 [==============================] - 37s 95ms/step - loss: 0.4204 - accuracy: 0.9359 - val_loss: 0.5859 - val_accuracy: 0.8862\n",
      "Epoch 75/100\n",
      "390/390 [==============================] - 37s 95ms/step - loss: 0.4192 - accuracy: 0.9361 - val_loss: 0.6514 - val_accuracy: 0.8693\n",
      "Epoch 76/100\n",
      "390/390 [==============================] - 37s 95ms/step - loss: 0.4216 - accuracy: 0.9352 - val_loss: 0.5920 - val_accuracy: 0.8880\n",
      "Epoch 77/100\n",
      "390/390 [==============================] - 37s 95ms/step - loss: 0.4150 - accuracy: 0.9374 - val_loss: 0.6148 - val_accuracy: 0.8838\n",
      "Epoch 78/100\n",
      "390/390 [==============================] - 37s 95ms/step - loss: 0.4190 - accuracy: 0.9363 - val_loss: 0.6574 - val_accuracy: 0.8739\n",
      "Epoch 79/100\n",
      "390/390 [==============================] - 37s 95ms/step - loss: 0.4125 - accuracy: 0.9371 - val_loss: 0.5548 - val_accuracy: 0.8973\n",
      "Epoch 80/100\n",
      "390/390 [==============================] - 37s 95ms/step - loss: 0.4153 - accuracy: 0.9377 - val_loss: 0.5662 - val_accuracy: 0.8978\n",
      "Epoch 81/100\n",
      "390/390 [==============================] - 37s 95ms/step - loss: 0.4060 - accuracy: 0.9400 - val_loss: 0.5702 - val_accuracy: 0.8946\n",
      "Epoch 82/100\n",
      "390/390 [==============================] - 37s 95ms/step - loss: 0.4084 - accuracy: 0.9385 - val_loss: 0.6241 - val_accuracy: 0.8805\n",
      "Epoch 83/100\n",
      "390/390 [==============================] - 37s 95ms/step - loss: 0.4105 - accuracy: 0.9379 - val_loss: 0.5645 - val_accuracy: 0.8943\n",
      "Epoch 84/100\n",
      "390/390 [==============================] - 37s 95ms/step - loss: 0.4064 - accuracy: 0.9395 - val_loss: 0.5544 - val_accuracy: 0.8965\n",
      "Epoch 85/100\n",
      "390/390 [==============================] - 37s 95ms/step - loss: 0.4096 - accuracy: 0.9386 - val_loss: 0.7560 - val_accuracy: 0.8477\n",
      "Epoch 86/100\n",
      "390/390 [==============================] - 37s 95ms/step - loss: 0.4010 - accuracy: 0.9413 - val_loss: 0.6098 - val_accuracy: 0.8828\n",
      "Epoch 87/100\n",
      "390/390 [==============================] - 37s 95ms/step - loss: 0.4045 - accuracy: 0.9400 - val_loss: 0.6237 - val_accuracy: 0.8804\n",
      "Epoch 88/100\n",
      "390/390 [==============================] - 37s 95ms/step - loss: 0.3975 - accuracy: 0.9423 - val_loss: 0.5761 - val_accuracy: 0.8893\n",
      "Epoch 89/100\n",
      "390/390 [==============================] - 37s 95ms/step - loss: 0.4064 - accuracy: 0.9383 - val_loss: 0.6007 - val_accuracy: 0.8837\n",
      "Epoch 90/100\n",
      "390/390 [==============================] - 37s 95ms/step - loss: 0.3953 - accuracy: 0.9414 - val_loss: 0.5546 - val_accuracy: 0.8987\n",
      "Epoch 91/100\n",
      "390/390 [==============================] - 37s 95ms/step - loss: 0.4026 - accuracy: 0.9395 - val_loss: 0.6594 - val_accuracy: 0.8751\n",
      "Epoch 92/100\n",
      "390/390 [==============================] - 37s 95ms/step - loss: 0.4007 - accuracy: 0.9402 - val_loss: 0.6257 - val_accuracy: 0.8848\n",
      "Epoch 93/100\n",
      "390/390 [==============================] - 37s 95ms/step - loss: 0.3929 - accuracy: 0.9423 - val_loss: 0.6212 - val_accuracy: 0.8794\n",
      "Epoch 94/100\n",
      "390/390 [==============================] - 37s 95ms/step - loss: 0.3973 - accuracy: 0.9410 - val_loss: 0.5956 - val_accuracy: 0.8839\n",
      "Epoch 95/100\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 0.3907 - accuracy: 0.9442 - val_loss: 0.5385 - val_accuracy: 0.9010\n",
      "Epoch 96/100\n",
      "390/390 [==============================] - 37s 95ms/step - loss: 0.3952 - accuracy: 0.9426 - val_loss: 0.5819 - val_accuracy: 0.8946\n",
      "Epoch 97/100\n",
      "390/390 [==============================] - 37s 95ms/step - loss: 0.3914 - accuracy: 0.9426 - val_loss: 0.5627 - val_accuracy: 0.8976\n",
      "Epoch 98/100\n",
      "390/390 [==============================] - 37s 95ms/step - loss: 0.3960 - accuracy: 0.9408 - val_loss: 0.5718 - val_accuracy: 0.8864\n",
      "Epoch 99/100\n",
      "390/390 [==============================] - 37s 95ms/step - loss: 0.3882 - accuracy: 0.9429 - val_loss: 0.5484 - val_accuracy: 0.9023\n",
      "Epoch 100/100\n",
      "390/390 [==============================] - 37s 95ms/step - loss: 0.3872 - accuracy: 0.9450 - val_loss: 0.5731 - val_accuracy: 0.8941\n",
      "\n",
      " Test accuracy\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8943    0.9140    0.9041      1000\n",
      "           1     0.9049    0.9800    0.9410      1000\n",
      "           2     0.8597    0.8700    0.8648      1000\n",
      "           3     0.8248    0.7860    0.8049      1000\n",
      "           4     0.9140    0.8710    0.8920      1000\n",
      "           5     0.9278    0.7710    0.8422      1000\n",
      "           6     0.8468    0.9620    0.9007      1000\n",
      "           7     0.8924    0.9450    0.9179      1000\n",
      "           8     0.9698    0.8990    0.9331      1000\n",
      "           9     0.9209    0.9430    0.9318      1000\n",
      "\n",
      "    accuracy                         0.8941     10000\n",
      "   macro avg     0.8955    0.8941    0.8932     10000\n",
      "weighted avg     0.8955    0.8941    0.8932     10000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1eac4b9f490>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the training and testing data\n",
    "print(\"[INFO] loading CIFAR-10 data...\")\n",
    "((trainX, trainY), (testX, testY)) = cifar10.load_data()\n",
    "trainX = trainX.astype(\"float\")\n",
    "testX = testX.astype(\"float\")\n",
    "\n",
    "# apply mean subtraction to the data\n",
    "mean = np.mean(trainX, axis=0)\n",
    "trainX -= mean\n",
    "testX -= mean\n",
    "\n",
    "# convert the labels from integers to one-hot vectors\n",
    "trainY = to_categorical(trainY)\n",
    "testY = to_categorical(testY)\n",
    "\n",
    "# create a data augmentation pipeline\n",
    "aug = ImageDataGenerator(width_shift_range = 0.1,\n",
    "                         height_shift_range = 0.1,\n",
    "                         horizontal_flip = True,\n",
    "                         fill_mode=\"nearest\")\n",
    "\n",
    "print(\"[INFO] compiling model...\")\n",
    "opt = Adam()\n",
    "model = ResNet.build(32, 32, 3, 10, (9, 9, 9), (64, 64, 128, 256), reg=0.0005)\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics=['accuracy'])\n",
    "\n",
    "print(\"[INFO] training network...\")\n",
    "H = model.fit(aug.flow(trainX, trainY, batch_size=128),\n",
    "              validation_data=(testX, testY),\n",
    "              steps_per_epoch=len(trainX) // 128,\n",
    "              epochs=100,\n",
    "              verbose=1)\n",
    "\n",
    "# print a classification report\n",
    "print('\\n Test accuracy')\n",
    "predictedY = model.predict(testX)\n",
    "predictedY = predictedY.argmax(axis=1)\n",
    "testY = testY.argmax(axis=1)\n",
    "print(classification_report(testY, predictedY, digits=4))\n",
    "\n",
    "# plot the training loss and accuracy\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, 100), H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, 100), H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0, 100), H.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0, 100), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet Experiment 2: Data Augmentation and a Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading CIFAR-10 data...\n",
      "[INFO] compiling model...\n",
      "[INFO] training network...\n",
      "Epoch 1/100\n",
      "390/390 [==============================] - 40s 95ms/step - loss: 2.2019 - accuracy: 0.3508 - val_loss: 1.9210 - val_accuracy: 0.4652\n",
      "Epoch 2/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 1.8316 - accuracy: 0.4977 - val_loss: 1.8254 - val_accuracy: 0.5008\n",
      "Epoch 3/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 1.6504 - accuracy: 0.5730 - val_loss: 1.5737 - val_accuracy: 0.6063\n",
      "Epoch 4/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 1.5067 - accuracy: 0.6254 - val_loss: 1.5280 - val_accuracy: 0.6260\n",
      "Epoch 5/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 1.3890 - accuracy: 0.6682 - val_loss: 1.4121 - val_accuracy: 0.6626\n",
      "Epoch 6/100\n",
      "390/390 [==============================] - 36s 93ms/step - loss: 1.2933 - accuracy: 0.7028 - val_loss: 1.2957 - val_accuracy: 0.7061\n",
      "Epoch 7/100\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.2154 - accuracy: 0.7304 - val_loss: 1.2978 - val_accuracy: 0.7068\n",
      "Epoch 8/100\n",
      "390/390 [==============================] - 36s 93ms/step - loss: 1.1456 - accuracy: 0.7551 - val_loss: 1.2954 - val_accuracy: 0.7139\n",
      "Epoch 9/100\n",
      "390/390 [==============================] - 36s 93ms/step - loss: 1.0925 - accuracy: 0.7709 - val_loss: 1.1684 - val_accuracy: 0.7489\n",
      "Epoch 10/100\n",
      "390/390 [==============================] - 36s 93ms/step - loss: 1.0394 - accuracy: 0.7888 - val_loss: 1.1214 - val_accuracy: 0.7725\n",
      "Epoch 11/100\n",
      "390/390 [==============================] - 36s 93ms/step - loss: 1.0007 - accuracy: 0.8041 - val_loss: 1.0769 - val_accuracy: 0.7813\n",
      "Epoch 12/100\n",
      "390/390 [==============================] - 36s 93ms/step - loss: 0.9631 - accuracy: 0.8157 - val_loss: 1.0973 - val_accuracy: 0.7784\n",
      "Epoch 13/100\n",
      "390/390 [==============================] - 36s 93ms/step - loss: 0.9347 - accuracy: 0.8237 - val_loss: 1.1198 - val_accuracy: 0.7667\n",
      "Epoch 14/100\n",
      "390/390 [==============================] - 36s 93ms/step - loss: 0.9057 - accuracy: 0.8334 - val_loss: 1.0215 - val_accuracy: 0.8005\n",
      "Epoch 15/100\n",
      "390/390 [==============================] - 36s 93ms/step - loss: 0.8829 - accuracy: 0.8389 - val_loss: 0.9596 - val_accuracy: 0.8116\n",
      "Epoch 16/100\n",
      "390/390 [==============================] - 36s 93ms/step - loss: 0.8564 - accuracy: 0.8472 - val_loss: 0.9913 - val_accuracy: 0.8027\n",
      "Epoch 17/100\n",
      "390/390 [==============================] - 36s 93ms/step - loss: 0.8353 - accuracy: 0.8525 - val_loss: 1.0488 - val_accuracy: 0.7860\n",
      "Epoch 18/100\n",
      "390/390 [==============================] - 36s 93ms/step - loss: 0.8166 - accuracy: 0.8593 - val_loss: 0.9879 - val_accuracy: 0.8114\n",
      "Epoch 19/100\n",
      "390/390 [==============================] - 36s 93ms/step - loss: 0.7938 - accuracy: 0.8657 - val_loss: 0.9423 - val_accuracy: 0.8178\n",
      "Epoch 20/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.7764 - accuracy: 0.8701 - val_loss: 0.9272 - val_accuracy: 0.8294\n",
      "Epoch 21/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.7626 - accuracy: 0.8745 - val_loss: 0.9041 - val_accuracy: 0.8318\n",
      "Epoch 22/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.7474 - accuracy: 0.8765 - val_loss: 0.9552 - val_accuracy: 0.8207\n",
      "Epoch 23/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.7316 - accuracy: 0.8817 - val_loss: 0.9334 - val_accuracy: 0.8206\n",
      "Epoch 24/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.7163 - accuracy: 0.8863 - val_loss: 0.9202 - val_accuracy: 0.8259\n",
      "Epoch 25/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.7014 - accuracy: 0.8886 - val_loss: 0.8551 - val_accuracy: 0.8440\n",
      "Epoch 26/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.6933 - accuracy: 0.8916 - val_loss: 0.8426 - val_accuracy: 0.8456\n",
      "Epoch 27/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.6781 - accuracy: 0.8961 - val_loss: 0.8647 - val_accuracy: 0.8449\n",
      "Epoch 28/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.6622 - accuracy: 0.9013 - val_loss: 0.8638 - val_accuracy: 0.8418\n",
      "Epoch 29/100\n",
      "390/390 [==============================] - 36s 93ms/step - loss: 0.6540 - accuracy: 0.9027 - val_loss: 0.9076 - val_accuracy: 0.8292\n",
      "Epoch 30/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.6453 - accuracy: 0.9049 - val_loss: 0.9053 - val_accuracy: 0.8376\n",
      "Epoch 31/100\n",
      "390/390 [==============================] - 36s 93ms/step - loss: 0.6317 - accuracy: 0.9085 - val_loss: 0.8888 - val_accuracy: 0.8375\n",
      "Epoch 32/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.6246 - accuracy: 0.9104 - val_loss: 0.8690 - val_accuracy: 0.8371\n",
      "Epoch 33/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.6090 - accuracy: 0.9149 - val_loss: 0.8391 - val_accuracy: 0.8506\n",
      "Epoch 34/100\n",
      "390/390 [==============================] - 36s 93ms/step - loss: 0.6025 - accuracy: 0.9158 - val_loss: 0.8294 - val_accuracy: 0.8493\n",
      "Epoch 35/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.5978 - accuracy: 0.9176 - val_loss: 0.8355 - val_accuracy: 0.8494\n",
      "Epoch 36/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.5819 - accuracy: 0.9229 - val_loss: 0.7912 - val_accuracy: 0.8645\n",
      "Epoch 37/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.5739 - accuracy: 0.9233 - val_loss: 0.8411 - val_accuracy: 0.8468\n",
      "Epoch 38/100\n",
      "390/390 [==============================] - 36s 93ms/step - loss: 0.5666 - accuracy: 0.9251 - val_loss: 0.8465 - val_accuracy: 0.8476\n",
      "Epoch 39/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.5588 - accuracy: 0.9271 - val_loss: 0.8375 - val_accuracy: 0.8521\n",
      "Epoch 40/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.5524 - accuracy: 0.9290 - val_loss: 0.8251 - val_accuracy: 0.8538\n",
      "Epoch 41/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.5461 - accuracy: 0.9309 - val_loss: 0.8270 - val_accuracy: 0.8547\n",
      "Epoch 42/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.5378 - accuracy: 0.9322 - val_loss: 0.7806 - val_accuracy: 0.8658\n",
      "Epoch 43/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.5290 - accuracy: 0.9350 - val_loss: 0.7276 - val_accuracy: 0.8805\n",
      "Epoch 44/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.5199 - accuracy: 0.9360 - val_loss: 0.7974 - val_accuracy: 0.8561\n",
      "Epoch 45/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.5144 - accuracy: 0.9391 - val_loss: 0.7703 - val_accuracy: 0.8661\n",
      "Epoch 46/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.5066 - accuracy: 0.9402 - val_loss: 0.7710 - val_accuracy: 0.8718\n",
      "Epoch 47/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.5023 - accuracy: 0.9409 - val_loss: 0.7460 - val_accuracy: 0.8739\n",
      "Epoch 48/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.4927 - accuracy: 0.9447 - val_loss: 0.7755 - val_accuracy: 0.8669\n",
      "Epoch 49/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.4872 - accuracy: 0.9451 - val_loss: 0.7575 - val_accuracy: 0.8719\n",
      "Epoch 50/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.4805 - accuracy: 0.9470 - val_loss: 0.7398 - val_accuracy: 0.8741\n",
      "Epoch 51/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.4734 - accuracy: 0.9492 - val_loss: 0.8352 - val_accuracy: 0.8546\n",
      "Epoch 52/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.4667 - accuracy: 0.9513 - val_loss: 0.7471 - val_accuracy: 0.8733\n",
      "Epoch 53/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.4617 - accuracy: 0.9523 - val_loss: 0.7669 - val_accuracy: 0.8690\n",
      "Epoch 54/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.4553 - accuracy: 0.9532 - val_loss: 0.8164 - val_accuracy: 0.8586\n",
      "Epoch 55/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.4521 - accuracy: 0.9547 - val_loss: 0.7191 - val_accuracy: 0.8807\n",
      "Epoch 56/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.4456 - accuracy: 0.9568 - val_loss: 0.7629 - val_accuracy: 0.8710\n",
      "Epoch 57/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.4395 - accuracy: 0.9578 - val_loss: 0.7153 - val_accuracy: 0.8848\n",
      "Epoch 58/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.4343 - accuracy: 0.9592 - val_loss: 0.7587 - val_accuracy: 0.8720\n",
      "Epoch 59/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.4323 - accuracy: 0.9584 - val_loss: 0.7415 - val_accuracy: 0.8803\n",
      "Epoch 60/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.4288 - accuracy: 0.9596 - val_loss: 0.7347 - val_accuracy: 0.8798\n",
      "Epoch 61/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.4242 - accuracy: 0.9610 - val_loss: 0.7519 - val_accuracy: 0.8761\n",
      "Epoch 62/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.4144 - accuracy: 0.9634 - val_loss: 0.7445 - val_accuracy: 0.8759\n",
      "Epoch 63/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.4104 - accuracy: 0.9657 - val_loss: 0.7560 - val_accuracy: 0.8761\n",
      "Epoch 64/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.4106 - accuracy: 0.9640 - val_loss: 0.7491 - val_accuracy: 0.8788\n",
      "Epoch 65/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.4020 - accuracy: 0.9667 - val_loss: 0.6902 - val_accuracy: 0.8908\n",
      "Epoch 66/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.3943 - accuracy: 0.9695 - val_loss: 0.7275 - val_accuracy: 0.8822\n",
      "Epoch 67/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.3909 - accuracy: 0.9707 - val_loss: 0.7505 - val_accuracy: 0.8807\n",
      "Epoch 68/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.3866 - accuracy: 0.9721 - val_loss: 0.7327 - val_accuracy: 0.8843\n",
      "Epoch 69/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.3858 - accuracy: 0.9712 - val_loss: 0.7735 - val_accuracy: 0.8732\n",
      "Epoch 70/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.3797 - accuracy: 0.9740 - val_loss: 0.7278 - val_accuracy: 0.8857\n",
      "Epoch 71/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.3750 - accuracy: 0.9742 - val_loss: 0.7435 - val_accuracy: 0.8805\n",
      "Epoch 72/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.3728 - accuracy: 0.9746 - val_loss: 0.7191 - val_accuracy: 0.8870\n",
      "Epoch 73/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.3654 - accuracy: 0.9767 - val_loss: 0.7221 - val_accuracy: 0.8866\n",
      "Epoch 74/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.3640 - accuracy: 0.9770 - val_loss: 0.7246 - val_accuracy: 0.8873\n",
      "Epoch 75/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.3568 - accuracy: 0.9800 - val_loss: 0.7439 - val_accuracy: 0.8830\n",
      "Epoch 76/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.3551 - accuracy: 0.9794 - val_loss: 0.7062 - val_accuracy: 0.8892\n",
      "Epoch 77/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.3530 - accuracy: 0.9802 - val_loss: 0.7237 - val_accuracy: 0.8844\n",
      "Epoch 78/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.3489 - accuracy: 0.9816 - val_loss: 0.7211 - val_accuracy: 0.8897\n",
      "Epoch 79/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.3446 - accuracy: 0.9822 - val_loss: 0.7670 - val_accuracy: 0.8792\n",
      "Epoch 80/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.3438 - accuracy: 0.9822 - val_loss: 0.7402 - val_accuracy: 0.8834\n",
      "Epoch 81/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.3368 - accuracy: 0.9845 - val_loss: 0.7143 - val_accuracy: 0.8904\n",
      "Epoch 82/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.3348 - accuracy: 0.9855 - val_loss: 0.7017 - val_accuracy: 0.8950\n",
      "Epoch 83/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.3319 - accuracy: 0.9859 - val_loss: 0.7033 - val_accuracy: 0.8952\n",
      "Epoch 84/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.3299 - accuracy: 0.9863 - val_loss: 0.7223 - val_accuracy: 0.8896\n",
      "Epoch 85/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.3290 - accuracy: 0.9868 - val_loss: 0.7309 - val_accuracy: 0.8908\n",
      "Epoch 86/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.3272 - accuracy: 0.9870 - val_loss: 0.7258 - val_accuracy: 0.8897\n",
      "Epoch 87/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.3227 - accuracy: 0.9881 - val_loss: 0.7169 - val_accuracy: 0.8933\n",
      "Epoch 88/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.3208 - accuracy: 0.9888 - val_loss: 0.7136 - val_accuracy: 0.8933\n",
      "Epoch 89/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.3190 - accuracy: 0.9895 - val_loss: 0.7132 - val_accuracy: 0.8938\n",
      "Epoch 90/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.3166 - accuracy: 0.9898 - val_loss: 0.7032 - val_accuracy: 0.8961\n",
      "Epoch 91/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.3145 - accuracy: 0.9908 - val_loss: 0.7078 - val_accuracy: 0.8947\n",
      "Epoch 92/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.3138 - accuracy: 0.9910 - val_loss: 0.7032 - val_accuracy: 0.8964\n",
      "Epoch 93/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.3120 - accuracy: 0.9912 - val_loss: 0.7035 - val_accuracy: 0.8967\n",
      "Epoch 94/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.3111 - accuracy: 0.9919 - val_loss: 0.7051 - val_accuracy: 0.8972\n",
      "Epoch 95/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.3094 - accuracy: 0.9920 - val_loss: 0.7016 - val_accuracy: 0.8955\n",
      "Epoch 96/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.3090 - accuracy: 0.9919 - val_loss: 0.7047 - val_accuracy: 0.8959\n",
      "Epoch 97/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.3070 - accuracy: 0.9925 - val_loss: 0.6979 - val_accuracy: 0.8982\n",
      "Epoch 98/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.3055 - accuracy: 0.9936 - val_loss: 0.6964 - val_accuracy: 0.8970\n",
      "Epoch 99/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.3064 - accuracy: 0.9929 - val_loss: 0.7015 - val_accuracy: 0.8957\n",
      "Epoch 100/100\n",
      "390/390 [==============================] - 36s 92ms/step - loss: 0.3045 - accuracy: 0.9938 - val_loss: 0.7078 - val_accuracy: 0.8951\n",
      "\n",
      " Test accuracy\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9137    0.9100    0.9118      1000\n",
      "           1     0.9403    0.9610    0.9505      1000\n",
      "           2     0.8655    0.8560    0.8607      1000\n",
      "           3     0.8058    0.7800    0.7927      1000\n",
      "           4     0.8754    0.8850    0.8802      1000\n",
      "           5     0.8625    0.8340    0.8480      1000\n",
      "           6     0.8614    0.9510    0.9040      1000\n",
      "           7     0.9568    0.9090    0.9323      1000\n",
      "           8     0.9518    0.9280    0.9397      1000\n",
      "           9     0.9204    0.9370    0.9286      1000\n",
      "\n",
      "    accuracy                         0.8951     10000\n",
      "   macro avg     0.8954    0.8951    0.8949     10000\n",
      "weighted avg     0.8954    0.8951    0.8949     10000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1eb37ddcc40>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "numberOfEpochs = 100\n",
    "initialLearningRate = 0.1\n",
    "\n",
    "# load the training and testing data\n",
    "print(\"[INFO] loading CIFAR-10 data...\")\n",
    "((trainX, trainY), (testX, testY)) = cifar10.load_data()\n",
    "trainX = trainX.astype(\"float\")\n",
    "testX = testX.astype(\"float\")\n",
    "\n",
    "# apply mean subtraction to the data\n",
    "mean = np.mean(trainX, axis=0)\n",
    "trainX -= mean\n",
    "testX -= mean\n",
    "\n",
    "# convert the labels from integers to one-hot vectors\n",
    "trainY = to_categorical(trainY)\n",
    "testY = to_categorical(testY)\n",
    "\n",
    "aug = ImageDataGenerator(width_shift_range = 0.1, height_shift_range = 0.1, horizontal_flip = True, fill_mode=\"nearest\")\n",
    "\n",
    "print(\"[INFO] compiling model...\")\n",
    "opt = SGD(lr = initialLearningRate)\n",
    "model = ResNet.build(32, 32, 3, 10, (9, 9, 9), (64, 64, 128, 256), reg=0.0005)\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics=['accuracy'])\n",
    "\n",
    "# print a model summary\n",
    "#print(model.summary())\n",
    "\n",
    "print(\"[INFO] training network...\")\n",
    "\n",
    "# code for a learning rate scheduler\n",
    "def polynomial_decay(epoch):\n",
    "    maxEpochs = numberOfEpochs\n",
    "    baseLearningRate = initialLearningRate\n",
    "    power = 1.0\n",
    "    \n",
    "    alpha = baseLearningRate * (1 - (epoch / float(numberOfEpochs))) ** power\n",
    "    \n",
    "    # return the learning rate\n",
    "    return alpha\n",
    "\n",
    "callbacks = [LearningRateScheduler(polynomial_decay)]\n",
    "\n",
    "H = model.fit(aug.flow(trainX, trainY, batch_size=128), validation_data=(testX, testY),\n",
    "              steps_per_epoch=len(trainX) // 128, epochs=numberOfEpochs,\n",
    "              callbacks = callbacks,\n",
    "              verbose=1)\n",
    "\n",
    "# print a classification report\n",
    "print('\\n Test accuracy')\n",
    "predictedY = model.predict(testX)\n",
    "predictedY = predictedY.argmax(axis=1)\n",
    "testY = testY.argmax(axis=1)\n",
    "print(classification_report(testY, predictedY, digits=4))\n",
    "\n",
    "# plot the training loss and accuracy\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, numberOfEpochs), H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, numberOfEpochs), H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0, numberOfEpochs), H.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0, numberOfEpochs), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are good but a little underwhelming here--according to Identity Mappings in Deep Residual Networks (He et. al., 2016, https://arxiv.org/abs/1603.05027), a similar approach can reach about 94% accuracy, but this is with a much deeper net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from seaborn import heatmap\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "heatmap(confusion_matrix(testY, predictedY), annot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error analysis shows class 3 (cat) is getting mixed up with class 5 (dog), which makes some sense since both are relatively small, furry animals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Ensemble Methods\n",
    "\n",
    "A common technique used with classification methods (both non-neural and neural) is to take an ensemble of models and combine them to make classification decisions. For example, we could run 5 neural nets, each with comparable accuracy overall, and classify each datapoint based on the majority vote of the 5 networks.\n",
    "\n",
    "(This also applies to regression problems if we do some sort of averaging of the predictions.)\n",
    "\n",
    "Ensembling almost always improves results compared to using just one net because different nets have unique talents and may make errors on different datapoints, but, assuming all the nets have good accuracy, they are typically correct, so these mistakes are frequently restricted to just a minority of models.\n",
    "\n",
    "Let's bring in a few (mini) modern CNN achitectures we wrote in the past to use for some ensembling. Note that any architecture would work for the forthcoming experiments, but the following nets can run relatively quickly.\n",
    "\n",
    "First, we import some things we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic packages\n",
    "import cv2\n",
    "import glob\n",
    "import imutils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# sklearn functions\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# keras functions\n",
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.layers import add\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### MiniVGGNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a class for a mini version of VGGNet (Simonyan and Zisserman, 2015)\n",
    "class MiniVGGNet:\n",
    "    def build(height, width, depth, classes):\n",
    "        # create the model and name it MiniVGGNet\n",
    "        model = Sequential(name = 'MiniVGGNet')\n",
    "                \n",
    "        # convolutional layer with 32 3x3 feature maps\n",
    "        model.add(Conv2D(32, (3, 3), padding = 'same', input_shape = (height, width, depth)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        # convolutional layer with 32 3x3 feature maps\n",
    "        model.add(Conv2D(32, (3, 3), padding = 'same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        # 2x2 max pooling layer with stride 2x2\n",
    "        model.add(MaxPooling2D(pool_size = (2, 2), strides = (2, 2)))\n",
    "        model.add(Dropout(0.25))\n",
    "        \n",
    "        # convolutional layer with 64 3x3 feature maps\n",
    "        model.add(Conv2D(64, (3, 3), padding = 'same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        # convolutional layer with 64 3x3 feature maps\n",
    "        model.add(Conv2D(64, (3, 3), padding = 'same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        # 2x2 max pooling layer with stride 2x2\n",
    "        model.add(MaxPooling2D(pool_size = (2, 2), strides = (2, 2)))\n",
    "        model.add(Dropout(0.25))\n",
    "        \n",
    "        # flatten the activations from a square to a vector\n",
    "        model.add(Flatten())\n",
    "        \n",
    "        # fully-connected layer\n",
    "        model.add(Dense(512))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.5))\n",
    "        \n",
    "        # fully-connected layer with softmax classifier\n",
    "        model.add(Dense(classes))\n",
    "        model.add(Activation('softmax'))\n",
    "        \n",
    "        # return the model\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### MiniGoogLeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniGoogLeNet:\n",
    "    def convolution_module(x, K, kX, kY, stride, channelsDim, padding=\"same\"):\n",
    "        # create a CONV -> BN -> RELU sequence\n",
    "        x = Conv2D(K, (kX, kY), strides = stride, padding = padding)(x)\n",
    "        x = BatchNormalization(axis = channelsDim)(x)\n",
    "        x = Activation('relu')(x)\n",
    "        \n",
    "        # return the output\n",
    "        return x\n",
    "    \n",
    "    def inception_module(x, numberOf1x1Kernels, numberOf3x3Kernels, channelsDim):\n",
    "        # define two \"parallel\" convolutions of size 1x1 and 3x3 concatenated across the channels dimension\n",
    "        convolution_1x1 = MiniGoogLeNet.convolution_module(x, numberOf1x1Kernels, 1, 1, (1, 1), channelsDim)\n",
    "        convolution_3x3 = MiniGoogLeNet.convolution_module(x, numberOf3x3Kernels, 3, 3, (1, 1), channelsDim)\n",
    "        x = concatenate([convolution_1x1, convolution_3x3], axis = channelsDim)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    def downsample_module(x, K, channelsDim):\n",
    "        # define a CONV and POOL and then concatenate across the channels dimension\n",
    "        convolution_3x3 = MiniGoogLeNet.convolution_module(x, K, 3, 3, (2, 2), channelsDim, padding = 'valid')\n",
    "        pool = MaxPooling2D((3, 3), strides = (2, 2))(x)\n",
    "        x = concatenate([convolution_3x3, pool], axis = channelsDim)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def build(width, height, depth, classes):\n",
    "        inputShape = (height, width, depth)\n",
    "        channelsDim = -1\n",
    "        \n",
    "        if backend.image_data_format() == \"channels_first\":\n",
    "            inputShape = (depth, height, width)\n",
    "            channelsDim = 1\n",
    "        \n",
    "        # define the model input and first CONV module\n",
    "        inputs = Input(shape = inputShape)\n",
    "        x = MiniGoogLeNet.convolution_module(inputs, 96, 3, 3, (1, 1), channelsDim)\n",
    "        \n",
    "        # two inception modules followed by a downsample module\n",
    "        x = MiniGoogLeNet.inception_module(x, 32, 32, channelsDim)\n",
    "        x = MiniGoogLeNet.inception_module(x, 32, 48, channelsDim)\n",
    "        x = MiniGoogLeNet.downsample_module(x, 80, channelsDim)\n",
    "        \n",
    "        # four inception modules followed by a downsample module\n",
    "        x = MiniGoogLeNet.inception_module(x, 112, 48, channelsDim)\n",
    "        x = MiniGoogLeNet.inception_module(x, 96, 64, channelsDim)\n",
    "        x = MiniGoogLeNet.inception_module(x, 80, 80, channelsDim)\n",
    "        x = MiniGoogLeNet.inception_module(x, 48, 96, channelsDim)\n",
    "        x = MiniGoogLeNet.downsample_module(x, 96, channelsDim)\n",
    "        \n",
    "        # two inception modules followed by global POOL and dropout\n",
    "        x = MiniGoogLeNet.inception_module(x, 176, 160, channelsDim)\n",
    "        x = MiniGoogLeNet.inception_module(x, 176, 160, channelsDim)\n",
    "        x = AveragePooling2D((7, 7))(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        \n",
    "        # softmax classifier\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(classes)(x)\n",
    "        x = Activation('softmax')(x)\n",
    "        \n",
    "        # create a model\n",
    "        model = Model(inputs, x, name='MiniGoogLeNet')\n",
    "        \n",
    "        # return the model\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet:\n",
    "    def residual_module(data, K, stride, channelsDim, reduce = False, reg = 0.0001, bnEpsilon = 0.00002, bnMomentum = 0.9):\n",
    "        shortcut = data\n",
    "        \n",
    "        # 1x1 CONVs\n",
    "        bn1 = BatchNormalization(axis = channelsDim, epsilon = bnEpsilon, momentum = bnMomentum)(data)\n",
    "        act1 = Activation('relu')(bn1)\n",
    "        conv1 = Conv2D(int(K * 0.25), (1, 1), use_bias = False, kernel_regularizer = l2(reg))(act1)\n",
    "        \n",
    "        # 3x3 CONVs\n",
    "        bn2 = BatchNormalization(axis = channelsDim, epsilon = bnEpsilon, momentum = bnMomentum)(conv1)\n",
    "        act2 = Activation('relu')(bn2)\n",
    "        conv2 = Conv2D(int(K * 0.25), (3, 3), strides = stride, padding = 'same', use_bias = False, kernel_regularizer = l2(reg))(act2)\n",
    "        \n",
    "        # 1x1 CONVs\n",
    "        bn3 = BatchNormalization(axis = channelsDim, epsilon = bnEpsilon, momentum = bnMomentum)(conv2)\n",
    "        act3 = Activation('relu')(bn3)\n",
    "        conv3 = Conv2D(K, (1, 1), use_bias = False, kernel_regularizer = l2(reg))(act3)\n",
    "        \n",
    "        # if we reduce the spatial size, apply a CONV layer to the shortcut\n",
    "        if reduce:\n",
    "            shortcut = Conv2D(K, (1, 1), strides = stride, use_bias = False, kernel_regularizer = l2(reg))(act1)\n",
    "            \n",
    "        # add the shortcut and the final CONV\n",
    "        x = add([conv3, shortcut])\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def build(width, height, depth, classes, stages, filters, reg = 0.0001, bnEpsilon = 0.00002, bnMomentum = 0.9, dataset='cifar'):\n",
    "        inputShape = (height, width, depth)\n",
    "        channelsDim = -1\n",
    "        \n",
    "        if backend.image_data_format() == 'channels_first':\n",
    "            inputShape = (depth, height, width)\n",
    "            channelsDim = 1\n",
    "            \n",
    "        # set the input and apply BN\n",
    "        inputs = Input(shape = inputShape)\n",
    "        x = BatchNormalization(axis = channelsDim, epsilon = bnEpsilon, momentum = bnMomentum)(inputs)\n",
    "        \n",
    "        if dataset == 'cifar':\n",
    "            # apply a single CONV layer\n",
    "            x = Conv2D(filters[0], (3, 3), use_bias = False, padding = 'same',\n",
    "                       kernel_regularizer = l2(reg))(x)\n",
    "        \n",
    "        # loop over the number of stages\n",
    "        for counter in range(0, len(stages)):\n",
    "            # initialize the stride\n",
    "            if counter == 0:\n",
    "                stride = (1, 1)\n",
    "            else:\n",
    "                stride = (2, 2)\n",
    "                    \n",
    "            # apply a residual module to reduce the spatial dimension of the image volume\n",
    "            x = ResNet.residual_module(x, filters[counter + 1], stride, channelsDim, reduce = True, bnEpsilon = bnEpsilon, bnMomentum = bnMomentum)\n",
    "            \n",
    "            # loop over the number of layers in the current stage\n",
    "            for j in range(0, stages[counter] - 1):\n",
    "                # apply a residual module\n",
    "                x = ResNet.residual_module(x, filters[counter + 1], (1, 1), channelsDim, bnEpsilon = bnEpsilon, bnMomentum = bnMomentum)\n",
    "                    \n",
    "        # apply BN -> ACT -> POOL\n",
    "        x = BatchNormalization(axis = channelsDim, epsilon = bnEpsilon, momentum = bnMomentum)(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = AveragePooling2D((8, 8))(x)\n",
    "        \n",
    "        # softmax classifier\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(classes, kernel_regularizer = l2(reg))(x)\n",
    "        x = Activation('softmax')(x)\n",
    "        \n",
    "        # create the model\n",
    "        model = Model(inputs, x, name = 'ResNet')\n",
    "        \n",
    "        # return the model\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training Ensembles\n",
    "\n",
    "Now, let's look at some ensembling methods. In the simplest case, we train several nets and average the classifications at the end. If the nets have similar performance, but make mistakes on *different* examples, this approach improves performance in most cases.\n",
    "\n",
    "**Quick GPU Check**: Before we start training models, let's check our GPU resources. If you have a GPU set up to work with TensorFlow, its name will be output and it will be used in training. If not, none will be output and your training will use your CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  2\n",
      "/device:GPU:0\n",
      "device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:2d:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "numGPUs = len(tf.config.experimental.list_physical_devices('GPU'))\n",
    "\n",
    "print(\"Num GPUs Available: \", numGPUs)\n",
    "\n",
    "if numGPUs > 0:\n",
    "    print(tf.test.gpu_device_name())\n",
    "    print(device_lib.list_local_devices()[1].physical_device_desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training with the Same Hyperparameters Multiple Times\n",
    "\n",
    "Next, let's train several MiniGoogLeNets (small versions of Inception nets) on CIFAR-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cifar10 data\n",
    "((trainX, trainY), (testX, testY)) = cifar10.load_data()\n",
    "labelNames = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# preprocess data\n",
    "trainX = trainX.astype('float')/255.0\n",
    "testX = testX.astype('float')/255.0\n",
    "trainY = to_categorical(trainY, 10)\n",
    "testY = to_categorical(testY, 10)\n",
    "\n",
    "# create an image generator for data augmentation with random shifting, rotation, and horizontal flips\n",
    "aug = ImageDataGenerator(rotation_range = 10,\n",
    "                         width_shift_range = 0.1,\n",
    "                         height_shift_range = 0.1,\n",
    "                         horizontal_flip = True,\n",
    "                         fill_mode = 'nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net 0 is being trained...\n",
      "Epoch 1/50\n",
      "781/781 [==============================] - 19s 23ms/step - loss: 1.3371 - accuracy: 0.5172 - val_loss: 1.2020 - val_accuracy: 0.5743\n",
      "Epoch 2/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.9042 - accuracy: 0.6838 - val_loss: 1.0691 - val_accuracy: 0.6448\n",
      "Epoch 3/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.7345 - accuracy: 0.7456 - val_loss: 0.8435 - val_accuracy: 0.7119\n",
      "Epoch 4/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.6302 - accuracy: 0.7827 - val_loss: 0.8261 - val_accuracy: 0.7138\n",
      "Epoch 5/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.5671 - accuracy: 0.8046 - val_loss: 0.6849 - val_accuracy: 0.7735\n",
      "Epoch 6/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.5118 - accuracy: 0.8244 - val_loss: 1.0000 - val_accuracy: 0.7098\n",
      "Epoch 7/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.4692 - accuracy: 0.8426 - val_loss: 0.4769 - val_accuracy: 0.8389\n",
      "Epoch 8/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.4361 - accuracy: 0.8520 - val_loss: 0.7064 - val_accuracy: 0.7848\n",
      "Epoch 9/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.4060 - accuracy: 0.8613 - val_loss: 0.6578 - val_accuracy: 0.8014\n",
      "Epoch 10/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.3774 - accuracy: 0.8702 - val_loss: 0.5385 - val_accuracy: 0.8210\n",
      "Epoch 11/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.3553 - accuracy: 0.8774 - val_loss: 0.6679 - val_accuracy: 0.8029\n",
      "Epoch 12/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.3375 - accuracy: 0.8840 - val_loss: 0.4405 - val_accuracy: 0.8532\n",
      "Epoch 13/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.3205 - accuracy: 0.8899 - val_loss: 0.4672 - val_accuracy: 0.8433\n",
      "Epoch 14/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.3042 - accuracy: 0.8949 - val_loss: 0.6962 - val_accuracy: 0.8020\n",
      "Epoch 15/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.2869 - accuracy: 0.9021 - val_loss: 0.4975 - val_accuracy: 0.8361\n",
      "Epoch 16/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.2729 - accuracy: 0.9053 - val_loss: 0.4578 - val_accuracy: 0.8608\n",
      "Epoch 17/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.2631 - accuracy: 0.9091 - val_loss: 0.4527 - val_accuracy: 0.8609\n",
      "Epoch 18/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.2472 - accuracy: 0.9144 - val_loss: 0.3836 - val_accuracy: 0.8751\n",
      "Epoch 19/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.2367 - accuracy: 0.9170 - val_loss: 0.5401 - val_accuracy: 0.8357\n",
      "Epoch 20/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.2287 - accuracy: 0.9193 - val_loss: 0.4789 - val_accuracy: 0.8559\n",
      "Epoch 21/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.2161 - accuracy: 0.9245 - val_loss: 0.4235 - val_accuracy: 0.8683\n",
      "Epoch 22/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.2077 - accuracy: 0.9285 - val_loss: 0.4042 - val_accuracy: 0.8719\n",
      "Epoch 23/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.2026 - accuracy: 0.9292 - val_loss: 0.4842 - val_accuracy: 0.8403\n",
      "Epoch 24/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.1923 - accuracy: 0.9333 - val_loss: 0.4252 - val_accuracy: 0.8647\n",
      "Epoch 25/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.1922 - accuracy: 0.9326 - val_loss: 0.4658 - val_accuracy: 0.8624\n",
      "Epoch 26/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.1773 - accuracy: 0.9391 - val_loss: 0.3755 - val_accuracy: 0.8907\n",
      "Epoch 27/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.1743 - accuracy: 0.9384 - val_loss: 0.4109 - val_accuracy: 0.8798\n",
      "Epoch 28/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.1640 - accuracy: 0.9423 - val_loss: 0.3829 - val_accuracy: 0.8929\n",
      "Epoch 29/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.1613 - accuracy: 0.9436 - val_loss: 0.5329 - val_accuracy: 0.8607\n",
      "Epoch 30/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.1543 - accuracy: 0.9458 - val_loss: 0.5250 - val_accuracy: 0.8632\n",
      "Epoch 31/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.1508 - accuracy: 0.9468 - val_loss: 0.5153 - val_accuracy: 0.8551\n",
      "Epoch 32/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.1529 - accuracy: 0.9466 - val_loss: 0.3958 - val_accuracy: 0.8824\n",
      "Epoch 33/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.1394 - accuracy: 0.9510 - val_loss: 0.4004 - val_accuracy: 0.8852\n",
      "Epoch 34/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.1411 - accuracy: 0.9504 - val_loss: 0.4584 - val_accuracy: 0.8732\n",
      "Epoch 35/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.1340 - accuracy: 0.9533 - val_loss: 0.4537 - val_accuracy: 0.8699\n",
      "Epoch 36/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.1280 - accuracy: 0.9552 - val_loss: 0.4570 - val_accuracy: 0.8823\n",
      "Epoch 37/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.1290 - accuracy: 0.9552 - val_loss: 0.3530 - val_accuracy: 0.8998\n",
      "Epoch 38/50\n",
      "781/781 [==============================] - 17s 21ms/step - loss: 0.1218 - accuracy: 0.9582 - val_loss: 0.3824 - val_accuracy: 0.8945\n",
      "Epoch 39/50\n",
      "781/781 [==============================] - 17s 21ms/step - loss: 0.1197 - accuracy: 0.9578 - val_loss: 0.4681 - val_accuracy: 0.8789\n",
      "Epoch 40/50\n",
      "781/781 [==============================] - 17s 21ms/step - loss: 0.1172 - accuracy: 0.9586 - val_loss: 0.3984 - val_accuracy: 0.8917\n",
      "Epoch 41/50\n",
      "781/781 [==============================] - 17s 21ms/step - loss: 0.1164 - accuracy: 0.9592 - val_loss: 0.4590 - val_accuracy: 0.8801\n",
      "Epoch 42/50\n",
      "781/781 [==============================] - 17s 21ms/step - loss: 0.1132 - accuracy: 0.9612 - val_loss: 0.5029 - val_accuracy: 0.8675\n",
      "Epoch 43/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.1083 - accuracy: 0.9610 - val_loss: 0.3456 - val_accuracy: 0.9004\n",
      "Epoch 44/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.1047 - accuracy: 0.9634 - val_loss: 0.5075 - val_accuracy: 0.8752\n",
      "Epoch 45/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.1073 - accuracy: 0.9628 - val_loss: 0.4532 - val_accuracy: 0.8815\n",
      "Epoch 46/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.0992 - accuracy: 0.9657 - val_loss: 0.4017 - val_accuracy: 0.8939\n",
      "Epoch 47/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.0974 - accuracy: 0.9657 - val_loss: 0.3967 - val_accuracy: 0.8957\n",
      "Epoch 48/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.0988 - accuracy: 0.9665 - val_loss: 0.4078 - val_accuracy: 0.8949\n",
      "Epoch 49/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.0945 - accuracy: 0.9667 - val_loss: 0.4239 - val_accuracy: 0.8921\n",
      "Epoch 50/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.0932 - accuracy: 0.9672 - val_loss: 0.4190 - val_accuracy: 0.8981\n",
      "INFO:tensorflow:Assets written to: models1\\model_0.model\\assets\n",
      "Net 1 is being trained...\n",
      "Epoch 1/50\n",
      "781/781 [==============================] - 19s 23ms/step - loss: 1.3507 - accuracy: 0.5152 - val_loss: 1.5939 - val_accuracy: 0.4885\n",
      "Epoch 2/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.9099 - accuracy: 0.6792 - val_loss: 1.4203 - val_accuracy: 0.5782\n",
      "Epoch 3/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.7474 - accuracy: 0.7405 - val_loss: 0.9301 - val_accuracy: 0.6695\n",
      "Epoch 4/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.6397 - accuracy: 0.7822 - val_loss: 0.8908 - val_accuracy: 0.6973\n",
      "Epoch 5/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.5745 - accuracy: 0.8048 - val_loss: 1.1254 - val_accuracy: 0.6801\n",
      "Epoch 6/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.5226 - accuracy: 0.8222 - val_loss: 0.5692 - val_accuracy: 0.8039\n",
      "Epoch 7/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.4707 - accuracy: 0.8383 - val_loss: 0.8711 - val_accuracy: 0.7358\n",
      "Epoch 8/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.4389 - accuracy: 0.8516 - val_loss: 0.6620 - val_accuracy: 0.7823\n",
      "Epoch 9/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.4044 - accuracy: 0.8628 - val_loss: 0.5194 - val_accuracy: 0.8245\n",
      "Epoch 10/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.3799 - accuracy: 0.8714 - val_loss: 0.6888 - val_accuracy: 0.7878\n",
      "Epoch 11/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.3632 - accuracy: 0.8754 - val_loss: 0.5523 - val_accuracy: 0.8243\n",
      "Epoch 12/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.3368 - accuracy: 0.8845 - val_loss: 0.8297 - val_accuracy: 0.7633\n",
      "Epoch 13/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.3213 - accuracy: 0.8893 - val_loss: 0.5406 - val_accuracy: 0.8284\n",
      "Epoch 14/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.3059 - accuracy: 0.8942 - val_loss: 0.4815 - val_accuracy: 0.8506\n",
      "Epoch 15/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.2892 - accuracy: 0.9012 - val_loss: 0.4165 - val_accuracy: 0.8617\n",
      "Epoch 16/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.2724 - accuracy: 0.9065 - val_loss: 0.4410 - val_accuracy: 0.8541\n",
      "Epoch 17/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.2637 - accuracy: 0.9091 - val_loss: 0.3984 - val_accuracy: 0.8685\n",
      "Epoch 18/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.2506 - accuracy: 0.9144 - val_loss: 0.3463 - val_accuracy: 0.8851\n",
      "Epoch 19/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.2321 - accuracy: 0.9194 - val_loss: 0.4881 - val_accuracy: 0.8513\n",
      "Epoch 20/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.2268 - accuracy: 0.9226 - val_loss: 0.4919 - val_accuracy: 0.8549\n",
      "Epoch 21/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.2178 - accuracy: 0.9243 - val_loss: 0.4135 - val_accuracy: 0.8680\n",
      "Epoch 22/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.2105 - accuracy: 0.9287 - val_loss: 0.4542 - val_accuracy: 0.8655\n",
      "Epoch 23/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.1971 - accuracy: 0.9316 - val_loss: 0.4462 - val_accuracy: 0.8691\n",
      "Epoch 24/50\n",
      "781/781 [==============================] - 17s 21ms/step - loss: 0.1941 - accuracy: 0.9318 - val_loss: 0.5048 - val_accuracy: 0.8626\n",
      "Epoch 25/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.1844 - accuracy: 0.9356 - val_loss: 0.3619 - val_accuracy: 0.8866\n",
      "Epoch 26/50\n",
      "781/781 [==============================] - 17s 21ms/step - loss: 0.1811 - accuracy: 0.9353 - val_loss: 0.4198 - val_accuracy: 0.8800\n",
      "Epoch 27/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.1707 - accuracy: 0.9409 - val_loss: 0.6261 - val_accuracy: 0.8368\n",
      "Epoch 28/50\n",
      "781/781 [==============================] - 17s 21ms/step - loss: 0.1700 - accuracy: 0.9406 - val_loss: 0.3611 - val_accuracy: 0.8930\n",
      "Epoch 29/50\n",
      "781/781 [==============================] - 17s 21ms/step - loss: 0.1584 - accuracy: 0.9449 - val_loss: 0.4407 - val_accuracy: 0.8720\n",
      "Epoch 30/50\n",
      "781/781 [==============================] - 17s 21ms/step - loss: 0.1539 - accuracy: 0.9465 - val_loss: 0.3826 - val_accuracy: 0.8889\n",
      "Epoch 31/50\n",
      "781/781 [==============================] - 17s 21ms/step - loss: 0.1504 - accuracy: 0.9479 - val_loss: 0.3972 - val_accuracy: 0.8813\n",
      "Epoch 32/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.1465 - accuracy: 0.9498 - val_loss: 0.5192 - val_accuracy: 0.8598\n",
      "Epoch 33/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.1373 - accuracy: 0.9530 - val_loss: 0.3651 - val_accuracy: 0.8951\n",
      "Epoch 34/50\n",
      "781/781 [==============================] - 17s 21ms/step - loss: 0.1342 - accuracy: 0.9527 - val_loss: 0.4823 - val_accuracy: 0.8731\n",
      "Epoch 35/50\n",
      "781/781 [==============================] - 17s 21ms/step - loss: 0.1344 - accuracy: 0.9534 - val_loss: 0.3926 - val_accuracy: 0.8802\n",
      "Epoch 36/50\n",
      "781/781 [==============================] - 17s 21ms/step - loss: 0.1266 - accuracy: 0.9554 - val_loss: 0.3660 - val_accuracy: 0.8957\n",
      "Epoch 37/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.1264 - accuracy: 0.9556 - val_loss: 0.4246 - val_accuracy: 0.8826\n",
      "Epoch 38/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.1254 - accuracy: 0.9555 - val_loss: 0.3496 - val_accuracy: 0.8947\n",
      "Epoch 39/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.1206 - accuracy: 0.9573 - val_loss: 0.5729 - val_accuracy: 0.8605\n",
      "Epoch 40/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.1183 - accuracy: 0.9587 - val_loss: 0.3536 - val_accuracy: 0.8976\n",
      "Epoch 41/50\n",
      "781/781 [==============================] - 17s 21ms/step - loss: 0.1088 - accuracy: 0.9616 - val_loss: 0.3495 - val_accuracy: 0.9026\n",
      "Epoch 42/50\n",
      "781/781 [==============================] - 17s 21ms/step - loss: 0.1082 - accuracy: 0.9633 - val_loss: 0.3603 - val_accuracy: 0.8959\n",
      "Epoch 43/50\n",
      "781/781 [==============================] - 17s 21ms/step - loss: 0.1079 - accuracy: 0.9621 - val_loss: 0.3266 - val_accuracy: 0.9132\n",
      "Epoch 44/50\n",
      "781/781 [==============================] - 17s 21ms/step - loss: 0.1025 - accuracy: 0.9636 - val_loss: 0.4522 - val_accuracy: 0.8918\n",
      "Epoch 45/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.1027 - accuracy: 0.9643 - val_loss: 0.4304 - val_accuracy: 0.8848\n",
      "Epoch 46/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.0999 - accuracy: 0.9647 - val_loss: 0.4351 - val_accuracy: 0.8915\n",
      "Epoch 47/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.0956 - accuracy: 0.9677 - val_loss: 0.3652 - val_accuracy: 0.9003\n",
      "Epoch 48/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.0959 - accuracy: 0.9668 - val_loss: 0.4506 - val_accuracy: 0.8925\n",
      "Epoch 49/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.0888 - accuracy: 0.9696 - val_loss: 0.3965 - val_accuracy: 0.8978\n",
      "Epoch 50/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.0907 - accuracy: 0.9676 - val_loss: 0.3798 - val_accuracy: 0.9025\n",
      "INFO:tensorflow:Assets written to: models1\\model_1.model\\assets\n",
      "Net 2 is being trained...\n",
      "Epoch 1/50\n",
      "781/781 [==============================] - 18s 21ms/step - loss: 1.3595 - accuracy: 0.5107 - val_loss: 1.4133 - val_accuracy: 0.5485\n",
      "Epoch 2/50\n",
      "781/781 [==============================] - 17s 21ms/step - loss: 0.9109 - accuracy: 0.6802 - val_loss: 1.2609 - val_accuracy: 0.6116\n",
      "Epoch 3/50\n",
      "781/781 [==============================] - 17s 21ms/step - loss: 0.7394 - accuracy: 0.7463 - val_loss: 0.8426 - val_accuracy: 0.7108\n",
      "Epoch 4/50\n",
      "781/781 [==============================] - 17s 21ms/step - loss: 0.6408 - accuracy: 0.7799 - val_loss: 0.7545 - val_accuracy: 0.7420\n",
      "Epoch 5/50\n",
      "781/781 [==============================] - 17s 21ms/step - loss: 0.5671 - accuracy: 0.8060 - val_loss: 0.8516 - val_accuracy: 0.7267\n",
      "Epoch 6/50\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.5104 - accuracy: 0.8240 - val_loss: 0.6404 - val_accuracy: 0.7805\n",
      "Epoch 7/50\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.4731 - accuracy: 0.8386 - val_loss: 0.5531 - val_accuracy: 0.8108\n",
      "Epoch 8/50\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.4327 - accuracy: 0.8518 - val_loss: 0.5217 - val_accuracy: 0.8207\n",
      "Epoch 9/50\n",
      "781/781 [==============================] - 17s 21ms/step - loss: 0.4009 - accuracy: 0.8616 - val_loss: 0.4225 - val_accuracy: 0.8588\n",
      "Epoch 10/50\n",
      "781/781 [==============================] - 17s 21ms/step - loss: 0.3766 - accuracy: 0.8707 - val_loss: 0.4973 - val_accuracy: 0.8344\n",
      "Epoch 11/50\n",
      "781/781 [==============================] - 17s 21ms/step - loss: 0.3556 - accuracy: 0.8784 - val_loss: 0.7204 - val_accuracy: 0.7890\n",
      "Epoch 12/50\n",
      "781/781 [==============================] - 17s 21ms/step - loss: 0.3351 - accuracy: 0.8860 - val_loss: 0.4487 - val_accuracy: 0.8500\n",
      "Epoch 13/50\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.3153 - accuracy: 0.8916 - val_loss: 0.5075 - val_accuracy: 0.8362\n",
      "Epoch 14/50\n",
      "781/781 [==============================] - 17s 21ms/step - loss: 0.2963 - accuracy: 0.8988 - val_loss: 0.6452 - val_accuracy: 0.8193\n",
      "Epoch 15/50\n",
      "781/781 [==============================] - 17s 21ms/step - loss: 0.2871 - accuracy: 0.9008 - val_loss: 0.6144 - val_accuracy: 0.8067\n",
      "Epoch 16/50\n",
      "781/781 [==============================] - 17s 21ms/step - loss: 0.2738 - accuracy: 0.9064 - val_loss: 0.6555 - val_accuracy: 0.8104\n",
      "Epoch 17/50\n",
      "781/781 [==============================] - 17s 21ms/step - loss: 0.2564 - accuracy: 0.9109 - val_loss: 0.3948 - val_accuracy: 0.8761\n",
      "Epoch 18/50\n",
      "781/781 [==============================] - 17s 21ms/step - loss: 0.2471 - accuracy: 0.9152 - val_loss: 0.6706 - val_accuracy: 0.8071\n",
      "Epoch 19/50\n",
      "781/781 [==============================] - 17s 21ms/step - loss: 0.2398 - accuracy: 0.9169 - val_loss: 0.4855 - val_accuracy: 0.8522\n",
      "Epoch 20/50\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.2224 - accuracy: 0.9237 - val_loss: 0.7056 - val_accuracy: 0.8032\n",
      "Epoch 21/50\n",
      "781/781 [==============================] - 17s 21ms/step - loss: 0.2166 - accuracy: 0.9249 - val_loss: 0.4060 - val_accuracy: 0.8741\n",
      "Epoch 22/50\n",
      "781/781 [==============================] - 17s 21ms/step - loss: 0.2077 - accuracy: 0.9276 - val_loss: 0.4312 - val_accuracy: 0.8703\n",
      "Epoch 23/50\n",
      "781/781 [==============================] - 17s 21ms/step - loss: 0.1986 - accuracy: 0.9314 - val_loss: 0.4348 - val_accuracy: 0.8652\n",
      "Epoch 24/50\n",
      "781/781 [==============================] - 17s 21ms/step - loss: 0.1924 - accuracy: 0.9337 - val_loss: 0.3229 - val_accuracy: 0.8916\n",
      "Epoch 25/50\n",
      "781/781 [==============================] - 17s 21ms/step - loss: 0.1859 - accuracy: 0.9353 - val_loss: 0.3684 - val_accuracy: 0.8895\n",
      "Epoch 26/50\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.1795 - accuracy: 0.9374 - val_loss: 0.4438 - val_accuracy: 0.8689\n",
      "Epoch 27/50\n",
      "781/781 [==============================] - 17s 21ms/step - loss: 0.1720 - accuracy: 0.9397 - val_loss: 0.5297 - val_accuracy: 0.8570\n",
      "Epoch 28/50\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.1686 - accuracy: 0.9422 - val_loss: 0.4330 - val_accuracy: 0.8764\n",
      "Epoch 29/50\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.1585 - accuracy: 0.9448 - val_loss: 0.4823 - val_accuracy: 0.8682\n",
      "Epoch 30/50\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.1515 - accuracy: 0.9466 - val_loss: 0.5411 - val_accuracy: 0.8593\n",
      "Epoch 31/50\n",
      "781/781 [==============================] - 17s 21ms/step - loss: 0.1533 - accuracy: 0.9467 - val_loss: 0.6053 - val_accuracy: 0.8441\n",
      "Epoch 32/50\n",
      "781/781 [==============================] - 17s 21ms/step - loss: 0.1460 - accuracy: 0.9487 - val_loss: 0.3522 - val_accuracy: 0.8991\n",
      "Epoch 33/50\n",
      "781/781 [==============================] - 17s 21ms/step - loss: 0.1437 - accuracy: 0.9498 - val_loss: 0.4156 - val_accuracy: 0.8834\n",
      "Epoch 34/50\n",
      "781/781 [==============================] - 17s 21ms/step - loss: 0.1363 - accuracy: 0.9516 - val_loss: 0.4537 - val_accuracy: 0.8811\n",
      "Epoch 35/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.1351 - accuracy: 0.9536 - val_loss: 0.3826 - val_accuracy: 0.8887\n",
      "Epoch 36/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.1287 - accuracy: 0.9550 - val_loss: 0.3334 - val_accuracy: 0.9023\n",
      "Epoch 37/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.1241 - accuracy: 0.9575 - val_loss: 0.5146 - val_accuracy: 0.8678\n",
      "Epoch 38/50\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.1277 - accuracy: 0.9553 - val_loss: 0.4486 - val_accuracy: 0.8804\n",
      "Epoch 39/50\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.1177 - accuracy: 0.9589 - val_loss: 0.4260 - val_accuracy: 0.8820\n",
      "Epoch 40/50\n",
      "781/781 [==============================] - 17s 21ms/step - loss: 0.1140 - accuracy: 0.9593 - val_loss: 0.3682 - val_accuracy: 0.9004\n",
      "Epoch 41/50\n",
      "781/781 [==============================] - 17s 21ms/step - loss: 0.1144 - accuracy: 0.9601 - val_loss: 0.3854 - val_accuracy: 0.8973\n",
      "Epoch 42/50\n",
      "781/781 [==============================] - 17s 21ms/step - loss: 0.1093 - accuracy: 0.9614 - val_loss: 0.5331 - val_accuracy: 0.8742\n",
      "Epoch 43/50\n",
      "781/781 [==============================] - 17s 21ms/step - loss: 0.1053 - accuracy: 0.9631 - val_loss: 0.4272 - val_accuracy: 0.8858\n",
      "Epoch 44/50\n",
      "781/781 [==============================] - 17s 21ms/step - loss: 0.1065 - accuracy: 0.9641 - val_loss: 0.5102 - val_accuracy: 0.8738\n",
      "Epoch 45/50\n",
      "781/781 [==============================] - 17s 21ms/step - loss: 0.1047 - accuracy: 0.9634 - val_loss: 0.4712 - val_accuracy: 0.8854\n",
      "Epoch 46/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.1046 - accuracy: 0.9647 - val_loss: 0.3816 - val_accuracy: 0.9013\n",
      "Epoch 47/50\n",
      "781/781 [==============================] - 17s 21ms/step - loss: 0.0961 - accuracy: 0.9668 - val_loss: 0.4714 - val_accuracy: 0.8833\n",
      "Epoch 48/50\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.0966 - accuracy: 0.9657 - val_loss: 0.3753 - val_accuracy: 0.9021\n",
      "Epoch 49/50\n",
      "781/781 [==============================] - 16s 21ms/step - loss: 0.0949 - accuracy: 0.9668 - val_loss: 0.4331 - val_accuracy: 0.8924\n",
      "Epoch 50/50\n",
      "781/781 [==============================] - 17s 21ms/step - loss: 0.0903 - accuracy: 0.9680 - val_loss: 0.3957 - val_accuracy: 0.8981\n",
      "INFO:tensorflow:Assets written to: models1\\model_2.model\\assets\n",
      "Net 3 is being trained...\n",
      "Epoch 1/50\n",
      "781/781 [==============================] - 18s 22ms/step - loss: 1.3669 - accuracy: 0.5082 - val_loss: 1.4656 - val_accuracy: 0.5447\n",
      "Epoch 2/50\n",
      "781/781 [==============================] - 17s 21ms/step - loss: 0.9200 - accuracy: 0.6780 - val_loss: 1.1143 - val_accuracy: 0.6233\n",
      "Epoch 3/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.7511 - accuracy: 0.7377 - val_loss: 0.8038 - val_accuracy: 0.7276\n",
      "Epoch 4/50\n",
      "781/781 [==============================] - 17s 21ms/step - loss: 0.6490 - accuracy: 0.7762 - val_loss: 0.9223 - val_accuracy: 0.7068\n",
      "Epoch 5/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.5776 - accuracy: 0.8022 - val_loss: 1.1224 - val_accuracy: 0.6668\n",
      "Epoch 6/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.5256 - accuracy: 0.8195 - val_loss: 0.6993 - val_accuracy: 0.7632\n",
      "Epoch 7/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.4802 - accuracy: 0.8351 - val_loss: 0.7315 - val_accuracy: 0.7631\n",
      "Epoch 8/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.4403 - accuracy: 0.8489 - val_loss: 0.5381 - val_accuracy: 0.8175\n",
      "Epoch 9/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.4122 - accuracy: 0.8598 - val_loss: 0.6365 - val_accuracy: 0.7996\n",
      "Epoch 10/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.3801 - accuracy: 0.8705 - val_loss: 0.6460 - val_accuracy: 0.7967\n",
      "Epoch 11/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.3615 - accuracy: 0.8765 - val_loss: 0.4904 - val_accuracy: 0.8280\n",
      "Epoch 12/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.3389 - accuracy: 0.8844 - val_loss: 0.5559 - val_accuracy: 0.8195\n",
      "Epoch 13/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.3200 - accuracy: 0.8909 - val_loss: 0.5917 - val_accuracy: 0.8243\n",
      "Epoch 14/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.3084 - accuracy: 0.8948 - val_loss: 0.5656 - val_accuracy: 0.8271\n",
      "Epoch 15/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.2911 - accuracy: 0.9010 - val_loss: 0.4827 - val_accuracy: 0.8505\n",
      "Epoch 16/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.2752 - accuracy: 0.9047 - val_loss: 0.5559 - val_accuracy: 0.8364\n",
      "Epoch 17/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.2618 - accuracy: 0.9102 - val_loss: 0.6823 - val_accuracy: 0.8086\n",
      "Epoch 18/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.2555 - accuracy: 0.9118 - val_loss: 0.4635 - val_accuracy: 0.8522\n",
      "Epoch 19/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.2398 - accuracy: 0.9175 - val_loss: 0.4486 - val_accuracy: 0.8555\n",
      "Epoch 20/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.2321 - accuracy: 0.9189 - val_loss: 0.4995 - val_accuracy: 0.8551\n",
      "Epoch 21/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.2166 - accuracy: 0.9236 - val_loss: 0.4347 - val_accuracy: 0.8622\n",
      "Epoch 22/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.2137 - accuracy: 0.9263 - val_loss: 0.5541 - val_accuracy: 0.8281\n",
      "Epoch 23/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.2026 - accuracy: 0.9286 - val_loss: 0.3556 - val_accuracy: 0.8850\n",
      "Epoch 24/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.2017 - accuracy: 0.9289 - val_loss: 0.3190 - val_accuracy: 0.8985\n",
      "Epoch 25/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.1869 - accuracy: 0.9355 - val_loss: 0.4262 - val_accuracy: 0.8717\n",
      "Epoch 26/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.1841 - accuracy: 0.9372 - val_loss: 0.3482 - val_accuracy: 0.8917\n",
      "Epoch 27/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.1722 - accuracy: 0.9400 - val_loss: 0.3994 - val_accuracy: 0.8820\n",
      "Epoch 28/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.1697 - accuracy: 0.9423 - val_loss: 0.4488 - val_accuracy: 0.8709\n",
      "Epoch 29/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.1630 - accuracy: 0.9417 - val_loss: 0.4781 - val_accuracy: 0.8593\n",
      "Epoch 30/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.1589 - accuracy: 0.9445 - val_loss: 0.5267 - val_accuracy: 0.8543\n",
      "Epoch 31/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.1511 - accuracy: 0.9468 - val_loss: 0.3965 - val_accuracy: 0.8803\n",
      "Epoch 32/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.1480 - accuracy: 0.9477 - val_loss: 0.4206 - val_accuracy: 0.8823\n",
      "Epoch 33/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.1414 - accuracy: 0.9511 - val_loss: 0.3946 - val_accuracy: 0.8859\n",
      "Epoch 34/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.1381 - accuracy: 0.9524 - val_loss: 0.3913 - val_accuracy: 0.8913\n",
      "Epoch 35/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.1351 - accuracy: 0.9530 - val_loss: 0.4344 - val_accuracy: 0.8831\n",
      "Epoch 36/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.1311 - accuracy: 0.9546 - val_loss: 0.4793 - val_accuracy: 0.8740\n",
      "Epoch 37/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.1261 - accuracy: 0.9556 - val_loss: 0.4757 - val_accuracy: 0.8811\n",
      "Epoch 38/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.1224 - accuracy: 0.9567 - val_loss: 0.3837 - val_accuracy: 0.8948\n",
      "Epoch 39/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.1207 - accuracy: 0.9568 - val_loss: 0.4646 - val_accuracy: 0.8813\n",
      "Epoch 40/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.1178 - accuracy: 0.9584 - val_loss: 0.3927 - val_accuracy: 0.8930\n",
      "Epoch 41/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.1153 - accuracy: 0.9591 - val_loss: 0.5416 - val_accuracy: 0.8655\n",
      "Epoch 42/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.1119 - accuracy: 0.9612 - val_loss: 0.3769 - val_accuracy: 0.8983\n",
      "Epoch 43/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.1074 - accuracy: 0.9619 - val_loss: 0.4101 - val_accuracy: 0.8936\n",
      "Epoch 44/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.1016 - accuracy: 0.9648 - val_loss: 0.4846 - val_accuracy: 0.8829\n",
      "Epoch 45/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.1039 - accuracy: 0.9634 - val_loss: 0.4121 - val_accuracy: 0.8969\n",
      "Epoch 46/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.1013 - accuracy: 0.9644 - val_loss: 0.4391 - val_accuracy: 0.8846\n",
      "Epoch 47/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.0986 - accuracy: 0.9658 - val_loss: 0.4141 - val_accuracy: 0.8941\n",
      "Epoch 48/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.0975 - accuracy: 0.9668 - val_loss: 0.4226 - val_accuracy: 0.8946\n",
      "Epoch 49/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.0952 - accuracy: 0.9660 - val_loss: 0.4057 - val_accuracy: 0.8967\n",
      "Epoch 50/50\n",
      "781/781 [==============================] - 17s 22ms/step - loss: 0.0907 - accuracy: 0.9685 - val_loss: 0.4072 - val_accuracy: 0.8954\n",
      "INFO:tensorflow:Assets written to: models1\\model_3.model\\assets\n"
     ]
    }
   ],
   "source": [
    "numberOfModels = 4\n",
    "epochs = 50\n",
    "\n",
    "for i in range(numberOfModels):\n",
    "    print('Net', i, 'is being trained...')\n",
    "    \n",
    "    # choose the optimizer\n",
    "    opt = Adam()\n",
    "    \n",
    "    # compile the model\n",
    "    model = MiniGoogLeNet.build(width = 32, height = 32, depth = 3, classes = 10)\n",
    "    \n",
    "    model.compile(loss = 'categorical_crossentropy',\n",
    "                  optimizer = opt,\n",
    "                  metrics = ['accuracy'])\n",
    "    \n",
    "    # train the model\n",
    "    H = model.fit(aug.flow(trainX, trainY, batch_size = 64),\n",
    "                  validation_data = (testX, testY),\n",
    "                  epochs = epochs,\n",
    "                  steps_per_epoch = len(trainX) // 64,\n",
    "                  verbose = 1)\n",
    "    \n",
    "    # save the model\n",
    "    p = ['models1', 'model_{}.model'.format(i)]\n",
    "    model.save(os.path.sep.join(p))\n",
    "    \n",
    "    # evaluate the network\n",
    "    predictions = model.predict(testX, batch_size=64)\n",
    "    report = classification_report(testY.argmax(axis=1),\n",
    "                                   predictions.argmax(axis=1),\n",
    "                                   target_names=labelNames)\n",
    "    \n",
    "    # save the classification report to file\n",
    "    p = ['output1', 'model_{}.txt'.format(i)]\n",
    "    f = open(os.path.sep.join(p), \"w\")\n",
    "    f.write(report)\n",
    "    f.close()\n",
    "    \n",
    "    # plot the training loss and accuracy\n",
    "    p = ['output1', 'model_{}.png'.format(i)]\n",
    "    plt.style.use('ggplot')\n",
    "    plt.figure()\n",
    "    plt.plot(np.arange(0, epochs), H.history['loss'], label = 'train_loss')\n",
    "    plt.plot(np.arange(0, epochs), H.history['val_loss'], label = 'val_loss')\n",
    "    plt.plot(np.arange(0, epochs), H.history['accuracy'], label = 'train_acc')\n",
    "    plt.plot(np.arange(0, epochs), H.history['val_accuracy'], label = 'val_acc')\n",
    "    \n",
    "    # add labels and legend\n",
    "    plt.title('Training Loss and Accuracy for model {}'.format(i))\n",
    "    plt.xlabel('Epoch #')\n",
    "    plt.ylabel('Loss/Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    # save graphs\n",
    "    plt.savefig(os.path.sep.join(p))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we test the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model 1/5\n",
      "Loading model 2/5\n",
      "Loading model 3/5\n",
      "Loading model 4/5\n",
      "Loading model 5/5\n",
      "Evaluating ensemble...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    airplane       0.94      0.93      0.94      1000\n",
      "  automobile       0.96      0.97      0.97      1000\n",
      "        bird       0.90      0.91      0.90      1000\n",
      "         cat       0.84      0.86      0.85      1000\n",
      "        deer       0.93      0.93      0.93      1000\n",
      "         dog       0.90      0.87      0.88      1000\n",
      "        frog       0.94      0.96      0.95      1000\n",
      "       horse       0.98      0.93      0.95      1000\n",
      "        ship       0.97      0.96      0.96      1000\n",
      "       truck       0.92      0.96      0.94      1000\n",
      "\n",
      "    accuracy                           0.93     10000\n",
      "   macro avg       0.93      0.93      0.93     10000\n",
      "weighted avg       0.93      0.93      0.93     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# construct the path used to collect the models then initialize the\n",
    "# models list\n",
    "modelPaths = os.path.sep.join(['models1', '*.model'])\n",
    "modelPaths = list(glob.glob(modelPaths))\n",
    "models = []\n",
    "\n",
    "# loop over the model paths, loading the model, and adding it to\n",
    "# the list of models\n",
    "for (i, modelPath) in enumerate(modelPaths):\n",
    "    print('Loading model {}/{}'.format(i + 1, len(modelPaths)))\n",
    "    models.append(load_model(modelPath))\n",
    "\n",
    "# initialize the list of predictions\n",
    "print('Evaluating ensemble...')\n",
    "predictions = []\n",
    "\n",
    "# loop over the models\n",
    "for model in models:\n",
    "    # use the current model to make predictions on the testing data,\n",
    "    # then store these predictions in the aggregate predictions list\n",
    "    predictions.append(model.predict(testX, batch_size=64))\n",
    "\n",
    "# average the probabilities across all model predictions\n",
    "predictions = np.average(predictions, axis=0)\n",
    "\n",
    "# show a classification report\n",
    "print(classification_report(testY.argmax(axis=1),\n",
    "                            predictions.argmax(axis=1),\n",
    "                            target_names=labelNames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Here, we actually ran the network with the very same hyperparameters 5 times and found that the best ones performed at around **90%**, but the ensemble of all 5 averaged together reseults in a higher accuracy rate of **93%**!\n",
    "\n",
    "Mathematically, what has happened is likely that the different nets converged to *different* local minima so that, while their individual accuracy rates were all similar, the different nets were, apparently misclassifying *different* test examples, resulting in the average classification of all the models being correct more frequently than any individual net.\n",
    "\n",
    "### Ensemble Training with Different Nets\n",
    "\n",
    "It stands to reason that nets that are more significantly different are more likely to make significantly different classification mistakes since they may work in very different ways. For example, our past GoogLeNet and ResNet experiments had about 90% success on CIFAR-10, but they are very different architectures. Let's try to create an ensemble of those!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cifar10 data\n",
    "((trainX, trainY), (testX, testY)) = cifar10.load_data()\n",
    "trainX = trainX.astype('float')\n",
    "testX = testX.astype('float')\n",
    "labelNames = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# preprocess data\n",
    "mean = np.mean(trainX, axis = 0)\n",
    "trainX -= mean\n",
    "testX -= mean\n",
    "trainY = to_categorical(trainY, 10)\n",
    "testY = to_categorical(testY, 10)\n",
    "\n",
    "# create an image generator for data augmentation with random shifting, rotation, and horizontal flips\n",
    "aug = ImageDataGenerator(rotation_range = 10, width_shift_range = 0.1, height_shift_range = 0.1, horizontal_flip = True, fill_mode = 'nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inception 1 is being trained...\n",
      "Epoch 1/50\n",
      "782/782 [==============================] - 18s 22ms/step - loss: 1.3342 - accuracy: 0.5204 - val_loss: 1.1854 - val_accuracy: 0.5753\n",
      "Epoch 2/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.8944 - accuracy: 0.6846 - val_loss: 1.1465 - val_accuracy: 0.6091\n",
      "Epoch 3/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.7266 - accuracy: 0.7490 - val_loss: 1.9836 - val_accuracy: 0.4995\n",
      "Epoch 4/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.6353 - accuracy: 0.7826 - val_loss: 1.1094 - val_accuracy: 0.6608\n",
      "Epoch 5/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.5645 - accuracy: 0.8071 - val_loss: 0.8909 - val_accuracy: 0.7114\n",
      "Epoch 6/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.5052 - accuracy: 0.8266 - val_loss: 0.7139 - val_accuracy: 0.7593\n",
      "Epoch 7/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.4683 - accuracy: 0.8396 - val_loss: 0.6865 - val_accuracy: 0.7832\n",
      "Epoch 8/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.4294 - accuracy: 0.8547 - val_loss: 0.6908 - val_accuracy: 0.7819\n",
      "Epoch 9/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.4015 - accuracy: 0.8616 - val_loss: 0.5686 - val_accuracy: 0.8171\n",
      "Epoch 10/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.3708 - accuracy: 0.8735 - val_loss: 0.6394 - val_accuracy: 0.8028\n",
      "Epoch 11/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.3504 - accuracy: 0.8803 - val_loss: 0.5255 - val_accuracy: 0.8294\n",
      "Epoch 12/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.3314 - accuracy: 0.8871 - val_loss: 0.5012 - val_accuracy: 0.8364\n",
      "Epoch 13/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.3118 - accuracy: 0.8925 - val_loss: 0.4737 - val_accuracy: 0.8440\n",
      "Epoch 14/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.2953 - accuracy: 0.8983 - val_loss: 0.4812 - val_accuracy: 0.8454\n",
      "Epoch 15/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.2790 - accuracy: 0.9049 - val_loss: 0.5238 - val_accuracy: 0.8328\n",
      "Epoch 16/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.2711 - accuracy: 0.9069 - val_loss: 0.4334 - val_accuracy: 0.8571\n",
      "Epoch 17/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.2555 - accuracy: 0.9115 - val_loss: 0.5210 - val_accuracy: 0.8454\n",
      "Epoch 18/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.2462 - accuracy: 0.9146 - val_loss: 0.4732 - val_accuracy: 0.8531\n",
      "Epoch 19/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.2266 - accuracy: 0.9221 - val_loss: 0.5221 - val_accuracy: 0.8407\n",
      "Epoch 20/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.2224 - accuracy: 0.9230 - val_loss: 0.4375 - val_accuracy: 0.8771\n",
      "Epoch 21/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.2132 - accuracy: 0.9263 - val_loss: 0.4317 - val_accuracy: 0.8652\n",
      "Epoch 22/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.2063 - accuracy: 0.9280 - val_loss: 0.7012 - val_accuracy: 0.7942\n",
      "Epoch 23/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.1981 - accuracy: 0.9308 - val_loss: 0.4815 - val_accuracy: 0.8559\n",
      "Epoch 24/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.1905 - accuracy: 0.9345 - val_loss: 0.6027 - val_accuracy: 0.8342\n",
      "Epoch 25/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.1858 - accuracy: 0.9356 - val_loss: 0.4239 - val_accuracy: 0.8770\n",
      "Epoch 26/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.1775 - accuracy: 0.9384 - val_loss: 0.4349 - val_accuracy: 0.8724\n",
      "Epoch 27/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.1740 - accuracy: 0.9390 - val_loss: 0.4372 - val_accuracy: 0.8721\n",
      "Epoch 28/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.1608 - accuracy: 0.9460 - val_loss: 0.4171 - val_accuracy: 0.8753\n",
      "Epoch 29/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.1563 - accuracy: 0.9451 - val_loss: 0.5535 - val_accuracy: 0.8556\n",
      "Epoch 30/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.1539 - accuracy: 0.9469 - val_loss: 0.6321 - val_accuracy: 0.8433\n",
      "Epoch 31/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.1477 - accuracy: 0.9472 - val_loss: 0.4216 - val_accuracy: 0.8802\n",
      "Epoch 32/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.1459 - accuracy: 0.9503 - val_loss: 0.4207 - val_accuracy: 0.8786\n",
      "Epoch 33/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.1381 - accuracy: 0.9513 - val_loss: 0.3645 - val_accuracy: 0.8954\n",
      "Epoch 34/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.1350 - accuracy: 0.9532 - val_loss: 0.3618 - val_accuracy: 0.8986\n",
      "Epoch 35/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.1295 - accuracy: 0.9546 - val_loss: 0.5135 - val_accuracy: 0.8637\n",
      "Epoch 36/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.1249 - accuracy: 0.9567 - val_loss: 0.3926 - val_accuracy: 0.8987\n",
      "Epoch 37/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.1224 - accuracy: 0.9568 - val_loss: 0.3717 - val_accuracy: 0.8989\n",
      "Epoch 38/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.1230 - accuracy: 0.9567 - val_loss: 0.4342 - val_accuracy: 0.8914\n",
      "Epoch 39/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.1150 - accuracy: 0.9602 - val_loss: 0.4175 - val_accuracy: 0.8850\n",
      "Epoch 40/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.1124 - accuracy: 0.9614 - val_loss: 0.4563 - val_accuracy: 0.8786\n",
      "Epoch 41/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.1119 - accuracy: 0.9614 - val_loss: 0.4308 - val_accuracy: 0.8872\n",
      "Epoch 42/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.1085 - accuracy: 0.9627 - val_loss: 0.3556 - val_accuracy: 0.9064\n",
      "Epoch 43/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.1078 - accuracy: 0.9629 - val_loss: 0.3985 - val_accuracy: 0.8927\n",
      "Epoch 44/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.1054 - accuracy: 0.9632 - val_loss: 0.3578 - val_accuracy: 0.8996\n",
      "Epoch 45/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.1045 - accuracy: 0.9633 - val_loss: 0.3979 - val_accuracy: 0.8934\n",
      "Epoch 46/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.0988 - accuracy: 0.9665 - val_loss: 0.4547 - val_accuracy: 0.8873\n",
      "Epoch 47/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.0947 - accuracy: 0.9669 - val_loss: 0.4125 - val_accuracy: 0.8881\n",
      "Epoch 48/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.0968 - accuracy: 0.9663 - val_loss: 0.5265 - val_accuracy: 0.8655\n",
      "Epoch 49/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.0917 - accuracy: 0.9676 - val_loss: 0.4844 - val_accuracy: 0.8863\n",
      "Epoch 50/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.0891 - accuracy: 0.9693 - val_loss: 0.3907 - val_accuracy: 0.8961\n",
      "INFO:tensorflow:Assets written to: models2\\model_0.model\\assets\n",
      "Inception 2 is being trained...\n",
      "Epoch 1/50\n",
      "782/782 [==============================] - 18s 22ms/step - loss: 1.4670 - accuracy: 0.4650 - val_loss: 1.6497 - val_accuracy: 0.4672\n",
      "Epoch 2/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.9999 - accuracy: 0.6447 - val_loss: 1.3449 - val_accuracy: 0.5668\n",
      "Epoch 3/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.8208 - accuracy: 0.7126 - val_loss: 0.9227 - val_accuracy: 0.6890\n",
      "Epoch 4/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.7074 - accuracy: 0.7532 - val_loss: 0.8020 - val_accuracy: 0.7362\n",
      "Epoch 5/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.6198 - accuracy: 0.7866 - val_loss: 0.8565 - val_accuracy: 0.7185\n",
      "Epoch 6/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.5591 - accuracy: 0.8080 - val_loss: 0.7600 - val_accuracy: 0.7474\n",
      "Epoch 7/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.5142 - accuracy: 0.8244 - val_loss: 0.8985 - val_accuracy: 0.7235\n",
      "Epoch 8/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.4770 - accuracy: 0.8368 - val_loss: 0.5718 - val_accuracy: 0.8088\n",
      "Epoch 9/50\n",
      "782/782 [==============================] - 17s 21ms/step - loss: 0.4476 - accuracy: 0.8466 - val_loss: 0.6142 - val_accuracy: 0.7983\n",
      "Epoch 10/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.4155 - accuracy: 0.8579 - val_loss: 0.5822 - val_accuracy: 0.8122\n",
      "Epoch 11/50\n",
      "782/782 [==============================] - 17s 21ms/step - loss: 0.3946 - accuracy: 0.8648 - val_loss: 0.7095 - val_accuracy: 0.7706\n",
      "Epoch 12/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.3723 - accuracy: 0.8724 - val_loss: 0.5878 - val_accuracy: 0.8169\n",
      "Epoch 13/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.3553 - accuracy: 0.8779 - val_loss: 0.4796 - val_accuracy: 0.8436\n",
      "Epoch 14/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.3337 - accuracy: 0.8868 - val_loss: 0.4508 - val_accuracy: 0.8515\n",
      "Epoch 15/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.3226 - accuracy: 0.8891 - val_loss: 0.5263 - val_accuracy: 0.8345\n",
      "Epoch 16/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.3028 - accuracy: 0.8957 - val_loss: 0.5249 - val_accuracy: 0.8320\n",
      "Epoch 17/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.2930 - accuracy: 0.8988 - val_loss: 0.4563 - val_accuracy: 0.8514\n",
      "Epoch 18/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.2755 - accuracy: 0.9032 - val_loss: 0.4672 - val_accuracy: 0.8550\n",
      "Epoch 19/50\n",
      "782/782 [==============================] - 17s 21ms/step - loss: 0.2704 - accuracy: 0.9065 - val_loss: 0.4627 - val_accuracy: 0.8531\n",
      "Epoch 20/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.2581 - accuracy: 0.9098 - val_loss: 0.5607 - val_accuracy: 0.8385\n",
      "Epoch 21/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.2486 - accuracy: 0.9137 - val_loss: 0.5144 - val_accuracy: 0.8449\n",
      "Epoch 22/50\n",
      "782/782 [==============================] - 17s 21ms/step - loss: 0.2292 - accuracy: 0.9207 - val_loss: 0.5010 - val_accuracy: 0.8477\n",
      "Epoch 23/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.2310 - accuracy: 0.9195 - val_loss: 0.3923 - val_accuracy: 0.8756\n",
      "Epoch 24/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.2137 - accuracy: 0.9262 - val_loss: 0.4239 - val_accuracy: 0.8664\n",
      "Epoch 25/50\n",
      "782/782 [==============================] - 17s 21ms/step - loss: 0.2072 - accuracy: 0.9281 - val_loss: 0.3968 - val_accuracy: 0.8736\n",
      "Epoch 26/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.2062 - accuracy: 0.9282 - val_loss: 0.4474 - val_accuracy: 0.8656\n",
      "Epoch 27/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.1918 - accuracy: 0.9335 - val_loss: 0.4593 - val_accuracy: 0.8567\n",
      "Epoch 28/50\n",
      "782/782 [==============================] - 17s 21ms/step - loss: 0.1876 - accuracy: 0.9346 - val_loss: 0.4623 - val_accuracy: 0.8592\n",
      "Epoch 29/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.1829 - accuracy: 0.9364 - val_loss: 0.3965 - val_accuracy: 0.8812\n",
      "Epoch 30/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.1789 - accuracy: 0.9375 - val_loss: 0.4385 - val_accuracy: 0.8672\n",
      "Epoch 31/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.1677 - accuracy: 0.9410 - val_loss: 0.4847 - val_accuracy: 0.8638\n",
      "Epoch 32/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.1651 - accuracy: 0.9424 - val_loss: 0.5830 - val_accuracy: 0.8388\n",
      "Epoch 33/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.1596 - accuracy: 0.9444 - val_loss: 0.4204 - val_accuracy: 0.8784\n",
      "Epoch 34/50\n",
      "782/782 [==============================] - 17s 21ms/step - loss: 0.1557 - accuracy: 0.9458 - val_loss: 0.3994 - val_accuracy: 0.8851\n",
      "Epoch 35/50\n",
      "782/782 [==============================] - 17s 21ms/step - loss: 0.1493 - accuracy: 0.9454 - val_loss: 0.3998 - val_accuracy: 0.8896\n",
      "Epoch 36/50\n",
      "782/782 [==============================] - 17s 21ms/step - loss: 0.1448 - accuracy: 0.9488 - val_loss: 0.4744 - val_accuracy: 0.8654\n",
      "Epoch 37/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.1413 - accuracy: 0.9514 - val_loss: 0.3845 - val_accuracy: 0.8898\n",
      "Epoch 38/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.1359 - accuracy: 0.9526 - val_loss: 0.4250 - val_accuracy: 0.8779\n",
      "Epoch 39/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.1374 - accuracy: 0.9523 - val_loss: 0.4755 - val_accuracy: 0.8675\n",
      "Epoch 40/50\n",
      "782/782 [==============================] - 17s 21ms/step - loss: 0.1297 - accuracy: 0.9540 - val_loss: 0.5016 - val_accuracy: 0.8675\n",
      "Epoch 41/50\n",
      "782/782 [==============================] - 17s 21ms/step - loss: 0.1237 - accuracy: 0.9566 - val_loss: 0.5633 - val_accuracy: 0.8667\n",
      "Epoch 42/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.1227 - accuracy: 0.9566 - val_loss: 0.4937 - val_accuracy: 0.8695\n",
      "Epoch 43/50\n",
      "782/782 [==============================] - 17s 21ms/step - loss: 0.1180 - accuracy: 0.9588 - val_loss: 0.4838 - val_accuracy: 0.8733\n",
      "Epoch 44/50\n",
      "782/782 [==============================] - 17s 21ms/step - loss: 0.1173 - accuracy: 0.9586 - val_loss: 0.4218 - val_accuracy: 0.8874\n",
      "Epoch 45/50\n",
      "782/782 [==============================] - 17s 21ms/step - loss: 0.1129 - accuracy: 0.9599 - val_loss: 0.4144 - val_accuracy: 0.8930\n",
      "Epoch 46/50\n",
      "782/782 [==============================] - 17s 21ms/step - loss: 0.1104 - accuracy: 0.9615 - val_loss: 0.4254 - val_accuracy: 0.8895\n",
      "Epoch 47/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.1147 - accuracy: 0.9597 - val_loss: 0.3697 - val_accuracy: 0.8958\n",
      "Epoch 48/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.1074 - accuracy: 0.9628 - val_loss: 0.5400 - val_accuracy: 0.8684\n",
      "Epoch 49/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.1036 - accuracy: 0.9638 - val_loss: 0.5100 - val_accuracy: 0.8775\n",
      "Epoch 50/50\n",
      "782/782 [==============================] - 17s 21ms/step - loss: 0.1041 - accuracy: 0.9637 - val_loss: 0.4009 - val_accuracy: 0.8957\n",
      "INFO:tensorflow:Assets written to: models2\\model_1.model\\assets\n",
      "ResNet 1 is being trained...\n",
      "Epoch 1/100\n",
      "782/782 [==============================] - 58s 70ms/step - loss: 1.8671 - accuracy: 0.4706 - val_loss: 1.4377 - val_accuracy: 0.6137\n",
      "Epoch 2/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 1.3267 - accuracy: 0.6402 - val_loss: 1.2303 - val_accuracy: 0.6777\n",
      "Epoch 3/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 1.1166 - accuracy: 0.7119 - val_loss: 1.1494 - val_accuracy: 0.7085\n",
      "Epoch 4/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.9975 - accuracy: 0.7518 - val_loss: 1.1819 - val_accuracy: 0.7136\n",
      "Epoch 5/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.9254 - accuracy: 0.7755 - val_loss: 1.0937 - val_accuracy: 0.7327\n",
      "Epoch 6/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.8669 - accuracy: 0.7966 - val_loss: 0.9405 - val_accuracy: 0.7822\n",
      "Epoch 7/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.8296 - accuracy: 0.8053 - val_loss: 0.8508 - val_accuracy: 0.8004\n",
      "Epoch 8/100\n",
      "782/782 [==============================] - 54s 68ms/step - loss: 0.7974 - accuracy: 0.8148 - val_loss: 0.8635 - val_accuracy: 0.7989\n",
      "Epoch 9/100\n",
      "782/782 [==============================] - 54s 68ms/step - loss: 0.7731 - accuracy: 0.8245 - val_loss: 0.8067 - val_accuracy: 0.8191\n",
      "Epoch 10/100\n",
      "782/782 [==============================] - 54s 68ms/step - loss: 0.7421 - accuracy: 0.8328 - val_loss: 0.8019 - val_accuracy: 0.8143\n",
      "Epoch 11/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.7233 - accuracy: 0.8410 - val_loss: 0.7779 - val_accuracy: 0.8205\n",
      "Epoch 12/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.7134 - accuracy: 0.8420 - val_loss: 0.7662 - val_accuracy: 0.8261\n",
      "Epoch 13/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.6967 - accuracy: 0.8481 - val_loss: 0.8620 - val_accuracy: 0.8023\n",
      "Epoch 14/100\n",
      "782/782 [==============================] - 54s 68ms/step - loss: 0.6791 - accuracy: 0.8534 - val_loss: 0.8231 - val_accuracy: 0.8208\n",
      "Epoch 15/100\n",
      "782/782 [==============================] - 54s 68ms/step - loss: 0.6671 - accuracy: 0.8568 - val_loss: 0.7658 - val_accuracy: 0.8281\n",
      "Epoch 16/100\n",
      "782/782 [==============================] - 53s 68ms/step - loss: 0.6556 - accuracy: 0.8602 - val_loss: 0.6962 - val_accuracy: 0.8498\n",
      "Epoch 17/100\n",
      "782/782 [==============================] - 53s 68ms/step - loss: 0.6483 - accuracy: 0.8631 - val_loss: 0.7350 - val_accuracy: 0.8404\n",
      "Epoch 18/100\n",
      "782/782 [==============================] - 54s 68ms/step - loss: 0.6368 - accuracy: 0.8667 - val_loss: 0.7320 - val_accuracy: 0.8341\n",
      "Epoch 19/100\n",
      "782/782 [==============================] - 54s 68ms/step - loss: 0.6313 - accuracy: 0.8683 - val_loss: 0.6914 - val_accuracy: 0.8542\n",
      "Epoch 20/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.6236 - accuracy: 0.8699 - val_loss: 0.7103 - val_accuracy: 0.8468\n",
      "Epoch 21/100\n",
      "782/782 [==============================] - 53s 68ms/step - loss: 0.6122 - accuracy: 0.8735 - val_loss: 0.6899 - val_accuracy: 0.8530\n",
      "Epoch 22/100\n",
      "782/782 [==============================] - 54s 68ms/step - loss: 0.6078 - accuracy: 0.8747 - val_loss: 0.6678 - val_accuracy: 0.8577\n",
      "Epoch 23/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.5980 - accuracy: 0.8777 - val_loss: 0.6706 - val_accuracy: 0.8547\n",
      "Epoch 24/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.5918 - accuracy: 0.8802 - val_loss: 0.7886 - val_accuracy: 0.8337\n",
      "Epoch 25/100\n",
      "782/782 [==============================] - 53s 68ms/step - loss: 0.5903 - accuracy: 0.8797 - val_loss: 0.6721 - val_accuracy: 0.8585\n",
      "Epoch 26/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.5822 - accuracy: 0.8827 - val_loss: 0.6328 - val_accuracy: 0.8699\n",
      "Epoch 27/100\n",
      "782/782 [==============================] - 53s 68ms/step - loss: 0.5735 - accuracy: 0.8851 - val_loss: 0.7481 - val_accuracy: 0.8361\n",
      "Epoch 28/100\n",
      "782/782 [==============================] - 54s 68ms/step - loss: 0.5739 - accuracy: 0.8845 - val_loss: 0.6334 - val_accuracy: 0.8705\n",
      "Epoch 29/100\n",
      "782/782 [==============================] - 53s 68ms/step - loss: 0.5686 - accuracy: 0.8876 - val_loss: 0.6900 - val_accuracy: 0.8502\n",
      "Epoch 30/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.5631 - accuracy: 0.8892 - val_loss: 0.6419 - val_accuracy: 0.8662\n",
      "Epoch 31/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.5528 - accuracy: 0.8928 - val_loss: 0.6891 - val_accuracy: 0.8545\n",
      "Epoch 32/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.5524 - accuracy: 0.8912 - val_loss: 0.5834 - val_accuracy: 0.8836\n",
      "Epoch 33/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.5477 - accuracy: 0.8931 - val_loss: 0.6022 - val_accuracy: 0.8762\n",
      "Epoch 34/100\n",
      "782/782 [==============================] - 53s 68ms/step - loss: 0.5480 - accuracy: 0.8923 - val_loss: 0.6664 - val_accuracy: 0.8597\n",
      "Epoch 35/100\n",
      "782/782 [==============================] - 53s 68ms/step - loss: 0.5431 - accuracy: 0.8933 - val_loss: 0.6517 - val_accuracy: 0.8625\n",
      "Epoch 36/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.5378 - accuracy: 0.8977 - val_loss: 0.6283 - val_accuracy: 0.8668\n",
      "Epoch 37/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.5317 - accuracy: 0.8975 - val_loss: 0.6738 - val_accuracy: 0.8594\n",
      "Epoch 38/100\n",
      "782/782 [==============================] - 53s 68ms/step - loss: 0.5328 - accuracy: 0.8980 - val_loss: 0.6873 - val_accuracy: 0.8554\n",
      "Epoch 39/100\n",
      "782/782 [==============================] - 54s 68ms/step - loss: 0.5265 - accuracy: 0.8997 - val_loss: 0.6146 - val_accuracy: 0.8741\n",
      "Epoch 40/100\n",
      "782/782 [==============================] - 54s 68ms/step - loss: 0.5274 - accuracy: 0.8975 - val_loss: 0.6199 - val_accuracy: 0.8747\n",
      "Epoch 41/100\n",
      "782/782 [==============================] - 54s 68ms/step - loss: 0.5235 - accuracy: 0.9002 - val_loss: 0.6683 - val_accuracy: 0.8574\n",
      "Epoch 42/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.5213 - accuracy: 0.9004 - val_loss: 0.6877 - val_accuracy: 0.8540\n",
      "Epoch 43/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.5184 - accuracy: 0.8998 - val_loss: 0.6278 - val_accuracy: 0.8674\n",
      "Epoch 44/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.5160 - accuracy: 0.9004 - val_loss: 0.6345 - val_accuracy: 0.8676\n",
      "Epoch 45/100\n",
      "782/782 [==============================] - 54s 68ms/step - loss: 0.5107 - accuracy: 0.9036 - val_loss: 0.6544 - val_accuracy: 0.8648\n",
      "Epoch 46/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.5082 - accuracy: 0.9045 - val_loss: 0.6850 - val_accuracy: 0.8485\n",
      "Epoch 47/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.5056 - accuracy: 0.9054 - val_loss: 0.5993 - val_accuracy: 0.8796\n",
      "Epoch 48/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.5021 - accuracy: 0.9048 - val_loss: 0.5880 - val_accuracy: 0.8824\n",
      "Epoch 49/100\n",
      "782/782 [==============================] - 54s 68ms/step - loss: 0.5039 - accuracy: 0.9046 - val_loss: 0.5776 - val_accuracy: 0.8832\n",
      "Epoch 50/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.4987 - accuracy: 0.9066 - val_loss: 0.6006 - val_accuracy: 0.8802\n",
      "Epoch 51/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.4981 - accuracy: 0.9057 - val_loss: 0.5963 - val_accuracy: 0.8797\n",
      "Epoch 52/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.4933 - accuracy: 0.9080 - val_loss: 0.6038 - val_accuracy: 0.8767\n",
      "Epoch 53/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.4907 - accuracy: 0.9101 - val_loss: 0.6338 - val_accuracy: 0.8646\n",
      "Epoch 54/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.4918 - accuracy: 0.9074 - val_loss: 0.5700 - val_accuracy: 0.8879\n",
      "Epoch 55/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.4858 - accuracy: 0.9109 - val_loss: 0.6270 - val_accuracy: 0.8707\n",
      "Epoch 56/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.4871 - accuracy: 0.9092 - val_loss: 0.5761 - val_accuracy: 0.8854\n",
      "Epoch 57/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.4851 - accuracy: 0.9106 - val_loss: 0.6139 - val_accuracy: 0.8772\n",
      "Epoch 58/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.4836 - accuracy: 0.9109 - val_loss: 0.6318 - val_accuracy: 0.8727\n",
      "Epoch 59/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.4814 - accuracy: 0.9112 - val_loss: 0.5939 - val_accuracy: 0.8769\n",
      "Epoch 60/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.4782 - accuracy: 0.9112 - val_loss: 0.5742 - val_accuracy: 0.8853\n",
      "Epoch 61/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.4755 - accuracy: 0.9127 - val_loss: 0.6394 - val_accuracy: 0.8643\n",
      "Epoch 62/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.4755 - accuracy: 0.9133 - val_loss: 0.5569 - val_accuracy: 0.8881\n",
      "Epoch 63/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.4738 - accuracy: 0.9146 - val_loss: 0.5712 - val_accuracy: 0.8819\n",
      "Epoch 64/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.4728 - accuracy: 0.9113 - val_loss: 0.5722 - val_accuracy: 0.8862\n",
      "Epoch 65/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.4672 - accuracy: 0.9150 - val_loss: 0.6080 - val_accuracy: 0.8756\n",
      "Epoch 66/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.4679 - accuracy: 0.9150 - val_loss: 0.6158 - val_accuracy: 0.8743\n",
      "Epoch 67/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.4680 - accuracy: 0.9143 - val_loss: 0.6167 - val_accuracy: 0.8710\n",
      "Epoch 68/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.4653 - accuracy: 0.9152 - val_loss: 0.5854 - val_accuracy: 0.8828\n",
      "Epoch 69/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.4636 - accuracy: 0.9156 - val_loss: 0.5769 - val_accuracy: 0.8865\n",
      "Epoch 70/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.4605 - accuracy: 0.9160 - val_loss: 0.5960 - val_accuracy: 0.8780\n",
      "Epoch 71/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.4606 - accuracy: 0.9168 - val_loss: 0.6092 - val_accuracy: 0.8714\n",
      "Epoch 72/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.4624 - accuracy: 0.9153 - val_loss: 0.5867 - val_accuracy: 0.8823\n",
      "Epoch 73/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.4540 - accuracy: 0.9186 - val_loss: 0.5571 - val_accuracy: 0.8893\n",
      "Epoch 74/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.4509 - accuracy: 0.9185 - val_loss: 0.5227 - val_accuracy: 0.8966\n",
      "Epoch 75/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.4526 - accuracy: 0.9181 - val_loss: 0.6660 - val_accuracy: 0.8568\n",
      "Epoch 76/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.4549 - accuracy: 0.9173 - val_loss: 0.6195 - val_accuracy: 0.8701\n",
      "Epoch 77/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.4515 - accuracy: 0.9179 - val_loss: 0.5726 - val_accuracy: 0.8843\n",
      "Epoch 78/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.4499 - accuracy: 0.9180 - val_loss: 0.6303 - val_accuracy: 0.8677\n",
      "Epoch 79/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.4489 - accuracy: 0.9187 - val_loss: 0.6213 - val_accuracy: 0.8731\n",
      "Epoch 80/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.4445 - accuracy: 0.9206 - val_loss: 0.5847 - val_accuracy: 0.8867\n",
      "Epoch 81/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.4470 - accuracy: 0.9198 - val_loss: 0.5597 - val_accuracy: 0.8862\n",
      "Epoch 82/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.4509 - accuracy: 0.9184 - val_loss: 0.5669 - val_accuracy: 0.8869\n",
      "Epoch 83/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.4390 - accuracy: 0.9220 - val_loss: 0.5859 - val_accuracy: 0.8834\n",
      "Epoch 84/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.4450 - accuracy: 0.9180 - val_loss: 0.5928 - val_accuracy: 0.8803\n",
      "Epoch 85/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.4471 - accuracy: 0.9204 - val_loss: 0.6205 - val_accuracy: 0.8682\n",
      "Epoch 86/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.4406 - accuracy: 0.9205 - val_loss: 0.6046 - val_accuracy: 0.8756\n",
      "Epoch 87/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.4373 - accuracy: 0.9219 - val_loss: 0.5336 - val_accuracy: 0.8950\n",
      "Epoch 88/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.4409 - accuracy: 0.9198 - val_loss: 0.5867 - val_accuracy: 0.8814\n",
      "Epoch 89/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.4380 - accuracy: 0.9218 - val_loss: 0.6135 - val_accuracy: 0.8738\n",
      "Epoch 90/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.4349 - accuracy: 0.9225 - val_loss: 0.5792 - val_accuracy: 0.8823\n",
      "Epoch 91/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.4383 - accuracy: 0.9212 - val_loss: 0.5293 - val_accuracy: 0.8948\n",
      "Epoch 92/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.4337 - accuracy: 0.9222 - val_loss: 0.5353 - val_accuracy: 0.8910\n",
      "Epoch 93/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.4311 - accuracy: 0.9232 - val_loss: 0.5248 - val_accuracy: 0.8958\n",
      "Epoch 94/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.4314 - accuracy: 0.9231 - val_loss: 0.5911 - val_accuracy: 0.8797\n",
      "Epoch 95/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.4334 - accuracy: 0.9231 - val_loss: 0.6314 - val_accuracy: 0.8680\n",
      "Epoch 96/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.4271 - accuracy: 0.9248 - val_loss: 0.5472 - val_accuracy: 0.8885\n",
      "Epoch 97/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.4321 - accuracy: 0.9227 - val_loss: 0.5264 - val_accuracy: 0.8994\n",
      "Epoch 98/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.4271 - accuracy: 0.9242 - val_loss: 0.5418 - val_accuracy: 0.8938\n",
      "Epoch 99/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.4260 - accuracy: 0.9249 - val_loss: 0.5393 - val_accuracy: 0.8958\n",
      "Epoch 100/100\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.4283 - accuracy: 0.9242 - val_loss: 0.5284 - val_accuracy: 0.8967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ryan\\miniconda3\\envs\\TF-2.5\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models2\\model_2.model\\assets\n",
      "ResNet 2 is being trained...\n",
      "Epoch 1/100\n",
      "782/782 [==============================] - 57s 68ms/step - loss: 1.8598 - accuracy: 0.4720 - val_loss: 1.6271 - val_accuracy: 0.5490\n",
      "Epoch 2/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 1.3348 - accuracy: 0.6364 - val_loss: 1.3786 - val_accuracy: 0.6392\n",
      "Epoch 3/100\n",
      "782/782 [==============================] - 52s 67ms/step - loss: 1.1348 - accuracy: 0.7050 - val_loss: 1.0792 - val_accuracy: 0.7263\n",
      "Epoch 4/100\n",
      "782/782 [==============================] - 52s 67ms/step - loss: 1.0190 - accuracy: 0.7432 - val_loss: 0.9795 - val_accuracy: 0.7614\n",
      "Epoch 5/100\n",
      "782/782 [==============================] - 52s 67ms/step - loss: 0.9438 - accuracy: 0.7702 - val_loss: 1.0125 - val_accuracy: 0.7473\n",
      "Epoch 6/100\n",
      "782/782 [==============================] - 52s 67ms/step - loss: 0.8875 - accuracy: 0.7883 - val_loss: 0.9546 - val_accuracy: 0.7686\n",
      "Epoch 7/100\n",
      "782/782 [==============================] - 52s 67ms/step - loss: 0.8440 - accuracy: 0.8034 - val_loss: 0.8612 - val_accuracy: 0.7979\n",
      "Epoch 8/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.8113 - accuracy: 0.8110 - val_loss: 0.9915 - val_accuracy: 0.7587\n",
      "Epoch 9/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.7780 - accuracy: 0.8209 - val_loss: 0.8263 - val_accuracy: 0.8072\n",
      "Epoch 10/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.7547 - accuracy: 0.8295 - val_loss: 0.8305 - val_accuracy: 0.8096\n",
      "Epoch 11/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.7345 - accuracy: 0.8349 - val_loss: 0.7706 - val_accuracy: 0.8255\n",
      "Epoch 12/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.7199 - accuracy: 0.8426 - val_loss: 0.8231 - val_accuracy: 0.8068\n",
      "Epoch 13/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.7020 - accuracy: 0.8456 - val_loss: 0.7504 - val_accuracy: 0.8349\n",
      "Epoch 14/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.6800 - accuracy: 0.8522 - val_loss: 0.7619 - val_accuracy: 0.8249\n",
      "Epoch 15/100\n",
      "782/782 [==============================] - 52s 67ms/step - loss: 0.6743 - accuracy: 0.8541 - val_loss: 0.7216 - val_accuracy: 0.8434\n",
      "Epoch 16/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.6615 - accuracy: 0.8604 - val_loss: 0.7424 - val_accuracy: 0.8329\n",
      "Epoch 17/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.6534 - accuracy: 0.8621 - val_loss: 0.7818 - val_accuracy: 0.8211\n",
      "Epoch 18/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.6412 - accuracy: 0.8644 - val_loss: 0.8823 - val_accuracy: 0.8034\n",
      "Epoch 19/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.6358 - accuracy: 0.8671 - val_loss: 0.7702 - val_accuracy: 0.8293\n",
      "Epoch 20/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.6249 - accuracy: 0.8688 - val_loss: 0.7695 - val_accuracy: 0.8308\n",
      "Epoch 21/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.6137 - accuracy: 0.8743 - val_loss: 0.8063 - val_accuracy: 0.8203\n",
      "Epoch 22/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.6086 - accuracy: 0.8745 - val_loss: 0.6960 - val_accuracy: 0.8485\n",
      "Epoch 23/100\n",
      "782/782 [==============================] - 52s 67ms/step - loss: 0.6044 - accuracy: 0.8779 - val_loss: 0.7878 - val_accuracy: 0.8207\n",
      "Epoch 24/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.5986 - accuracy: 0.8771 - val_loss: 0.7157 - val_accuracy: 0.8451\n",
      "Epoch 25/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.5876 - accuracy: 0.8820 - val_loss: 0.6848 - val_accuracy: 0.8527\n",
      "Epoch 26/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.5819 - accuracy: 0.8833 - val_loss: 0.6448 - val_accuracy: 0.8652\n",
      "Epoch 27/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.5803 - accuracy: 0.8825 - val_loss: 0.6870 - val_accuracy: 0.8520\n",
      "Epoch 28/100\n",
      "782/782 [==============================] - 53s 68ms/step - loss: 0.5736 - accuracy: 0.8850 - val_loss: 0.6520 - val_accuracy: 0.8584\n",
      "Epoch 29/100\n",
      "782/782 [==============================] - 52s 67ms/step - loss: 0.5663 - accuracy: 0.8869 - val_loss: 0.6968 - val_accuracy: 0.8502\n",
      "Epoch 30/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.5628 - accuracy: 0.8892 - val_loss: 0.6422 - val_accuracy: 0.8653\n",
      "Epoch 31/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.5619 - accuracy: 0.8888 - val_loss: 0.7164 - val_accuracy: 0.8430\n",
      "Epoch 32/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.5548 - accuracy: 0.8913 - val_loss: 0.6869 - val_accuracy: 0.8507\n",
      "Epoch 33/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.5495 - accuracy: 0.8932 - val_loss: 0.6939 - val_accuracy: 0.8464\n",
      "Epoch 34/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.5473 - accuracy: 0.8932 - val_loss: 0.6768 - val_accuracy: 0.8509\n",
      "Epoch 35/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.5415 - accuracy: 0.8942 - val_loss: 0.6123 - val_accuracy: 0.8747\n",
      "Epoch 36/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.5421 - accuracy: 0.8933 - val_loss: 0.7051 - val_accuracy: 0.8515\n",
      "Epoch 37/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.5362 - accuracy: 0.8962 - val_loss: 0.6151 - val_accuracy: 0.8724\n",
      "Epoch 38/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.5289 - accuracy: 0.8987 - val_loss: 0.6481 - val_accuracy: 0.8630\n",
      "Epoch 39/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.5376 - accuracy: 0.8944 - val_loss: 0.7265 - val_accuracy: 0.8394\n",
      "Epoch 40/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.5263 - accuracy: 0.8978 - val_loss: 0.5869 - val_accuracy: 0.8780\n",
      "Epoch 41/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.5234 - accuracy: 0.8997 - val_loss: 0.5901 - val_accuracy: 0.8789\n",
      "Epoch 42/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.5195 - accuracy: 0.9002 - val_loss: 0.6468 - val_accuracy: 0.8643\n",
      "Epoch 43/100\n",
      "782/782 [==============================] - 52s 67ms/step - loss: 0.5193 - accuracy: 0.9007 - val_loss: 0.6926 - val_accuracy: 0.8488\n",
      "Epoch 44/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.5196 - accuracy: 0.9015 - val_loss: 0.6135 - val_accuracy: 0.8733\n",
      "Epoch 45/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.5136 - accuracy: 0.9023 - val_loss: 0.6004 - val_accuracy: 0.8792\n",
      "Epoch 46/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.5090 - accuracy: 0.9028 - val_loss: 0.6876 - val_accuracy: 0.8505\n",
      "Epoch 47/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.5149 - accuracy: 0.9008 - val_loss: 0.6151 - val_accuracy: 0.8715\n",
      "Epoch 48/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.5077 - accuracy: 0.9021 - val_loss: 0.6468 - val_accuracy: 0.8658\n",
      "Epoch 49/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.5070 - accuracy: 0.9038 - val_loss: 0.5505 - val_accuracy: 0.8901\n",
      "Epoch 50/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.5031 - accuracy: 0.9036 - val_loss: 0.6834 - val_accuracy: 0.8540\n",
      "Epoch 51/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.5011 - accuracy: 0.9042 - val_loss: 0.5714 - val_accuracy: 0.8875\n",
      "Epoch 52/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.4938 - accuracy: 0.9076 - val_loss: 0.5990 - val_accuracy: 0.8774\n",
      "Epoch 53/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.4908 - accuracy: 0.9082 - val_loss: 0.5936 - val_accuracy: 0.8791\n",
      "Epoch 54/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.4862 - accuracy: 0.9117 - val_loss: 0.5725 - val_accuracy: 0.8825\n",
      "Epoch 55/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.4884 - accuracy: 0.9096 - val_loss: 0.6029 - val_accuracy: 0.8698\n",
      "Epoch 56/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.4875 - accuracy: 0.9098 - val_loss: 0.6359 - val_accuracy: 0.8690\n",
      "Epoch 57/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.4866 - accuracy: 0.9087 - val_loss: 0.6049 - val_accuracy: 0.8748\n",
      "Epoch 58/100\n",
      "782/782 [==============================] - 53s 68ms/step - loss: 0.4859 - accuracy: 0.9085 - val_loss: 0.6337 - val_accuracy: 0.8674\n",
      "Epoch 59/100\n",
      "782/782 [==============================] - 53s 68ms/step - loss: 0.4853 - accuracy: 0.9087 - val_loss: 0.6758 - val_accuracy: 0.8594\n",
      "Epoch 60/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.4741 - accuracy: 0.9120 - val_loss: 0.6019 - val_accuracy: 0.8792\n",
      "Epoch 61/100\n",
      "782/782 [==============================] - 53s 68ms/step - loss: 0.4740 - accuracy: 0.9118 - val_loss: 0.6350 - val_accuracy: 0.8714\n",
      "Epoch 62/100\n",
      "782/782 [==============================] - 53s 68ms/step - loss: 0.4780 - accuracy: 0.9105 - val_loss: 0.5463 - val_accuracy: 0.8935\n",
      "Epoch 63/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.4761 - accuracy: 0.9116 - val_loss: 0.7081 - val_accuracy: 0.8479\n",
      "Epoch 64/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.4701 - accuracy: 0.9119 - val_loss: 0.5457 - val_accuracy: 0.8923\n",
      "Epoch 65/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.4691 - accuracy: 0.9136 - val_loss: 0.6039 - val_accuracy: 0.8765\n",
      "Epoch 66/100\n",
      "782/782 [==============================] - 53s 68ms/step - loss: 0.4661 - accuracy: 0.9150 - val_loss: 0.5756 - val_accuracy: 0.8824\n",
      "Epoch 67/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.4708 - accuracy: 0.9130 - val_loss: 0.6575 - val_accuracy: 0.8552\n",
      "Epoch 68/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.4609 - accuracy: 0.9166 - val_loss: 0.5719 - val_accuracy: 0.8857\n",
      "Epoch 69/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.4682 - accuracy: 0.9137 - val_loss: 0.5512 - val_accuracy: 0.8897\n",
      "Epoch 70/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.4627 - accuracy: 0.9160 - val_loss: 0.5996 - val_accuracy: 0.8772\n",
      "Epoch 71/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.4626 - accuracy: 0.9151 - val_loss: 0.6298 - val_accuracy: 0.8665\n",
      "Epoch 72/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.4579 - accuracy: 0.9171 - val_loss: 0.6229 - val_accuracy: 0.8727\n",
      "Epoch 73/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.4548 - accuracy: 0.9174 - val_loss: 0.5408 - val_accuracy: 0.8909\n",
      "Epoch 74/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.4542 - accuracy: 0.9179 - val_loss: 0.5874 - val_accuracy: 0.8783\n",
      "Epoch 75/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.4563 - accuracy: 0.9163 - val_loss: 0.5510 - val_accuracy: 0.8863\n",
      "Epoch 76/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.4575 - accuracy: 0.9165 - val_loss: 0.6006 - val_accuracy: 0.8755\n",
      "Epoch 77/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.4528 - accuracy: 0.9177 - val_loss: 0.5441 - val_accuracy: 0.8917\n",
      "Epoch 78/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.4497 - accuracy: 0.9184 - val_loss: 0.5654 - val_accuracy: 0.8840\n",
      "Epoch 79/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.4466 - accuracy: 0.9192 - val_loss: 0.6628 - val_accuracy: 0.8603\n",
      "Epoch 80/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.4484 - accuracy: 0.9180 - val_loss: 0.5324 - val_accuracy: 0.8956\n",
      "Epoch 81/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.4505 - accuracy: 0.9183 - val_loss: 0.5517 - val_accuracy: 0.8864\n",
      "Epoch 82/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.4475 - accuracy: 0.9181 - val_loss: 0.5687 - val_accuracy: 0.8886\n",
      "Epoch 83/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.4444 - accuracy: 0.9197 - val_loss: 0.5368 - val_accuracy: 0.8932\n",
      "Epoch 84/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.4436 - accuracy: 0.9203 - val_loss: 0.5449 - val_accuracy: 0.8916\n",
      "Epoch 85/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.4415 - accuracy: 0.9206 - val_loss: 0.6134 - val_accuracy: 0.8683\n",
      "Epoch 86/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.4407 - accuracy: 0.9212 - val_loss: 0.5405 - val_accuracy: 0.8912\n",
      "Epoch 87/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.4397 - accuracy: 0.9207 - val_loss: 0.6316 - val_accuracy: 0.8667\n",
      "Epoch 88/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.4374 - accuracy: 0.9216 - val_loss: 0.5703 - val_accuracy: 0.8844\n",
      "Epoch 89/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.4366 - accuracy: 0.9218 - val_loss: 0.5116 - val_accuracy: 0.9017\n",
      "Epoch 90/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.4375 - accuracy: 0.9210 - val_loss: 0.5852 - val_accuracy: 0.8826\n",
      "Epoch 91/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.4351 - accuracy: 0.9222 - val_loss: 0.5970 - val_accuracy: 0.8744\n",
      "Epoch 92/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.4366 - accuracy: 0.9206 - val_loss: 0.5926 - val_accuracy: 0.8751\n",
      "Epoch 93/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.4343 - accuracy: 0.9199 - val_loss: 0.5204 - val_accuracy: 0.8971\n",
      "Epoch 94/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.4335 - accuracy: 0.9226 - val_loss: 0.5345 - val_accuracy: 0.8922\n",
      "Epoch 95/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.4349 - accuracy: 0.9218 - val_loss: 0.5358 - val_accuracy: 0.8922\n",
      "Epoch 96/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.4303 - accuracy: 0.9232 - val_loss: 0.5153 - val_accuracy: 0.8957\n",
      "Epoch 97/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.4274 - accuracy: 0.9231 - val_loss: 0.5634 - val_accuracy: 0.8876\n",
      "Epoch 98/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.4269 - accuracy: 0.9235 - val_loss: 0.5977 - val_accuracy: 0.8728\n",
      "Epoch 99/100\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.4333 - accuracy: 0.9216 - val_loss: 0.6109 - val_accuracy: 0.8747\n",
      "Epoch 100/100\n",
      "782/782 [==============================] - 53s 68ms/step - loss: 0.4256 - accuracy: 0.9227 - val_loss: 0.5515 - val_accuracy: 0.8883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ryan\\miniconda3\\envs\\TF-2.5\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models2\\model_3.model\\assets\n",
      "VGGNet 1 is being trained...\n",
      "Epoch 1/100\n",
      "782/782 [==============================] - 13s 16ms/step - loss: 1.6478 - accuracy: 0.4336 - val_loss: 1.3050 - val_accuracy: 0.5338\n",
      "Epoch 2/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 1.1817 - accuracy: 0.5782 - val_loss: 1.0846 - val_accuracy: 0.6241\n",
      "Epoch 3/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 1.0298 - accuracy: 0.6367 - val_loss: 0.9022 - val_accuracy: 0.6849\n",
      "Epoch 4/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.9354 - accuracy: 0.6704 - val_loss: 0.8129 - val_accuracy: 0.7241\n",
      "Epoch 5/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.8700 - accuracy: 0.6952 - val_loss: 0.7911 - val_accuracy: 0.7191\n",
      "Epoch 6/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.8185 - accuracy: 0.7125 - val_loss: 0.7774 - val_accuracy: 0.7367\n",
      "Epoch 7/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.7901 - accuracy: 0.7248 - val_loss: 0.7590 - val_accuracy: 0.7379\n",
      "Epoch 8/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.7611 - accuracy: 0.7345 - val_loss: 0.8305 - val_accuracy: 0.7164\n",
      "Epoch 9/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.7370 - accuracy: 0.7426 - val_loss: 0.6538 - val_accuracy: 0.7767\n",
      "Epoch 10/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.7093 - accuracy: 0.7522 - val_loss: 0.6687 - val_accuracy: 0.7716\n",
      "Epoch 11/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.6971 - accuracy: 0.7574 - val_loss: 0.8311 - val_accuracy: 0.7230\n",
      "Epoch 12/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.6746 - accuracy: 0.7639 - val_loss: 0.6237 - val_accuracy: 0.7873\n",
      "Epoch 13/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.6576 - accuracy: 0.7699 - val_loss: 0.6121 - val_accuracy: 0.7897\n",
      "Epoch 14/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.6468 - accuracy: 0.7768 - val_loss: 0.6229 - val_accuracy: 0.7922\n",
      "Epoch 15/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.6362 - accuracy: 0.7780 - val_loss: 0.6648 - val_accuracy: 0.7782\n",
      "Epoch 16/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.6211 - accuracy: 0.7837 - val_loss: 0.5735 - val_accuracy: 0.8054\n",
      "Epoch 17/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.6066 - accuracy: 0.7885 - val_loss: 0.5607 - val_accuracy: 0.8072\n",
      "Epoch 18/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.6023 - accuracy: 0.7903 - val_loss: 0.6186 - val_accuracy: 0.7937\n",
      "Epoch 19/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.5887 - accuracy: 0.7955 - val_loss: 0.5945 - val_accuracy: 0.8040\n",
      "Epoch 20/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.5825 - accuracy: 0.7960 - val_loss: 0.5360 - val_accuracy: 0.8204\n",
      "Epoch 21/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.5677 - accuracy: 0.8034 - val_loss: 0.6689 - val_accuracy: 0.7758\n",
      "Epoch 22/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.5702 - accuracy: 0.8020 - val_loss: 0.5911 - val_accuracy: 0.8036\n",
      "Epoch 23/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.5621 - accuracy: 0.8039 - val_loss: 0.5648 - val_accuracy: 0.8109\n",
      "Epoch 24/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.5566 - accuracy: 0.8061 - val_loss: 0.5373 - val_accuracy: 0.8195\n",
      "Epoch 25/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.5431 - accuracy: 0.8104 - val_loss: 0.9269 - val_accuracy: 0.7196\n",
      "Epoch 26/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.5305 - accuracy: 0.8142 - val_loss: 0.5264 - val_accuracy: 0.8232\n",
      "Epoch 27/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.5295 - accuracy: 0.8152 - val_loss: 0.5358 - val_accuracy: 0.8231\n",
      "Epoch 28/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.5205 - accuracy: 0.8174 - val_loss: 0.5429 - val_accuracy: 0.8219\n",
      "Epoch 29/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.5181 - accuracy: 0.8191 - val_loss: 0.5246 - val_accuracy: 0.8254\n",
      "Epoch 30/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.5178 - accuracy: 0.8193 - val_loss: 0.5060 - val_accuracy: 0.8326\n",
      "Epoch 31/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.5058 - accuracy: 0.8236 - val_loss: 0.5279 - val_accuracy: 0.8199\n",
      "Epoch 32/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.5009 - accuracy: 0.8266 - val_loss: 0.4807 - val_accuracy: 0.8371\n",
      "Epoch 33/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.4994 - accuracy: 0.8262 - val_loss: 0.5997 - val_accuracy: 0.8110\n",
      "Epoch 34/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.4980 - accuracy: 0.8289 - val_loss: 0.5783 - val_accuracy: 0.8129\n",
      "Epoch 35/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.4928 - accuracy: 0.8276 - val_loss: 0.4873 - val_accuracy: 0.8358\n",
      "Epoch 36/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.4860 - accuracy: 0.8325 - val_loss: 0.5366 - val_accuracy: 0.8207\n",
      "Epoch 37/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.4821 - accuracy: 0.8320 - val_loss: 0.5162 - val_accuracy: 0.8266\n",
      "Epoch 38/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.4798 - accuracy: 0.8329 - val_loss: 0.4525 - val_accuracy: 0.8473\n",
      "Epoch 39/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.4782 - accuracy: 0.8314 - val_loss: 0.5023 - val_accuracy: 0.8321\n",
      "Epoch 40/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.4788 - accuracy: 0.8345 - val_loss: 0.4856 - val_accuracy: 0.8393\n",
      "Epoch 41/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.4686 - accuracy: 0.8361 - val_loss: 0.4790 - val_accuracy: 0.8368\n",
      "Epoch 42/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.4688 - accuracy: 0.8364 - val_loss: 0.4597 - val_accuracy: 0.8459\n",
      "Epoch 43/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.4678 - accuracy: 0.8353 - val_loss: 0.5842 - val_accuracy: 0.8098\n",
      "Epoch 44/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.4612 - accuracy: 0.8399 - val_loss: 0.5283 - val_accuracy: 0.8285\n",
      "Epoch 45/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.4565 - accuracy: 0.8410 - val_loss: 0.5391 - val_accuracy: 0.8257\n",
      "Epoch 46/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.4524 - accuracy: 0.8435 - val_loss: 0.5369 - val_accuracy: 0.8274\n",
      "Epoch 47/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.4545 - accuracy: 0.8419 - val_loss: 0.5806 - val_accuracy: 0.8100\n",
      "Epoch 48/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.4494 - accuracy: 0.8433 - val_loss: 0.4512 - val_accuracy: 0.8477\n",
      "Epoch 49/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.4518 - accuracy: 0.8422 - val_loss: 0.5258 - val_accuracy: 0.8287\n",
      "Epoch 50/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.4488 - accuracy: 0.8433 - val_loss: 0.4281 - val_accuracy: 0.8557\n",
      "Epoch 51/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.4405 - accuracy: 0.8454 - val_loss: 0.4889 - val_accuracy: 0.8382\n",
      "Epoch 52/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.4387 - accuracy: 0.8470 - val_loss: 0.4352 - val_accuracy: 0.8545\n",
      "Epoch 53/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.4385 - accuracy: 0.8472 - val_loss: 0.4550 - val_accuracy: 0.8495\n",
      "Epoch 54/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.4352 - accuracy: 0.8482 - val_loss: 0.4153 - val_accuracy: 0.8596\n",
      "Epoch 55/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.4364 - accuracy: 0.8510 - val_loss: 0.6120 - val_accuracy: 0.8100\n",
      "Epoch 56/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.4300 - accuracy: 0.8499 - val_loss: 0.4794 - val_accuracy: 0.8463\n",
      "Epoch 57/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.4283 - accuracy: 0.8500 - val_loss: 0.5003 - val_accuracy: 0.8375\n",
      "Epoch 58/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.4256 - accuracy: 0.8514 - val_loss: 0.4276 - val_accuracy: 0.8623\n",
      "Epoch 59/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.4253 - accuracy: 0.8515 - val_loss: 0.4462 - val_accuracy: 0.8500\n",
      "Epoch 60/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.4203 - accuracy: 0.8547 - val_loss: 0.4516 - val_accuracy: 0.8516\n",
      "Epoch 61/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.4179 - accuracy: 0.8546 - val_loss: 0.4204 - val_accuracy: 0.8613\n",
      "Epoch 62/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.4223 - accuracy: 0.8525 - val_loss: 0.4887 - val_accuracy: 0.8369\n",
      "Epoch 63/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.4179 - accuracy: 0.8534 - val_loss: 0.4301 - val_accuracy: 0.8549\n",
      "Epoch 64/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.4214 - accuracy: 0.8531 - val_loss: 0.4686 - val_accuracy: 0.8437\n",
      "Epoch 65/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.4120 - accuracy: 0.8555 - val_loss: 0.4786 - val_accuracy: 0.8427\n",
      "Epoch 66/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.4155 - accuracy: 0.8549 - val_loss: 0.4421 - val_accuracy: 0.8545\n",
      "Epoch 67/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.4138 - accuracy: 0.8563 - val_loss: 0.4731 - val_accuracy: 0.8471\n",
      "Epoch 68/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.4130 - accuracy: 0.8553 - val_loss: 0.4308 - val_accuracy: 0.8590\n",
      "Epoch 69/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.4087 - accuracy: 0.8581 - val_loss: 0.4320 - val_accuracy: 0.8548\n",
      "Epoch 70/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.4074 - accuracy: 0.8579 - val_loss: 0.5267 - val_accuracy: 0.8322\n",
      "Epoch 71/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.3996 - accuracy: 0.8607 - val_loss: 0.4308 - val_accuracy: 0.8560\n",
      "Epoch 72/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.4022 - accuracy: 0.8616 - val_loss: 0.4830 - val_accuracy: 0.8425\n",
      "Epoch 73/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.4054 - accuracy: 0.8579 - val_loss: 0.4406 - val_accuracy: 0.8550\n",
      "Epoch 74/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.3984 - accuracy: 0.8594 - val_loss: 0.5347 - val_accuracy: 0.8361\n",
      "Epoch 75/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.3938 - accuracy: 0.8623 - val_loss: 0.4618 - val_accuracy: 0.8474\n",
      "Epoch 76/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.3949 - accuracy: 0.8618 - val_loss: 0.6033 - val_accuracy: 0.8117\n",
      "Epoch 77/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.3928 - accuracy: 0.8628 - val_loss: 0.4925 - val_accuracy: 0.8425\n",
      "Epoch 78/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.3919 - accuracy: 0.8641 - val_loss: 0.4540 - val_accuracy: 0.8527\n",
      "Epoch 79/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.3965 - accuracy: 0.8635 - val_loss: 0.4135 - val_accuracy: 0.8644\n",
      "Epoch 80/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.3880 - accuracy: 0.8619 - val_loss: 0.4803 - val_accuracy: 0.8478\n",
      "Epoch 81/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.3936 - accuracy: 0.8619 - val_loss: 0.3983 - val_accuracy: 0.8681\n",
      "Epoch 82/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.3860 - accuracy: 0.8656 - val_loss: 0.4023 - val_accuracy: 0.8658\n",
      "Epoch 83/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.3823 - accuracy: 0.8668 - val_loss: 0.4298 - val_accuracy: 0.8577\n",
      "Epoch 84/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.3800 - accuracy: 0.8671 - val_loss: 0.4545 - val_accuracy: 0.8545\n",
      "Epoch 85/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.3840 - accuracy: 0.8643 - val_loss: 0.4486 - val_accuracy: 0.8515\n",
      "Epoch 86/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.3818 - accuracy: 0.8659 - val_loss: 0.4850 - val_accuracy: 0.8448\n",
      "Epoch 87/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.3860 - accuracy: 0.8648 - val_loss: 0.4672 - val_accuracy: 0.8489\n",
      "Epoch 88/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.3841 - accuracy: 0.8652 - val_loss: 0.4058 - val_accuracy: 0.8648\n",
      "Epoch 89/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.3864 - accuracy: 0.8657 - val_loss: 0.4024 - val_accuracy: 0.8645\n",
      "Epoch 90/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.3790 - accuracy: 0.8671 - val_loss: 0.4194 - val_accuracy: 0.8618\n",
      "Epoch 91/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.3723 - accuracy: 0.8703 - val_loss: 0.4621 - val_accuracy: 0.8513\n",
      "Epoch 92/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.3744 - accuracy: 0.8691 - val_loss: 0.3882 - val_accuracy: 0.8722\n",
      "Epoch 93/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.3776 - accuracy: 0.8671 - val_loss: 0.4218 - val_accuracy: 0.8620\n",
      "Epoch 94/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.3745 - accuracy: 0.8693 - val_loss: 0.3896 - val_accuracy: 0.8711\n",
      "Epoch 95/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.3722 - accuracy: 0.8704 - val_loss: 0.4249 - val_accuracy: 0.8602\n",
      "Epoch 96/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.3662 - accuracy: 0.8715 - val_loss: 0.4327 - val_accuracy: 0.8587\n",
      "Epoch 97/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.3684 - accuracy: 0.8726 - val_loss: 0.3775 - val_accuracy: 0.8755\n",
      "Epoch 98/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.3674 - accuracy: 0.8712 - val_loss: 0.4521 - val_accuracy: 0.8561\n",
      "Epoch 99/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.3744 - accuracy: 0.8691 - val_loss: 0.4172 - val_accuracy: 0.8607\n",
      "Epoch 100/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.3661 - accuracy: 0.8701 - val_loss: 0.4870 - val_accuracy: 0.8462\n",
      "INFO:tensorflow:Assets written to: models2\\model_4.model\\assets\n",
      "VGGNet 2 is being trained...\n",
      "Epoch 1/100\n",
      "782/782 [==============================] - 13s 16ms/step - loss: 1.6044 - accuracy: 0.4461 - val_loss: 1.1458 - val_accuracy: 0.5907\n",
      "Epoch 2/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 1.1602 - accuracy: 0.5855 - val_loss: 1.6129 - val_accuracy: 0.5023\n",
      "Epoch 3/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 1.0311 - accuracy: 0.6344 - val_loss: 1.4528 - val_accuracy: 0.5574\n",
      "Epoch 4/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.9428 - accuracy: 0.6672 - val_loss: 0.8714 - val_accuracy: 0.6987\n",
      "Epoch 5/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.8826 - accuracy: 0.6893 - val_loss: 0.7583 - val_accuracy: 0.7427\n",
      "Epoch 6/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.8267 - accuracy: 0.7103 - val_loss: 0.8259 - val_accuracy: 0.7223\n",
      "Epoch 7/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.7901 - accuracy: 0.7232 - val_loss: 0.8587 - val_accuracy: 0.7142\n",
      "Epoch 8/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.7656 - accuracy: 0.7324 - val_loss: 0.7729 - val_accuracy: 0.7406\n",
      "Epoch 9/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.7422 - accuracy: 0.7401 - val_loss: 0.8739 - val_accuracy: 0.7116\n",
      "Epoch 10/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.7145 - accuracy: 0.7488 - val_loss: 0.6672 - val_accuracy: 0.7702\n",
      "Epoch 11/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.6966 - accuracy: 0.7568 - val_loss: 0.7598 - val_accuracy: 0.7409\n",
      "Epoch 12/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.6830 - accuracy: 0.7603 - val_loss: 0.6273 - val_accuracy: 0.7867\n",
      "Epoch 13/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.6590 - accuracy: 0.7694 - val_loss: 0.6729 - val_accuracy: 0.7757\n",
      "Epoch 14/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.6529 - accuracy: 0.7718 - val_loss: 0.6603 - val_accuracy: 0.7780\n",
      "Epoch 15/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.6398 - accuracy: 0.7773 - val_loss: 0.6031 - val_accuracy: 0.7968\n",
      "Epoch 16/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.6242 - accuracy: 0.7824 - val_loss: 0.6528 - val_accuracy: 0.7825\n",
      "Epoch 17/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.6100 - accuracy: 0.7889 - val_loss: 0.5998 - val_accuracy: 0.7963\n",
      "Epoch 18/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.6062 - accuracy: 0.7886 - val_loss: 0.7679 - val_accuracy: 0.7522\n",
      "Epoch 19/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.5939 - accuracy: 0.7938 - val_loss: 0.6444 - val_accuracy: 0.7870\n",
      "Epoch 20/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.5810 - accuracy: 0.7953 - val_loss: 0.6997 - val_accuracy: 0.7699\n",
      "Epoch 21/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.5686 - accuracy: 0.8014 - val_loss: 0.5551 - val_accuracy: 0.8132\n",
      "Epoch 22/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.5614 - accuracy: 0.8059 - val_loss: 0.5408 - val_accuracy: 0.8176\n",
      "Epoch 23/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.5629 - accuracy: 0.8038 - val_loss: 0.5531 - val_accuracy: 0.8135\n",
      "Epoch 24/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.5489 - accuracy: 0.8085 - val_loss: 0.6128 - val_accuracy: 0.7973\n",
      "Epoch 25/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.5466 - accuracy: 0.8093 - val_loss: 0.6151 - val_accuracy: 0.7988\n",
      "Epoch 26/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.5347 - accuracy: 0.8146 - val_loss: 0.5544 - val_accuracy: 0.8174\n",
      "Epoch 27/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.5300 - accuracy: 0.8151 - val_loss: 0.5601 - val_accuracy: 0.8148\n",
      "Epoch 28/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.5258 - accuracy: 0.8167 - val_loss: 0.5654 - val_accuracy: 0.8121\n",
      "Epoch 29/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.5183 - accuracy: 0.8204 - val_loss: 0.5530 - val_accuracy: 0.8134\n",
      "Epoch 30/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.5160 - accuracy: 0.8196 - val_loss: 0.5176 - val_accuracy: 0.8260\n",
      "Epoch 31/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.5080 - accuracy: 0.8219 - val_loss: 0.5789 - val_accuracy: 0.8092\n",
      "Epoch 32/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.5092 - accuracy: 0.8229 - val_loss: 0.5776 - val_accuracy: 0.8054\n",
      "Epoch 33/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.4944 - accuracy: 0.8268 - val_loss: 0.6589 - val_accuracy: 0.7833\n",
      "Epoch 34/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.4968 - accuracy: 0.8255 - val_loss: 0.4844 - val_accuracy: 0.8354\n",
      "Epoch 35/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.4890 - accuracy: 0.8294 - val_loss: 0.5319 - val_accuracy: 0.8218\n",
      "Epoch 36/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.4814 - accuracy: 0.8303 - val_loss: 0.5186 - val_accuracy: 0.8298\n",
      "Epoch 37/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.4801 - accuracy: 0.8317 - val_loss: 0.5152 - val_accuracy: 0.8318\n",
      "Epoch 38/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.4776 - accuracy: 0.8345 - val_loss: 0.4745 - val_accuracy: 0.8374\n",
      "Epoch 39/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.4778 - accuracy: 0.8328 - val_loss: 0.5025 - val_accuracy: 0.8340\n",
      "Epoch 40/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.4754 - accuracy: 0.8342 - val_loss: 0.5592 - val_accuracy: 0.8172\n",
      "Epoch 41/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.4701 - accuracy: 0.8355 - val_loss: 0.4731 - val_accuracy: 0.8424\n",
      "Epoch 42/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.4633 - accuracy: 0.8382 - val_loss: 0.4801 - val_accuracy: 0.8381\n",
      "Epoch 43/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.4600 - accuracy: 0.8376 - val_loss: 0.5024 - val_accuracy: 0.8344\n",
      "Epoch 44/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.4553 - accuracy: 0.8421 - val_loss: 0.4871 - val_accuracy: 0.8380\n",
      "Epoch 45/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.4576 - accuracy: 0.8408 - val_loss: 0.4482 - val_accuracy: 0.8503\n",
      "Epoch 46/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.4519 - accuracy: 0.8411 - val_loss: 0.5032 - val_accuracy: 0.8363\n",
      "Epoch 47/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.4504 - accuracy: 0.8433 - val_loss: 0.5017 - val_accuracy: 0.8357\n",
      "Epoch 48/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.4492 - accuracy: 0.8429 - val_loss: 0.4909 - val_accuracy: 0.8391\n",
      "Epoch 49/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.4464 - accuracy: 0.8434 - val_loss: 0.4803 - val_accuracy: 0.8403\n",
      "Epoch 50/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.4445 - accuracy: 0.8430 - val_loss: 0.4752 - val_accuracy: 0.8416\n",
      "Epoch 51/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.4402 - accuracy: 0.8475 - val_loss: 0.4884 - val_accuracy: 0.8401\n",
      "Epoch 52/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.4341 - accuracy: 0.8494 - val_loss: 0.5237 - val_accuracy: 0.8317\n",
      "Epoch 53/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.4318 - accuracy: 0.8482 - val_loss: 0.4583 - val_accuracy: 0.8492\n",
      "Epoch 54/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.4358 - accuracy: 0.8492 - val_loss: 0.5299 - val_accuracy: 0.8276\n",
      "Epoch 55/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.4321 - accuracy: 0.8497 - val_loss: 0.4511 - val_accuracy: 0.8480\n",
      "Epoch 56/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.4313 - accuracy: 0.8501 - val_loss: 0.4404 - val_accuracy: 0.8539\n",
      "Epoch 57/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.4280 - accuracy: 0.8505 - val_loss: 0.4187 - val_accuracy: 0.8590\n",
      "Epoch 58/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.4244 - accuracy: 0.8512 - val_loss: 0.5117 - val_accuracy: 0.8372\n",
      "Epoch 59/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.4222 - accuracy: 0.8517 - val_loss: 0.4751 - val_accuracy: 0.8464\n",
      "Epoch 60/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.4171 - accuracy: 0.8542 - val_loss: 0.5008 - val_accuracy: 0.8368\n",
      "Epoch 61/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.4207 - accuracy: 0.8537 - val_loss: 0.5241 - val_accuracy: 0.8309\n",
      "Epoch 62/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.4167 - accuracy: 0.8541 - val_loss: 0.4892 - val_accuracy: 0.8421\n",
      "Epoch 63/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.4160 - accuracy: 0.8540 - val_loss: 0.4189 - val_accuracy: 0.8587\n",
      "Epoch 64/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.4077 - accuracy: 0.8571 - val_loss: 0.5110 - val_accuracy: 0.8369\n",
      "Epoch 65/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.4068 - accuracy: 0.8583 - val_loss: 0.4838 - val_accuracy: 0.8473\n",
      "Epoch 66/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.4072 - accuracy: 0.8572 - val_loss: 0.4745 - val_accuracy: 0.8432\n",
      "Epoch 67/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.4060 - accuracy: 0.8578 - val_loss: 0.4142 - val_accuracy: 0.8622\n",
      "Epoch 68/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.4040 - accuracy: 0.8583 - val_loss: 0.4454 - val_accuracy: 0.8520\n",
      "Epoch 69/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.4067 - accuracy: 0.8573 - val_loss: 0.4521 - val_accuracy: 0.8504\n",
      "Epoch 70/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.3997 - accuracy: 0.8590 - val_loss: 0.4746 - val_accuracy: 0.8516\n",
      "Epoch 71/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.4011 - accuracy: 0.8599 - val_loss: 0.4623 - val_accuracy: 0.8484\n",
      "Epoch 72/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.3998 - accuracy: 0.8596 - val_loss: 0.5244 - val_accuracy: 0.8353\n",
      "Epoch 73/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.4008 - accuracy: 0.8591 - val_loss: 0.4029 - val_accuracy: 0.8659\n",
      "Epoch 74/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.3970 - accuracy: 0.8612 - val_loss: 0.4698 - val_accuracy: 0.8453\n",
      "Epoch 75/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.3942 - accuracy: 0.8627 - val_loss: 0.4958 - val_accuracy: 0.8405\n",
      "Epoch 76/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.3908 - accuracy: 0.8649 - val_loss: 0.4194 - val_accuracy: 0.8601\n",
      "Epoch 77/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.3918 - accuracy: 0.8622 - val_loss: 0.4598 - val_accuracy: 0.8497\n",
      "Epoch 78/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.3904 - accuracy: 0.8617 - val_loss: 0.4797 - val_accuracy: 0.8483\n",
      "Epoch 79/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.3847 - accuracy: 0.8650 - val_loss: 0.4421 - val_accuracy: 0.8583\n",
      "Epoch 80/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.3875 - accuracy: 0.8637 - val_loss: 0.4172 - val_accuracy: 0.8601\n",
      "Epoch 81/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.3890 - accuracy: 0.8636 - val_loss: 0.5106 - val_accuracy: 0.8357\n",
      "Epoch 82/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.3856 - accuracy: 0.8668 - val_loss: 0.4599 - val_accuracy: 0.8520\n",
      "Epoch 83/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.3799 - accuracy: 0.8680 - val_loss: 0.4135 - val_accuracy: 0.8615\n",
      "Epoch 84/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.3801 - accuracy: 0.8650 - val_loss: 0.4401 - val_accuracy: 0.8547\n",
      "Epoch 85/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.3781 - accuracy: 0.8685 - val_loss: 0.4244 - val_accuracy: 0.8595\n",
      "Epoch 86/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.3797 - accuracy: 0.8649 - val_loss: 0.4328 - val_accuracy: 0.8579\n",
      "Epoch 87/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.3782 - accuracy: 0.8660 - val_loss: 0.4177 - val_accuracy: 0.8633\n",
      "Epoch 88/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.3846 - accuracy: 0.8669 - val_loss: 0.4639 - val_accuracy: 0.8489\n",
      "Epoch 89/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.3742 - accuracy: 0.8663 - val_loss: 0.4304 - val_accuracy: 0.8605\n",
      "Epoch 90/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.3760 - accuracy: 0.8675 - val_loss: 0.4687 - val_accuracy: 0.8535\n",
      "Epoch 91/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.3733 - accuracy: 0.8690 - val_loss: 0.4529 - val_accuracy: 0.8519\n",
      "Epoch 92/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.3773 - accuracy: 0.8675 - val_loss: 0.4709 - val_accuracy: 0.8483\n",
      "Epoch 93/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.3789 - accuracy: 0.8665 - val_loss: 0.4677 - val_accuracy: 0.8439\n",
      "Epoch 94/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.3690 - accuracy: 0.8707 - val_loss: 0.4255 - val_accuracy: 0.8594\n",
      "Epoch 95/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.3704 - accuracy: 0.8709 - val_loss: 0.5286 - val_accuracy: 0.8347\n",
      "Epoch 96/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.3644 - accuracy: 0.8741 - val_loss: 0.5622 - val_accuracy: 0.8282\n",
      "Epoch 97/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.3683 - accuracy: 0.8723 - val_loss: 0.4393 - val_accuracy: 0.8567\n",
      "Epoch 98/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.3617 - accuracy: 0.8715 - val_loss: 0.4634 - val_accuracy: 0.8530\n",
      "Epoch 99/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.3702 - accuracy: 0.8706 - val_loss: 0.5050 - val_accuracy: 0.8386\n",
      "Epoch 100/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.3649 - accuracy: 0.8727 - val_loss: 0.3928 - val_accuracy: 0.8714\n",
      "INFO:tensorflow:Assets written to: models2\\model_5.model\\assets\n"
     ]
    }
   ],
   "source": [
    "numberOfModels = 2\n",
    "\n",
    "opt = Adam()\n",
    "\n",
    "for i in range(numberOfModels):\n",
    "    print('Inception', i + 1, 'is being trained...')\n",
    "    \n",
    "    # compile the model\n",
    "    model = MiniGoogLeNet.build(width = 32, height = 32, depth = 3, classes = 10)\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics=['accuracy'])\n",
    "    \n",
    "    # train the model\n",
    "    H = model.fit(aug.flow(trainX, trainY, batch_size = 64), validation_data = (testX, testY), epochs = 50, verbose = 1)\n",
    "    \n",
    "    # save the model\n",
    "    p = ['models2', 'model_{}.model'.format(i)]\n",
    "    model.save(os.path.sep.join(p))\n",
    "    \n",
    "    # evaluate the network\n",
    "    predictions = model.predict(testX, batch_size=64)\n",
    "    report = classification_report(testY.argmax(axis=1), predictions.argmax(axis=1), target_names=labelNames)\n",
    "    \n",
    "    # save the classification report to file\n",
    "    p = ['output2', 'model_{}.txt'.format(i)]\n",
    "    f = open(os.path.sep.join(p), 'w')\n",
    "    f.write(report)\n",
    "    f.close()\n",
    "    \n",
    "    # plot the training loss and accuracy\n",
    "    p = ['output2', 'model_{}.png'.format(i)]\n",
    "    plt.style.use('ggplot')\n",
    "    plt.figure()\n",
    "    plt.plot(np.arange(0, 50), H.history['loss'], label = 'train_loss')\n",
    "    plt.plot(np.arange(0, 50), H.history['val_loss'], label = 'val_loss')\n",
    "    plt.plot(np.arange(0, 50), H.history['accuracy'], label = 'train_acc')\n",
    "    plt.plot(np.arange(0, 50), H.history['val_accuracy'], label = 'val_acc')\n",
    "    \n",
    "    # add labels and legend\n",
    "    plt.title('Training Loss and Accuracy for model {}'.format(i))\n",
    "    plt.xlabel('Epoch #')\n",
    "    plt.ylabel('Loss/Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    # save graphs\n",
    "    plt.savefig(os.path.sep.join(p))\n",
    "    plt.close()\n",
    "    \n",
    "\n",
    "for i in range(numberOfModels, 2*numberOfModels):\n",
    "    print('ResNet', i - numberOfModels + 1, 'is being trained...')\n",
    "    \n",
    "    # compile the model\n",
    "    model = ResNet.build(32, 32, 3, 10, (9, 9, 9), (64, 64, 128, 256), reg=0.0005)\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics=['accuracy'])\n",
    "    \n",
    "    # train the model\n",
    "    H = model.fit(aug.flow(trainX, trainY, batch_size = 64), validation_data = (testX, testY), epochs = 100, verbose = 1)\n",
    "    \n",
    "    # save the model\n",
    "    p = ['models2', 'model_{}.model'.format(i)]\n",
    "    model.save(os.path.sep.join(p))\n",
    "    \n",
    "    # evaluate the network\n",
    "    predictions = model.predict(testX, batch_size=64)\n",
    "    report = classification_report(testY.argmax(axis=1), predictions.argmax(axis=1), target_names=labelNames)\n",
    "    \n",
    "    # save the classification report to file\n",
    "    p = ['output2', 'model_{}.txt'.format(i)]\n",
    "    f = open(os.path.sep.join(p), 'w')\n",
    "    f.write(report)\n",
    "    f.close()\n",
    "    \n",
    "    # plot the training loss and accuracy\n",
    "    p = ['output2', 'model_{}.png'.format(i)]\n",
    "    plt.style.use('ggplot')\n",
    "    plt.figure()\n",
    "    plt.plot(np.arange(0, epochs), H.history['loss'], label = 'train_loss')\n",
    "    plt.plot(np.arange(0, epochs), H.history['val_loss'], label = 'val_loss')\n",
    "    plt.plot(np.arange(0, epochs), H.history['accuracy'], label = 'train_acc')\n",
    "    plt.plot(np.arange(0, epochs), H.history['val_accuracy'], label = 'val_acc')\n",
    "    \n",
    "    # add labels and legend\n",
    "    plt.title('Training Loss and Accuracy for model {}'.format(i))\n",
    "    plt.xlabel('Epoch #')\n",
    "    plt.ylabel('Loss/Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    # save graphs\n",
    "    plt.savefig(os.path.sep.join(p))\n",
    "    plt.close()\n",
    "    \n",
    "for i in range(2*numberOfModels, 3*numberOfModels):\n",
    "    print('VGGNet', i - 2 * numberOfModels + 1, 'is being trained...')\n",
    "    \n",
    "    # compile the model\n",
    "    model = MiniVGGNet.build(32, 32, 3, 10)\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics=['accuracy'])\n",
    "    \n",
    "    # train the model\n",
    "    H = model.fit(aug.flow(trainX, trainY, batch_size = 64), validation_data = (testX, testY), epochs = 100, verbose = 1)\n",
    "    \n",
    "    # save the model\n",
    "    p = ['models2', 'model_{}.model'.format(i)]\n",
    "    model.save(os.path.sep.join(p))\n",
    "    \n",
    "    # evaluate the network\n",
    "    predictions = model.predict(testX, batch_size=64)\n",
    "    report = classification_report(testY.argmax(axis=1), predictions.argmax(axis=1), target_names=labelNames)\n",
    "    \n",
    "    # save the classification report to file\n",
    "    p = ['output2', 'model_{}.txt'.format(i)]\n",
    "    f = open(os.path.sep.join(p), 'w')\n",
    "    f.write(report)\n",
    "    f.close()\n",
    "    \n",
    "    # plot the training loss and accuracy\n",
    "    p = ['output2', 'model_{}.png'.format(i)]\n",
    "    plt.style.use('ggplot')\n",
    "    plt.figure()\n",
    "    plt.plot(np.arange(0, epochs), H.history['loss'], label = 'train_loss')\n",
    "    plt.plot(np.arange(0, epochs), H.history['val_loss'], label = 'val_loss')\n",
    "    plt.plot(np.arange(0, epochs), H.history['accuracy'], label = 'train_acc')\n",
    "    plt.plot(np.arange(0, epochs), H.history['val_accuracy'], label = 'val_acc')\n",
    "    \n",
    "    # add labels and legend\n",
    "    plt.title('Training Loss and Accuracy for model {}'.format(i))\n",
    "    plt.xlabel('Epoch #')\n",
    "    plt.ylabel('Loss/Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    # save graphs\n",
    "    plt.savefig(os.path.sep.join(p))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model 1/6\n",
      "Loading model 2/6\n",
      "Loading model 3/6\n",
      "Loading model 4/6\n",
      "Loading model 5/6\n",
      "Loading model 6/6\n",
      "Evaluating ensemble...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    airplane       0.94      0.94      0.94      1000\n",
      "  automobile       0.97      0.97      0.97      1000\n",
      "        bird       0.93      0.89      0.91      1000\n",
      "         cat       0.92      0.79      0.85      1000\n",
      "        deer       0.92      0.94      0.93      1000\n",
      "         dog       0.90      0.89      0.90      1000\n",
      "        frog       0.87      0.98      0.92      1000\n",
      "       horse       0.96      0.96      0.96      1000\n",
      "        ship       0.96      0.96      0.96      1000\n",
      "       truck       0.93      0.96      0.95      1000\n",
      "\n",
      "    accuracy                           0.93     10000\n",
      "   macro avg       0.93      0.93      0.93     10000\n",
      "weighted avg       0.93      0.93      0.93     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# construct the path used to collect the models then initialize the\n",
    "# models list\n",
    "modelPaths = os.path.sep.join(['models2', '*.model'])\n",
    "modelPaths = list(glob.glob(modelPaths))\n",
    "models = []\n",
    "\n",
    "# loop over the model paths, loading the model, and adding it to\n",
    "# the list of models\n",
    "for (i, modelPath) in enumerate(modelPaths):\n",
    "    print('Loading model {}/{}'.format(i + 1, len(modelPaths)))\n",
    "    models.append(load_model(modelPath))\n",
    "\n",
    "# initialize the list of predictions\n",
    "print('Evaluating ensemble...')\n",
    "predictions = []\n",
    "\n",
    "# loop over the models\n",
    "for model in models:\n",
    "    # use the current model to make predictions on the testing data,\n",
    "    # then store these predictions in the aggregate predictions list\n",
    "    predictions.append(model.predict(testX, batch_size = 64))\n",
    "\n",
    "# average the probabilities across all model predictions, then show\n",
    "# a classification report\n",
    "predictions = np.average(predictions, axis = 0)\n",
    "print(classification_report(testY.argmax(axis = 1), predictions.argmax(axis = 1), target_names = labelNames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By ensembling the Inception nets and ResNets, we improve to 93\\% accuracy on CIFAR-10, approximately the same as an ensemble of Inception nets. While mixing model types did not perform better here, it does frequently lead to better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Snapshot Ensembles\n",
    "\n",
    "Snapshot ensembles (Huang, et. al., 2017) train a single net in such a way that it repeatedly converges to different local minima, save each state of the model, and use (a subset of) those saved states to build an ensemble. While there may be more risk of this one net landing in similar local minima when training one net compared to training several unrelated nets, it can be dramatically cheaper computationally to ensemble in this way.\n",
    "\n",
    "One way of accomplishing this is to use a cyclic learning rate that starts high and anneals to smaller values until the net settles, saves the state of the model, and then returns to a high learning rate to repeat the cycle. This way, it settles down to a state we will use in the ensemble, but then, a high learning rate will let it jump out of the local minimum it has hopefully reached and continue to a new one. This way, our learning algorithms explore more of the parameter space and capture multiple local minima.\n",
    "\n",
    "The immediate question may be, \"Why not just train a new model from a random initialization each time?\" But, what tends to happen is that most training time occurs when a net is trying to reach its first local minimum. Routing from there to a new one has been shown to be much cheaper under the right circumstances.\n",
    "\n",
    "To accomplish this, we simply need to write a learning rate scheduler that applies a cyclical learning rate of the form (Loschilov and Hutter, 2016):\n",
    "\n",
    "$$\\alpha(t) = f\\left(\\text{mod}\\left(t - 1, \\left\\lceil \\frac{T}{M}\\right\\rceil\\right)\\right),$$\n",
    "\n",
    "where $t$ is the iteration number, $T$ is the total number of training epochs, $M$ is the number of snapshots we will capture, and $f$ is a monotonically decreasing function. So here, we specify how many epochs to train and the number of snapshots we want to take, and then the learning rate cycles every $\\frac{\\text{number of training epochs}}{\\text{number of snapshots}}$ number of epochs.\n",
    "\n",
    "Loschilov and Hutter proposed a shifted cosine function of the form\n",
    "\n",
    "$$\\alpha(t)=\\frac{\\alpha_0}{2}\\left(\\cos\\left(\\frac{\\pi\\text{ mod}(t-1,\\lceil T/M\\rceil)}{\\lceil T/M \\rceil}\\right)+1\\right),$$\n",
    "\n",
    "where $\\alpha_0$ is the initial learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cifar10 data\n",
    "((trainX, trainY), (testX, testY)) = cifar10.load_data()\n",
    "labelNames = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# preprocess data\n",
    "trainX = trainX.astype('float')/255.0\n",
    "testX = testX.astype('float')/255.0\n",
    "trainY = to_categorical(trainY, 10)\n",
    "testY = to_categorical(testY, 10)\n",
    "\n",
    "# create an image generator for data augmentation with random shifting, rotation, and horizontal flips\n",
    "aug = ImageDataGenerator(rotation_range = 10,\n",
    "                         width_shift_range = 0.1,\n",
    "                         height_shift_range = 0.1,\n",
    "                         horizontal_flip = True,\n",
    "                         fill_mode = 'nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "782/782 [==============================] - 17s 21ms/step - loss: 2.0874 - accuracy: 0.2317 - val_loss: 1.7338 - val_accuracy: 0.3812\n",
      "Epoch 2/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 1.4446 - accuracy: 0.4746 - val_loss: 2.0240 - val_accuracy: 0.4296\n",
      "Epoch 3/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.9765 - accuracy: 0.6546 - val_loss: 1.0558 - val_accuracy: 0.6300\n",
      "Epoch 4/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.7932 - accuracy: 0.7249 - val_loss: 0.8375 - val_accuracy: 0.7094\n",
      "Epoch 5/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.6865 - accuracy: 0.7622 - val_loss: 0.9051 - val_accuracy: 0.7175\n",
      "Epoch 6/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.6101 - accuracy: 0.7901 - val_loss: 0.7758 - val_accuracy: 0.7413\n",
      "Epoch 7/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.5519 - accuracy: 0.8108 - val_loss: 0.6379 - val_accuracy: 0.7846\n",
      "Epoch 8/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.5027 - accuracy: 0.8284 - val_loss: 0.6116 - val_accuracy: 0.8010\n",
      "Epoch 9/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.4571 - accuracy: 0.8426 - val_loss: 0.6383 - val_accuracy: 0.7993\n",
      "Epoch 10/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.4223 - accuracy: 0.8569 - val_loss: 0.6544 - val_accuracy: 0.7857\n",
      "Epoch 11/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.3907 - accuracy: 0.8669 - val_loss: 0.8264 - val_accuracy: 0.7631\n",
      "Epoch 12/100\n",
      "782/782 [==============================] - 17s 21ms/step - loss: 0.3636 - accuracy: 0.8765 - val_loss: 0.5078 - val_accuracy: 0.8315\n",
      "Epoch 13/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.3420 - accuracy: 0.8840 - val_loss: 0.5068 - val_accuracy: 0.8338\n",
      "Epoch 14/100\n",
      "782/782 [==============================] - 17s 21ms/step - loss: 0.3094 - accuracy: 0.8951 - val_loss: 0.5262 - val_accuracy: 0.8283\n",
      "Epoch 15/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.2840 - accuracy: 0.9018 - val_loss: 0.4930 - val_accuracy: 0.8467\n",
      "Epoch 16/100\n",
      "782/782 [==============================] - 17s 21ms/step - loss: 0.2680 - accuracy: 0.9079 - val_loss: 0.3862 - val_accuracy: 0.8728\n",
      "Epoch 17/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.2471 - accuracy: 0.9154 - val_loss: 0.3867 - val_accuracy: 0.8754\n",
      "Epoch 18/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.2378 - accuracy: 0.9198 - val_loss: 0.3335 - val_accuracy: 0.8902\n",
      "Epoch 19/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.2217 - accuracy: 0.9239 - val_loss: 0.3255 - val_accuracy: 0.8939\n",
      "Epoch 20/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.2160 - accuracy: 0.9256\n",
      "Epoch 00020: saving model to snapshots\\model_20.model\n",
      "INFO:tensorflow:Assets written to: snapshots\\model_20.model\\assets\n",
      "782/782 [==============================] - 20s 26ms/step - loss: 0.2160 - accuracy: 0.9256 - val_loss: 0.3311 - val_accuracy: 0.8928\n",
      "Epoch 21/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.2099 - accuracy: 0.9288 - val_loss: 0.3274 - val_accuracy: 0.8938\n",
      "Epoch 22/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.4018 - accuracy: 0.8623 - val_loss: 0.6371 - val_accuracy: 0.7958\n",
      "Epoch 23/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.3614 - accuracy: 0.8758 - val_loss: 0.6738 - val_accuracy: 0.7932\n",
      "Epoch 24/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.3423 - accuracy: 0.8833 - val_loss: 0.8371 - val_accuracy: 0.7501\n",
      "Epoch 25/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.3199 - accuracy: 0.8894 - val_loss: 0.5368 - val_accuracy: 0.8239\n",
      "Epoch 26/100\n",
      "782/782 [==============================] - 17s 21ms/step - loss: 0.3001 - accuracy: 0.8955 - val_loss: 0.5120 - val_accuracy: 0.8357\n",
      "Epoch 27/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.2768 - accuracy: 0.9045 - val_loss: 0.4504 - val_accuracy: 0.8561\n",
      "Epoch 28/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.2606 - accuracy: 0.9117 - val_loss: 0.5677 - val_accuracy: 0.8206\n",
      "Epoch 29/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.2399 - accuracy: 0.9169 - val_loss: 0.4864 - val_accuracy: 0.8435\n",
      "Epoch 30/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.2273 - accuracy: 0.9208 - val_loss: 0.4306 - val_accuracy: 0.8624\n",
      "Epoch 31/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.2055 - accuracy: 0.9284 - val_loss: 0.4418 - val_accuracy: 0.8682\n",
      "Epoch 32/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.1894 - accuracy: 0.9351 - val_loss: 0.3781 - val_accuracy: 0.8805\n",
      "Epoch 33/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.1715 - accuracy: 0.9409 - val_loss: 0.4033 - val_accuracy: 0.8747\n",
      "Epoch 34/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.1572 - accuracy: 0.9452 - val_loss: 0.3336 - val_accuracy: 0.8960\n",
      "Epoch 35/100\n",
      "782/782 [==============================] - 17s 21ms/step - loss: 0.1445 - accuracy: 0.9497 - val_loss: 0.3498 - val_accuracy: 0.8932\n",
      "Epoch 36/100\n",
      "782/782 [==============================] - 17s 21ms/step - loss: 0.1327 - accuracy: 0.9539 - val_loss: 0.3579 - val_accuracy: 0.8926\n",
      "Epoch 37/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.1250 - accuracy: 0.9578 - val_loss: 0.3075 - val_accuracy: 0.9089\n",
      "Epoch 38/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.1126 - accuracy: 0.9614 - val_loss: 0.3155 - val_accuracy: 0.9043\n",
      "Epoch 39/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.1074 - accuracy: 0.9630 - val_loss: 0.3113 - val_accuracy: 0.9072\n",
      "Epoch 40/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.1058 - accuracy: 0.9640\n",
      "Epoch 00040: saving model to snapshots\\model_40.model\n",
      "INFO:tensorflow:Assets written to: snapshots\\model_40.model\\assets\n",
      "782/782 [==============================] - 20s 26ms/step - loss: 0.1058 - accuracy: 0.9640 - val_loss: 0.3084 - val_accuracy: 0.9084\n",
      "Epoch 41/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.1027 - accuracy: 0.9656 - val_loss: 0.3042 - val_accuracy: 0.9090\n",
      "Epoch 42/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.2476 - accuracy: 0.9147 - val_loss: 0.9096 - val_accuracy: 0.7542\n",
      "Epoch 43/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.2319 - accuracy: 0.9200 - val_loss: 0.4676 - val_accuracy: 0.8578\n",
      "Epoch 44/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.2168 - accuracy: 0.9241 - val_loss: 0.3857 - val_accuracy: 0.8761\n",
      "Epoch 45/100\n",
      "782/782 [==============================] - 17s 21ms/step - loss: 0.2077 - accuracy: 0.9282 - val_loss: 0.4428 - val_accuracy: 0.8614\n",
      "Epoch 46/100\n",
      "782/782 [==============================] - 17s 21ms/step - loss: 0.1922 - accuracy: 0.9327 - val_loss: 0.5351 - val_accuracy: 0.8446\n",
      "Epoch 47/100\n",
      "782/782 [==============================] - 17s 21ms/step - loss: 0.1773 - accuracy: 0.9387 - val_loss: 0.4806 - val_accuracy: 0.8601\n",
      "Epoch 48/100\n",
      "782/782 [==============================] - 17s 21ms/step - loss: 0.1650 - accuracy: 0.9423 - val_loss: 0.4399 - val_accuracy: 0.8738\n",
      "Epoch 49/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.1538 - accuracy: 0.9473 - val_loss: 0.4960 - val_accuracy: 0.8609\n",
      "Epoch 50/100\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.1419 - accuracy: 0.9502 - val_loss: 0.4302 - val_accuracy: 0.8797\n",
      "Epoch 51/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.1260 - accuracy: 0.9557 - val_loss: 0.3780 - val_accuracy: 0.8879\n",
      "Epoch 52/100\n",
      "782/782 [==============================] - 17s 21ms/step - loss: 0.1160 - accuracy: 0.9600 - val_loss: 0.3626 - val_accuracy: 0.8977\n",
      "Epoch 53/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.1028 - accuracy: 0.9640 - val_loss: 0.3603 - val_accuracy: 0.8970\n",
      "Epoch 54/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.0964 - accuracy: 0.9674 - val_loss: 0.3749 - val_accuracy: 0.8961\n",
      "Epoch 55/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.0845 - accuracy: 0.9707 - val_loss: 0.3437 - val_accuracy: 0.9049\n",
      "Epoch 56/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.0783 - accuracy: 0.9738 - val_loss: 0.3129 - val_accuracy: 0.9110\n",
      "Epoch 57/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.0703 - accuracy: 0.9766 - val_loss: 0.3311 - val_accuracy: 0.9069\n",
      "Epoch 58/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.0641 - accuracy: 0.9791 - val_loss: 0.3295 - val_accuracy: 0.9084\n",
      "Epoch 59/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.0605 - accuracy: 0.9803 - val_loss: 0.3191 - val_accuracy: 0.9107\n",
      "Epoch 60/100\n",
      "780/782 [============================>.] - ETA: 0s - loss: 0.0603 - accuracy: 0.9794\n",
      "Epoch 00060: saving model to snapshots\\model_60.model\n",
      "INFO:tensorflow:Assets written to: snapshots\\model_60.model\\assets\n",
      "782/782 [==============================] - 23s 29ms/step - loss: 0.0602 - accuracy: 0.9794 - val_loss: 0.3176 - val_accuracy: 0.9113\n",
      "Epoch 61/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.0596 - accuracy: 0.9799 - val_loss: 0.3198 - val_accuracy: 0.9117\n",
      "Epoch 62/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.1780 - accuracy: 0.9384 - val_loss: 1.2210 - val_accuracy: 0.7830\n",
      "Epoch 63/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.1696 - accuracy: 0.9412 - val_loss: 0.6188 - val_accuracy: 0.8423\n",
      "Epoch 64/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.1564 - accuracy: 0.9453 - val_loss: 0.5257 - val_accuracy: 0.8573\n",
      "Epoch 65/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.1448 - accuracy: 0.9488 - val_loss: 0.3983 - val_accuracy: 0.8841\n",
      "Epoch 66/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.1403 - accuracy: 0.9509 - val_loss: 0.4470 - val_accuracy: 0.8775\n",
      "Epoch 67/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.1271 - accuracy: 0.9568 - val_loss: 0.3593 - val_accuracy: 0.8985\n",
      "Epoch 68/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.1197 - accuracy: 0.9583 - val_loss: 0.4601 - val_accuracy: 0.8760\n",
      "Epoch 69/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.1067 - accuracy: 0.9632 - val_loss: 0.3719 - val_accuracy: 0.8980\n",
      "Epoch 70/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.0930 - accuracy: 0.9672 - val_loss: 0.3770 - val_accuracy: 0.8991\n",
      "Epoch 71/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.0887 - accuracy: 0.9698 - val_loss: 0.3313 - val_accuracy: 0.9018\n",
      "Epoch 72/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.0783 - accuracy: 0.9732 - val_loss: 0.3601 - val_accuracy: 0.9043\n",
      "Epoch 73/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.0673 - accuracy: 0.9777 - val_loss: 0.3722 - val_accuracy: 0.9051\n",
      "Epoch 74/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.0612 - accuracy: 0.9798 - val_loss: 0.3734 - val_accuracy: 0.9054\n",
      "Epoch 75/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.0545 - accuracy: 0.9824 - val_loss: 0.3388 - val_accuracy: 0.9112\n",
      "Epoch 76/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.0497 - accuracy: 0.9835 - val_loss: 0.3394 - val_accuracy: 0.9129\n",
      "Epoch 77/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.0455 - accuracy: 0.9847 - val_loss: 0.3204 - val_accuracy: 0.9143\n",
      "Epoch 78/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.0396 - accuracy: 0.9869 - val_loss: 0.3279 - val_accuracy: 0.9155\n",
      "Epoch 79/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.0398 - accuracy: 0.9875 - val_loss: 0.3258 - val_accuracy: 0.9141\n",
      "Epoch 80/100\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.0392 - accuracy: 0.9872\n",
      "Epoch 00080: saving model to snapshots\\model_80.model\n",
      "INFO:tensorflow:Assets written to: snapshots\\model_80.model\\assets\n",
      "782/782 [==============================] - 20s 26ms/step - loss: 0.0392 - accuracy: 0.9872 - val_loss: 0.3297 - val_accuracy: 0.9152\n",
      "Epoch 81/100\n",
      "782/782 [==============================] - 17s 21ms/step - loss: 0.0382 - accuracy: 0.9885 - val_loss: 0.3278 - val_accuracy: 0.9155\n",
      "Epoch 82/100\n",
      "782/782 [==============================] - 17s 21ms/step - loss: 0.1231 - accuracy: 0.9570 - val_loss: 0.5931 - val_accuracy: 0.8579\n",
      "Epoch 83/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.1312 - accuracy: 0.9539 - val_loss: 0.5028 - val_accuracy: 0.8726\n",
      "Epoch 84/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.1157 - accuracy: 0.9590 - val_loss: 0.6352 - val_accuracy: 0.8538\n",
      "Epoch 85/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.1123 - accuracy: 0.9608 - val_loss: 0.5107 - val_accuracy: 0.8712\n",
      "Epoch 86/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.1039 - accuracy: 0.9635 - val_loss: 0.6940 - val_accuracy: 0.8488\n",
      "Epoch 87/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.0978 - accuracy: 0.9659 - val_loss: 0.4755 - val_accuracy: 0.8826\n",
      "Epoch 88/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.0834 - accuracy: 0.9710 - val_loss: 0.3941 - val_accuracy: 0.8996\n",
      "Epoch 89/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.0778 - accuracy: 0.9743 - val_loss: 0.4039 - val_accuracy: 0.8989\n",
      "Epoch 90/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.0714 - accuracy: 0.9752 - val_loss: 0.3961 - val_accuracy: 0.9018\n",
      "Epoch 91/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.0668 - accuracy: 0.9773 - val_loss: 0.4029 - val_accuracy: 0.8956\n",
      "Epoch 92/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.0550 - accuracy: 0.9816 - val_loss: 0.3471 - val_accuracy: 0.9105\n",
      "Epoch 93/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.0491 - accuracy: 0.9833 - val_loss: 0.3948 - val_accuracy: 0.9080\n",
      "Epoch 94/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.0434 - accuracy: 0.9856 - val_loss: 0.3658 - val_accuracy: 0.9127\n",
      "Epoch 95/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.0378 - accuracy: 0.9873 - val_loss: 0.3476 - val_accuracy: 0.9129\n",
      "Epoch 96/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.0354 - accuracy: 0.9889 - val_loss: 0.3422 - val_accuracy: 0.9165\n",
      "Epoch 97/100\n",
      "782/782 [==============================] - 17s 21ms/step - loss: 0.0316 - accuracy: 0.9900 - val_loss: 0.3469 - val_accuracy: 0.9166\n",
      "Epoch 98/100\n",
      "782/782 [==============================] - 17s 21ms/step - loss: 0.0293 - accuracy: 0.9908 - val_loss: 0.3472 - val_accuracy: 0.9167\n",
      "Epoch 99/100\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.0293 - accuracy: 0.9911 - val_loss: 0.3399 - val_accuracy: 0.9184\n",
      "Epoch 100/100\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.0269 - accuracy: 0.9916 ETA: 0s - loss: 0.0269 - accuracy: 0.99\n",
      "Epoch 00100: saving model to snapshots\\model_100.model\n",
      "INFO:tensorflow:Assets written to: snapshots\\model_100.model\\assets\n",
      "782/782 [==============================] - 20s 26ms/step - loss: 0.0269 - accuracy: 0.9916 - val_loss: 0.3371 - val_accuracy: 0.9197\n"
     ]
    }
   ],
   "source": [
    "models = 5\n",
    "initialLearningRate = 0.2\n",
    "epochs = 100\n",
    "batchSize = 64\n",
    "\n",
    "# code for a learning rate scheduler\n",
    "def shiftedCosine(epoch):\n",
    "    maxEpochs = epochs\n",
    "    baseLearningRate = initialLearningRate\n",
    "\n",
    "    alpha = (initialLearningRate/2) * (np.cos(np.pi * np.mod(epoch - 1, np.ceil(epochs/models)) / np.ceil(epochs/models)) + 1)\n",
    "    \n",
    "    # return the learning rate\n",
    "    return alpha\n",
    "    \n",
    "# choose the optimizer\n",
    "opt = SGD(learning_rate = initialLearningRate)\n",
    "    \n",
    "# compile the model\n",
    "model = MiniGoogLeNet.build(width = 32, height = 32, depth = 3, classes = 10)\n",
    "\n",
    "batchesPerEpoch = np.ceil(len(trainX) / batchSize)\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics = ['accuracy'])\n",
    "\n",
    "# train the model\n",
    "checkpoint = ModelCheckpoint('snapshots/model_{epoch:02d}.model',\n",
    "                             save_weights_only = False,\n",
    "                             save_best_only = False,\n",
    "                             save_freq = int(batchesPerEpoch * epochs / models),\n",
    "                             verbose = 1)\n",
    "\n",
    "callbacks = [LearningRateScheduler(shiftedCosine), checkpoint]\n",
    "\n",
    "H = model.fit(aug.flow(trainX, trainY, batch_size = batchSize),\n",
    "              validation_data = (testX, testY),\n",
    "              epochs = epochs,\n",
    "              callbacks = callbacks,\n",
    "              verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we test the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model 1/5\n",
      "Loading model 2/5\n",
      "Loading model 3/5\n",
      "Loading model 4/5\n",
      "Loading model 5/5\n",
      "Evaluating ensemble...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    airplane       0.92      0.93      0.93      1000\n",
      "  automobile       0.95      0.97      0.96      1000\n",
      "        bird       0.89      0.89      0.89      1000\n",
      "         cat       0.87      0.81      0.84      1000\n",
      "        deer       0.90      0.93      0.91      1000\n",
      "         dog       0.91      0.83      0.87      1000\n",
      "        frog       0.88      0.97      0.92      1000\n",
      "       horse       0.94      0.94      0.94      1000\n",
      "        ship       0.96      0.95      0.95      1000\n",
      "       truck       0.94      0.95      0.95      1000\n",
      "\n",
      "    accuracy                           0.92     10000\n",
      "   macro avg       0.92      0.92      0.92     10000\n",
      "weighted avg       0.92      0.92      0.92     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# construct the path used to collect the models then initialize the models list\n",
    "modelPaths = os.path.sep.join(['0', '*.model'])\n",
    "modelPaths = list(glob.glob(modelPaths))\n",
    "models = []\n",
    "\n",
    "# loop over the model paths, loading the model, and adding it to\n",
    "# the list of models\n",
    "for (i, modelPath) in enumerate(modelPaths):\n",
    "    print('Loading model {}/{}'.format(i + 1, len(modelPaths)))\n",
    "    models.append(load_model(modelPath))\n",
    "\n",
    "# initialize the list of predictions\n",
    "print('Evaluating ensemble...')\n",
    "predictions = []\n",
    "\n",
    "# loop over the models\n",
    "for model in models:\n",
    "    # use the current model to make predictions on the testing data,\n",
    "    # then store these predictions in the aggregate predictions list\n",
    "    predictions.append(model.predict(testX, batch_size=64))\n",
    "    \n",
    "# average the probabilities across all model predictions, then show\n",
    "# a classification report\n",
    "predictions = np.average(predictions, axis=0)\n",
    "\n",
    "print(classification_report(testY.argmax(axis=1), predictions.argmax(axis=1),\n",
    "                            target_names=labelNames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using the snapshot ensemble, we get better performance out of the Inception net, even though we only trained for 100 epochs. The performance falls slightly short of the full ensemble from the previous example (93\\% accuracy), but we only trained 1/5 as much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Lecture 17 - Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning\n",
    "\n",
    "Transfer learning is where we take nets pre-trained on huge datasets like ImageNet, load the weights, and use those as starting points for training on a new dataset. The idea is that knowledge learning about one dataset, if it is somewhat related to your dataset, can *transfer* to knowledge of your dataset with only partial training.\n",
    "\n",
    "For example, a neural net that is effective at classifying words spoken in English might be effective for learning words spoken in Spanish without totally starting from scratch with training and hyperparameter tuning. After all, both languages have roughly the same alphabet and many similarities in pronunciation of letters.\n",
    "\n",
    "One approach takes a pre-trained convolutional neural net containing all the parameters successful at classifying the intended dataset, remove the fully-connected layers at the end, and feed your *different* dataset through the net to the end of the last pooling layer. Then, treat the outputs from each input as a new dataset that has been preprocessed by ths pre-trained CNN. Lastly, apply another classifier to this dataset.\n",
    "\n",
    "Another common approach is to take the same kind of pre-trained CNN and re-initialize the weights of the fully-connected layers at the end. Then, \"freeze\" all the parameters before the fully-connected layers. Last, train the parameters of the last few layers with a small learning rate on the new dataset, but with one caveat: as backpropagation moves backward through the network determining weight updates, it stops when it reaches the frozen layers and does not adjust those weights at all.\n",
    "\n",
    "Let's learn how to implement both approaches!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic packages\n",
    "from imutils import paths\n",
    "import cv2\n",
    "import h5py\n",
    "import imutils\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import progressbar\n",
    "import random\n",
    "\n",
    "# sklearn functions\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# keras functions\n",
    "from tensorflow.keras.applications import imagenet_utils\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import load_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDF5\n",
    "\n",
    "If we want to work with huge pre-trained neural nets like VGG19 or other deep CNNs, storing them takes far more space than our RAM is likely to support, so we need to store them on HDD/SDDs in an efficient way. Keras's model format is pretty large, but HDF5 is a good data format for this, but we need some code to be able to interface with this format, which we write below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HDF5DatasetWriter:\n",
    "    def __init__(self, dims, outputPath, dataKey = 'images', bufferSize = 1000):\n",
    "        # check if outputpath exists\n",
    "        if os.path.exists(outputPath):\n",
    "            raise ValueError('The supplied `outputPath` already exists and cannot be overwritten.'\n",
    "                            'Delete the file manually before continuing.', outputPath)\n",
    "            \n",
    "        # open the HDF5 database for writing and create two datasets: one to store the\n",
    "        # images/features and one to store the labels\n",
    "        self.db = h5py.File(outputPath, 'w')\n",
    "        self.data = self.db.create_dataset(dataKey, dims, dtype = 'float')\n",
    "        self.labels = self.db.create_dataset('labels', (dims[0],), dtype = 'float')\n",
    "        \n",
    "        # store the buffer size and initialize the buffer and index\n",
    "        self.bufferSize = bufferSize\n",
    "        self.buffer = {'data': [], 'labels': []}\n",
    "        self.index = 0\n",
    "        \n",
    "    def add(self, rows, labels):\n",
    "        # add the rows and labels to the buffer\n",
    "        self.buffer['data'].extend(rows)\n",
    "        self.buffer['labels'].extend(labels)\n",
    "        \n",
    "        # check if the buffer needs to be flushed to disk\n",
    "        if len(self.buffer['data']) >= self.bufferSize:\n",
    "            self.flush()\n",
    "            \n",
    "    def flush(self):\n",
    "        # write the buffer to disk and reset buffer\n",
    "        i = self.index + len(self.buffer['data'])\n",
    "        self.data[self.index:i] = self.buffer['data']\n",
    "        self.labels[self.index:i] = self.buffer['labels']\n",
    "        \n",
    "        self.index = i\n",
    "        self.buffer = {'data': [], 'labels': []}\n",
    "        \n",
    "    def storeClassLabels(self, classLabels):\n",
    "        # create a dataset to store class label names, then store them\n",
    "        dt = h5py.special_dtype(vlen = str)\n",
    "        labelSet = self.db.create_dataset('label_names', (len(classLabels),), dtype = dt)\n",
    "        labelSet[:] = classLabels\n",
    "        \n",
    "    def close(self):\n",
    "        # flush entries to disk if needed\n",
    "        if len(self.buffer['data']) > 0:\n",
    "            self.flush()\n",
    "            \n",
    "        # close the dataset\n",
    "        self.db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction\n",
    "\n",
    "Let's write some code to extract features from an arbitrary image dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractFeatures(batch_size, dataset, output, buffer_size = 1000):\n",
    "\n",
    "    bs = batch_size\n",
    "    \n",
    "    # grab the list of images that we'll be describing then randomly\n",
    "    # shuffle them to allow for easy training and testing splits via\n",
    "    # array slicing during training time\n",
    "    print(\"[INFO] loading images...\")\n",
    "    imagePaths = list(paths.list_images(dataset))\n",
    "    random.shuffle(imagePaths)\n",
    "\n",
    "    # extract the class labels from the image paths then encode the\n",
    "    # labels\n",
    "    labels = [p.split(os.path.sep)[-2] for p in imagePaths]\n",
    "    le = LabelEncoder()\n",
    "    labels = le.fit_transform(labels)\n",
    "\n",
    "    # load the VGG16 network\n",
    "    print(\"[INFO] loading network...\")\n",
    "    model = VGG16(weights=\"imagenet\", include_top=False)\n",
    "\n",
    "    # initialize the HDF5 dataset writer, then store the class label\n",
    "    # names in the dataset\n",
    "    dataset = HDF5DatasetWriter((len(imagePaths), 512 * 7 * 7),\n",
    "        output, dataKey=\"features\", bufferSize=buffer_size)\n",
    "    dataset.storeClassLabels(le.classes_)\n",
    "\n",
    "    # initialize the progress bar\n",
    "    widgets = [\"Extracting Features: \", progressbar.Percentage(), \" \",\n",
    "               progressbar.Bar(), \" \", progressbar.ETA()]\n",
    "    pbar = progressbar.ProgressBar(maxval=len(imagePaths), widgets=widgets).start()\n",
    "\n",
    "    # loop over the images in batches\n",
    "    for i in np.arange(0, len(imagePaths), bs):\n",
    "        # extract the batch of images and labels, then initialize the\n",
    "        # list of actual images that will be passed through the network\n",
    "        # for feature extraction\n",
    "        batchPaths = imagePaths[i:i + bs]\n",
    "        batchLabels = labels[i:i + bs]\n",
    "        batchImages = []\n",
    "\n",
    "        # loop over the images and labels in the current batch\n",
    "        for (j, imagePath) in enumerate(batchPaths):\n",
    "            # load the input image using the Keras helper utility\n",
    "            # while ensuring the image is resized to 224x224 pixels\n",
    "            image = load_img(imagePath, target_size=(224, 224))\n",
    "            image = img_to_array(image)\n",
    "\n",
    "            # preprocess the image by (1) expanding the dimensions and\n",
    "            # (2) subtracting the mean RGB pixel intensity from the\n",
    "            # ImageNet dataset\n",
    "            image = np.expand_dims(image, axis=0)\n",
    "            image = imagenet_utils.preprocess_input(image)\n",
    "\n",
    "            # add the image to the batch\n",
    "            batchImages.append(image)\n",
    "\n",
    "        # pass the images through the network and use the outputs as\n",
    "        # our actual features\n",
    "        batchImages = np.vstack(batchImages)\n",
    "        features = model.predict(batchImages, batch_size=bs)\n",
    "\n",
    "        # reshape the features so that each image is represented by\n",
    "        # a flattened feature vector of the `MaxPooling2D` outputs\n",
    "        features = features.reshape((features.shape[0], 512 * 7 * 7))\n",
    "\n",
    "        # add the features and labels to our HDF5 dataset\n",
    "        dataset.add(features, batchLabels)\n",
    "        pbar.update(i)\n",
    "\n",
    "    # close the dataset\n",
    "    dataset.close()\n",
    "    pbar.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Features from Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading images...\n",
      "[INFO] loading network...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100% |#####################################| Time: 0:00:48\n"
     ]
    }
   ],
   "source": [
    "extractFeatures(32, '../data/animals/images', '../data/animals/hdf5/features.hdf5', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading images...\n",
      "[INFO] loading network...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100% |#####################################| Time: 0:01:12\n"
     ]
    }
   ],
   "source": [
    "extractFeatures(32, '../data/caltech-101/images', '../data/caltech-101/hdf5/features.hdf5', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:   0% |                                    | ETA:  --:--:--\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading images...\n",
      "[INFO] loading network...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100% |#####################################| Time: 0:00:19\n"
     ]
    }
   ],
   "source": [
    "extractFeatures(32, '../data/flowers17/images', '../data/flowers17/hdf5/features.hdf5', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Classifier on Extracted Features\n",
    "\n",
    "We have used a VGG16 net pre-trained on ImageNet and used it to extract features from three *different* datasets. We will now train a simple classifier on this new dataset of features extracted by the VGG16 net and see if the learning can actually be transferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transferClassify(dbPath, modelPath, jobs = -1):\n",
    "    # open HDF5\n",
    "    db = h5py.File(dbPath, 'r')\n",
    "    \n",
    "    # 75% to training (recall, we shuffled the data before writing to HDF5) is\n",
    "    # before index i\n",
    "    i = int(db['labels'].shape[0] * 0.75)\n",
    "    \n",
    "    # define parameters we want to tune\n",
    "    parameters = {'C': [0.1, 1.0, 10.0, 100.0, 1000.0, 10000.0]}\n",
    "    \n",
    "    # evaluate the model at each value of C\n",
    "    model = GridSearchCV(LogisticRegression(solver = 'lbfgs', multi_class = 'auto'),\n",
    "                        parameters, cv = 3, n_jobs = jobs)\n",
    "    \n",
    "    # fit the model to the training set\n",
    "    model.fit(db['features'][:i], db['labels'][:i])\n",
    "    print('Best hyperparameters: {}'.format(model.best_params_))\n",
    "    \n",
    "    # evaluate the model\n",
    "    print('Evaluating...')\n",
    "    \n",
    "    # extract the label names as strings\n",
    "    names = db['label_names'][:]\n",
    "    names = [x.decode(\"utf-8\") for x in names]\n",
    "    \n",
    "    # predict on the testing set\n",
    "    predictions = model.predict(db['features'][i:])\n",
    "    print(classification_report(db['labels'][i:],\n",
    "                                predictions,\n",
    "                                target_names = names))\n",
    "    \n",
    "    # save the model\n",
    "    print('Saving model...')\n",
    "    f = open(modelPath, 'wb')\n",
    "    f.write(pickle.dumps(model.best_estimator_))\n",
    "    f.close()\n",
    "    \n",
    "    # cloes the database\n",
    "    db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'C': 10000.0}\n",
      "Evaluating...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        cats       0.98      0.99      0.98       248\n",
      "        dogs       0.99      0.98      0.98       251\n",
      "       panda       1.00      1.00      1.00       251\n",
      "\n",
      "    accuracy                           0.99       750\n",
      "   macro avg       0.99      0.99      0.99       750\n",
      "weighted avg       0.99      0.99      0.99       750\n",
      "\n",
      "Saving model...\n"
     ]
    }
   ],
   "source": [
    "transferClassify('../data/animals/hdf5/features.hdf5', 'animals.cpickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ryan\\miniconda3\\envs\\TF-2.5\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'C': 0.1}\n",
      "Evaluating...\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "          Faces       0.96      0.98      0.97       124\n",
      "     Faces_easy       0.97      0.97      0.97       108\n",
      "       Leopards       0.98      1.00      0.99        55\n",
      "     Motorbikes       1.00      1.00      1.00       203\n",
      "      accordion       1.00      1.00      1.00        11\n",
      "      airplanes       0.99      0.99      0.99       195\n",
      "         anchor       0.64      0.82      0.72        11\n",
      "            ant       0.83      0.77      0.80        13\n",
      "         barrel       1.00      0.91      0.95        11\n",
      "           bass       0.92      1.00      0.96        11\n",
      "         beaver       0.86      0.46      0.60        13\n",
      "      binocular       1.00      1.00      1.00        10\n",
      "         bonsai       0.93      1.00      0.96        39\n",
      "          brain       0.97      1.00      0.98        29\n",
      "   brontosaurus       1.00      1.00      1.00         7\n",
      "         buddha       1.00      1.00      1.00        19\n",
      "      butterfly       1.00      0.95      0.97        19\n",
      "         camera       1.00      1.00      1.00        11\n",
      "         cannon       1.00      0.73      0.84        11\n",
      "       car_side       1.00      1.00      1.00        31\n",
      "    ceiling_fan       0.93      0.93      0.93        14\n",
      "      cellphone       1.00      1.00      1.00        11\n",
      "          chair       0.80      0.89      0.84         9\n",
      "     chandelier       0.90      1.00      0.95        28\n",
      "    cougar_body       1.00      0.67      0.80        12\n",
      "    cougar_face       0.89      0.85      0.87        20\n",
      "           crab       0.92      0.63      0.75        19\n",
      "       crayfish       0.78      0.70      0.74        10\n",
      "      crocodile       0.79      0.92      0.85        12\n",
      " crocodile_head       1.00      0.94      0.97        18\n",
      "            cup       1.00      1.00      1.00        10\n",
      "      dalmatian       1.00      1.00      1.00        22\n",
      "    dollar_bill       1.00      0.92      0.96        13\n",
      "        dolphin       1.00      0.90      0.95        10\n",
      "      dragonfly       1.00      1.00      1.00        12\n",
      "electric_guitar       1.00      1.00      1.00        10\n",
      "       elephant       0.94      0.88      0.91        17\n",
      "            emu       1.00      0.91      0.95        11\n",
      "      euphonium       1.00      0.96      0.98        23\n",
      "           ewer       1.00      0.95      0.98        22\n",
      "          ferry       1.00      1.00      1.00        20\n",
      "       flamingo       1.00      0.82      0.90        17\n",
      "  flamingo_head       1.00      1.00      1.00        11\n",
      "       garfield       0.88      0.88      0.88         8\n",
      "        gerenuk       0.86      1.00      0.92         6\n",
      "     gramophone       0.90      1.00      0.95         9\n",
      "    grand_piano       1.00      1.00      1.00        30\n",
      "      hawksbill       0.88      1.00      0.94        22\n",
      "      headphone       0.91      1.00      0.95        10\n",
      "       hedgehog       0.82      1.00      0.90        14\n",
      "     helicopter       0.89      0.89      0.89        18\n",
      "           ibis       0.92      1.00      0.96        23\n",
      "   inline_skate       1.00      0.83      0.91         6\n",
      "    joshua_tree       0.88      1.00      0.93        14\n",
      "       kangaroo       0.90      1.00      0.95        18\n",
      "          ketch       0.85      0.85      0.85        34\n",
      "           lamp       0.94      0.88      0.91        17\n",
      "         laptop       1.00      1.00      1.00        17\n",
      "          llama       0.95      0.79      0.86        24\n",
      "        lobster       0.67      0.57      0.62         7\n",
      "          lotus       0.77      0.56      0.65        18\n",
      "       mandolin       1.00      1.00      1.00        13\n",
      "         mayfly       1.00      0.90      0.95        10\n",
      "        menorah       0.97      1.00      0.98        30\n",
      "      metronome       1.00      1.00      1.00         8\n",
      "        minaret       0.92      1.00      0.96        24\n",
      "       nautilus       0.95      1.00      0.97        18\n",
      "        octopus       1.00      0.55      0.71        11\n",
      "          okapi       1.00      1.00      1.00        11\n",
      "         pagoda       0.92      1.00      0.96        11\n",
      "          panda       1.00      0.83      0.91         6\n",
      "         pigeon       0.94      1.00      0.97        17\n",
      "          pizza       0.83      1.00      0.91        10\n",
      "       platypus       0.75      1.00      0.86         6\n",
      "        pyramid       1.00      1.00      1.00        12\n",
      "       revolver       1.00      1.00      1.00        23\n",
      "          rhino       0.91      1.00      0.95        10\n",
      "        rooster       1.00      0.90      0.95        10\n",
      "      saxophone       1.00      0.92      0.96        12\n",
      "       schooner       0.60      0.60      0.60        10\n",
      "       scissors       1.00      0.90      0.95        10\n",
      "       scorpion       0.89      1.00      0.94        17\n",
      "      sea_horse       0.67      0.92      0.77        13\n",
      "         snoopy       1.00      1.00      1.00         6\n",
      "    soccer_ball       1.00      0.94      0.97        16\n",
      "        stapler       1.00      1.00      1.00        13\n",
      "       starfish       0.86      1.00      0.92        18\n",
      "    stegosaurus       0.92      0.86      0.89        14\n",
      "      stop_sign       0.91      1.00      0.95        20\n",
      "     strawberry       1.00      0.91      0.95        11\n",
      "      sunflower       0.94      1.00      0.97        15\n",
      "           tick       0.92      1.00      0.96        12\n",
      "      trilobite       1.00      1.00      1.00        23\n",
      "       umbrella       0.95      1.00      0.98        20\n",
      "          watch       1.00      0.96      0.98        51\n",
      "    water_lilly       0.50      0.78      0.61         9\n",
      "     wheelchair       1.00      1.00      1.00        10\n",
      "       wild_cat       1.00      1.00      1.00         9\n",
      "  windsor_chair       1.00      1.00      1.00        16\n",
      "         wrench       0.93      0.87      0.90        15\n",
      "       yin_yang       1.00      0.96      0.98        23\n",
      "\n",
      "       accuracy                           0.95      2170\n",
      "      macro avg       0.93      0.93      0.93      2170\n",
      "   weighted avg       0.95      0.95      0.95      2170\n",
      "\n",
      "Saving model...\n"
     ]
    }
   ],
   "source": [
    "transferClassify('../data/caltech-101/hdf5/features.hdf5', 'caltech101.cpickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'C': 100.0}\n",
      "Evaluating...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    bluebell       0.89      1.00      0.94        17\n",
      "   buttercup       0.81      0.89      0.85        19\n",
      "   coltsfoot       0.91      0.95      0.93        22\n",
      "     cowslip       0.54      0.88      0.67         8\n",
      "      crocus       0.95      0.95      0.95        20\n",
      "    daffodil       0.88      0.75      0.81        20\n",
      "       daisy       1.00      0.95      0.98        22\n",
      "   dandelion       0.95      0.88      0.91        24\n",
      "  fritillary       0.96      0.93      0.95        28\n",
      "        iris       1.00      0.94      0.97        16\n",
      "  lilyvalley       0.94      0.94      0.94        17\n",
      "       pansy       1.00      0.80      0.89        25\n",
      "    snowdrop       0.76      1.00      0.86        19\n",
      "   sunflower       1.00      1.00      1.00        25\n",
      "   tigerlily       1.00      1.00      1.00        14\n",
      "       tulip       0.92      0.81      0.86        27\n",
      "  windflower       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           0.92       340\n",
      "   macro avg       0.91      0.92      0.91       340\n",
      "weighted avg       0.93      0.92      0.92       340\n",
      "\n",
      "Saving model...\n"
     ]
    }
   ],
   "source": [
    "transferClassify('../data/flowers17/hdf5/features.hdf5', 'flowers17.cpickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning By Fine-Tuning\n",
    "\n",
    "To operate on individual layers, we need to determine how to access them with code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printLayerNames(include_top = True):\n",
    "    \n",
    "    # load VGG16 pre-trained on ImageNet\n",
    "    print('Loading network...')\n",
    "    model = VGG16(weights = 'imagenet', include_top = include_top)\n",
    "    \n",
    "    # loop over the layers and display them\n",
    "    for (i, layer) in enumerate(model.layers):\n",
    "        print('{}\\t{}'.format(i, layer.__class__.__name__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer names with the head\n",
      "Loading network...\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
      "553467904/553467096 [==============================] - 20s 0us/step\n",
      "0\tInputLayer\n",
      "1\tConv2D\n",
      "2\tConv2D\n",
      "3\tMaxPooling2D\n",
      "4\tConv2D\n",
      "5\tConv2D\n",
      "6\tMaxPooling2D\n",
      "7\tConv2D\n",
      "8\tConv2D\n",
      "9\tConv2D\n",
      "10\tMaxPooling2D\n",
      "11\tConv2D\n",
      "12\tConv2D\n",
      "13\tConv2D\n",
      "14\tMaxPooling2D\n",
      "15\tConv2D\n",
      "16\tConv2D\n",
      "17\tConv2D\n",
      "18\tMaxPooling2D\n",
      "19\tFlatten\n",
      "20\tDense\n",
      "21\tDense\n",
      "22\tDense\n",
      "\n",
      "Layer names without the head\n",
      "Loading network...\n",
      "0\tInputLayer\n",
      "1\tConv2D\n",
      "2\tConv2D\n",
      "3\tMaxPooling2D\n",
      "4\tConv2D\n",
      "5\tConv2D\n",
      "6\tMaxPooling2D\n",
      "7\tConv2D\n",
      "8\tConv2D\n",
      "9\tConv2D\n",
      "10\tMaxPooling2D\n",
      "11\tConv2D\n",
      "12\tConv2D\n",
      "13\tConv2D\n",
      "14\tMaxPooling2D\n",
      "15\tConv2D\n",
      "16\tConv2D\n",
      "17\tConv2D\n",
      "18\tMaxPooling2D\n"
     ]
    }
   ],
   "source": [
    "print('Layer names with the head')\n",
    "printLayerNames()\n",
    "\n",
    "print('\\nLayer names without the head')\n",
    "printLayerNames(include_top = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the first 18 layers are the same, but the full network, including the top of the net, i.e. the fully connected layers. This \"top\" is also known as the \"head\".\n",
    "\n",
    "### Network Surgery\n",
    "\n",
    "Once we've chopped off the head of the net, we need to replace it with a newly initialized head so we can train it and attempt to transfer the knowledge of the lower layers of the net to a new dataset by retraining this new head.\n",
    "\n",
    "Let's create a net to go at the head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCHeadNet:\n",
    "    def build(baseModel, classes, D):\n",
    "        # initialize the head model and add a fully-connected layer\n",
    "        headModel = baseModel.output\n",
    "        headModel = Flatten(name = 'flatten')(headModel)\n",
    "        headModel = Dense(D, activation = 'relu')(headModel)\n",
    "        headModel = Dropout(0.5)(headModel)\n",
    "        \n",
    "        # add a softmax layer\n",
    "        headModel = Dense(classes, activation = 'softmax')(headModel)\n",
    "        \n",
    "        # return the model\n",
    "        return headModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Preprocessing Code\n",
    "\n",
    "This code comes from Adrian Rosebrock's *Deep Learning for Computer Vision* book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset loader that applies specified preprocessors\n",
    "class SimpleDatasetLoader:\n",
    "    def __init__(self, preprocessors=None):\n",
    "        # store the image preprocessor\n",
    "        self.preprocessors = preprocessors\n",
    "\n",
    "        # if the preprocessors are None, initialize them as an empty list\n",
    "        if self.preprocessors is None:\n",
    "            self.preprocessors = []\n",
    "\n",
    "    def load(self, imagePaths, verbose=-1):\n",
    "        # initialize the list of features and labels\n",
    "        data = []\n",
    "        labels = []\n",
    "\n",
    "        # loop over the input images\n",
    "        for (i, imagePath) in enumerate(imagePaths):\n",
    "            # load the image and extract the class label assuming that our path has\n",
    "            # the following format: /path/to/dataset/{class}/{image}.jpg\n",
    "            image = cv2.imread(imagePath)\n",
    "            label = imagePath.split(os.path.sep)[-2]\n",
    "\n",
    "            # check to see if our preprocessors are not None\n",
    "            if self.preprocessors is not None:\n",
    "                # loop over the preprocessors and apply each to the image\n",
    "                for p in self.preprocessors:\n",
    "                    image = p.preprocess(image)\n",
    "\n",
    "            # treat our processed image as a \"feature vector\" by updating the data\n",
    "            # list followed by the labels\n",
    "            data.append(image)\n",
    "            labels.append(label)\n",
    "\n",
    "            # show an update every `verbose` images\n",
    "            if verbose > 0 and i > 0 and (i + 1) % verbose == 0:\n",
    "                print(\"[INFO] processed {}/{}\".format(i + 1, len(imagePaths)))\n",
    "\n",
    "        # return a tuple of the data and labels\n",
    "        return (np.array(data), np.array(labels))\n",
    "\n",
    "# resize images while maintaining aspect ratio\n",
    "class AspectAwarePreprocessor:\n",
    "    def __init__(self, width, height, inter=cv2.INTER_AREA):\n",
    "        # store the target image width, height, and interpolation method used\n",
    "        # when resizing\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.inter = inter\n",
    "\n",
    "    def preprocess(self, image):\n",
    "        # grab the dimensions of the image and then initialize the deltas to use\n",
    "        # when cropping\n",
    "        (h, w) = image.shape[:2]\n",
    "        dW = 0\n",
    "        dH = 0\n",
    "\n",
    "        # if the width is smaller than the height, then resize along the width (i.e.,\n",
    "        # the smaller dimension) and then update the deltas to crop the height to the\n",
    "        # desired dimension\n",
    "        if w < h:\n",
    "            image = imutils.resize(image, width=self.width, inter=self.inter)\n",
    "            dH = int((image.shape[0] - self.height) / 2.0)\n",
    "\n",
    "        # otherwise, the height is smaller than the width so resize along the height\n",
    "        # and then update the deltas crop along the width\n",
    "        else:\n",
    "            image = imutils.resize(image, height=self.height, inter=self.inter)\n",
    "            dW = int((image.shape[1] - self.width) / 2.0)\n",
    "\n",
    "        # now that our images have been resized, we need to re-grab the width and\n",
    "        # height, followed by performing the crop\n",
    "        (h, w) = image.shape[:2]\n",
    "        image = image[dH:h - dH, dW:w - dW]\n",
    "\n",
    "        # finally, resize the image to the provided spatial dimensions to ensure our\n",
    "        # output image is always a fixed size\n",
    "        return cv2.resize(image, (self.width, self.height), interpolation=self.inter)\n",
    "\n",
    "# convert images to arrays\n",
    "class ImageToArrayPreprocessor:\n",
    "    def __init__(self, dataFormat=None):\n",
    "        # store the image data format\n",
    "        self.dataFormat = dataFormat\n",
    "\n",
    "    def preprocess(self, image):\n",
    "        # apply the Keras utility function that correctly rearranges the dimensions\n",
    "        # of the image\n",
    "        return img_to_array(image, data_format=self.dataFormat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning\n",
    "\n",
    "Now, let's try to implement some fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fineTune(datasetPath, modelPath):\n",
    "    # initialize an image generator for data augmentation\n",
    "    aug = ImageDataGenerator(rotation_range = 30, width_shift_range = 0.1, height_shift_range = 0.1,\n",
    "                             shear_range = 0.2, zoom_range = 0.2, horizontal_flip = True,\n",
    "                             fill_mode = 'nearest')\n",
    "    \n",
    "    # get the list of image paths and extract class labels\n",
    "    imagePaths = list(paths.list_images(datasetPath))\n",
    "    classNames = [pt.split(os.path.sep)[-2] for pt in imagePaths]\n",
    "    classNames = [str(x) for x in np.unique(classNames)]\n",
    "    \n",
    "    # initialize image preprocessors\n",
    "    aap = AspectAwarePreprocessor(224, 224)\n",
    "    iap = ImageToArrayPreprocessor()\n",
    "    \n",
    "    # load dataset and scale pixel intensities to [0, 1]\n",
    "    sdl = SimpleDatasetLoader(preprocessors = [aap, iap])\n",
    "    (data, labels) = sdl.load(imagePaths, verbose = 500)\n",
    "    data = data.astype('float') / 255.0\n",
    "    \n",
    "    # partition into training/test splits\n",
    "    (trainX, testX, trainY, testY) = train_test_split(data, labels, test_size = 0.25)\n",
    "    \n",
    "    trainY = LabelBinarizer().fit_transform(trainY)\n",
    "    testY = LabelBinarizer().fit_transform(testY)\n",
    "    \n",
    "    # load VGG16 net without the head\n",
    "    baseModel = VGG16(weights = 'imagenet', include_top = False,\n",
    "                      input_tensor = Input(shape = (224, 224, 3)))\n",
    "    \n",
    "    # initialize the new head\n",
    "    headModel = FCHeadNet.build(baseModel, len(classNames), 256)\n",
    "    \n",
    "    # build the model with the new head\n",
    "    model = Model(inputs = baseModel.input, outputs = headModel)\n",
    "    \n",
    "    # loop over the layers of the base model and \"freeze\" them so they are not\n",
    "    # updated during training\n",
    "    for layer in baseModel.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    # compile the model with a small learning rate\n",
    "    print('Compiling model...')\n",
    "    opt = RMSprop(learning_rate = 0.001)\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics = ['accuracy'])\n",
    "    \n",
    "    # train the head of the network for a few iterations to initialize it a little better\n",
    "    # than randomly\n",
    "    print('Training head...')\n",
    "    model.fit(aug.flow(trainX, trainY, batch_size = 32), validation_data = (testX, testY),\n",
    "              epochs = 25, verbose = 1)\n",
    "    \n",
    "    # evaluate the net after this \"smart\" initialization\n",
    "    print('Evaluating after initialization...')\n",
    "    predictions = model.predict(testX, batch_size = 32)\n",
    "    print(classification_report(testY.argmax(axis = 1), predictions.argmax(axis = 1),\n",
    "                                target_names = classNames))\n",
    "    \n",
    "    # unfreeze the final set of convolutional layers\n",
    "    for layer in baseModel.layers[15:]:\n",
    "        layer.trainable = True\n",
    "        \n",
    "    # recompile the model\n",
    "    print('Re-compiling model...')\n",
    "    opt = SGD(learning_rate = 0.001)\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics = ['accuracy'])\n",
    "    \n",
    "    # train the model again, fine-tuning the last few convolutional layers plus the head\n",
    "    model.fit(aug.flow(trainX, trainY, batch_size = 32), validation_data = (testX, testY),\n",
    "              epochs = 100, steps_per_epoch = len(trainX) // 32, verbose = 1)\n",
    "    \n",
    "    # evaluate the net\n",
    "    print('Evaluating after fine-tuning...')\n",
    "    predictions = model.predict(testX, batch_size = 32)\n",
    "    print(classification_report(testY.argmax(axis = 1), predictions.argmax(axis = 1),\n",
    "                                target_names = classNames))\n",
    "    \n",
    "    # save the model\n",
    "    print('Saving the model to disk...')\n",
    "    model.save(modelPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] processed 500/1360\n",
      "[INFO] processed 1000/1360\n",
      "Compiling model...\n",
      "Training head...\n",
      "Epoch 1/25\n",
      "32/32 [==============================] - 6s 188ms/step - loss: 4.4729 - accuracy: 0.1343 - val_loss: 2.2210 - val_accuracy: 0.3176\n",
      "Epoch 2/25\n",
      "32/32 [==============================] - 6s 188ms/step - loss: 2.4206 - accuracy: 0.2598 - val_loss: 1.6009 - val_accuracy: 0.5324\n",
      "Epoch 3/25\n",
      "32/32 [==============================] - 6s 185ms/step - loss: 1.9297 - accuracy: 0.3725 - val_loss: 1.3554 - val_accuracy: 0.6147\n",
      "Epoch 4/25\n",
      "32/32 [==============================] - 6s 185ms/step - loss: 1.6727 - accuracy: 0.4863 - val_loss: 1.2081 - val_accuracy: 0.6441\n",
      "Epoch 5/25\n",
      "32/32 [==============================] - 6s 186ms/step - loss: 1.4715 - accuracy: 0.5167 - val_loss: 1.0848 - val_accuracy: 0.6618\n",
      "Epoch 6/25\n",
      "32/32 [==============================] - 6s 185ms/step - loss: 1.4999 - accuracy: 0.5235 - val_loss: 1.0940 - val_accuracy: 0.6559\n",
      "Epoch 7/25\n",
      "32/32 [==============================] - 6s 185ms/step - loss: 1.2565 - accuracy: 0.5902 - val_loss: 0.8428 - val_accuracy: 0.7618\n",
      "Epoch 8/25\n",
      "32/32 [==============================] - 6s 186ms/step - loss: 1.2044 - accuracy: 0.6186 - val_loss: 0.7816 - val_accuracy: 0.7647\n",
      "Epoch 9/25\n",
      "32/32 [==============================] - 6s 185ms/step - loss: 1.1336 - accuracy: 0.6284 - val_loss: 0.6270 - val_accuracy: 0.8147\n",
      "Epoch 10/25\n",
      "32/32 [==============================] - 6s 186ms/step - loss: 1.0562 - accuracy: 0.6500 - val_loss: 0.7869 - val_accuracy: 0.7353\n",
      "Epoch 11/25\n",
      "32/32 [==============================] - 6s 186ms/step - loss: 1.0541 - accuracy: 0.6627 - val_loss: 0.6786 - val_accuracy: 0.7706\n",
      "Epoch 12/25\n",
      "32/32 [==============================] - 6s 186ms/step - loss: 0.9383 - accuracy: 0.7029 - val_loss: 0.6745 - val_accuracy: 0.7971\n",
      "Epoch 13/25\n",
      "32/32 [==============================] - 6s 187ms/step - loss: 0.9141 - accuracy: 0.6990 - val_loss: 0.6626 - val_accuracy: 0.7794\n",
      "Epoch 14/25\n",
      "32/32 [==============================] - 6s 185ms/step - loss: 0.8836 - accuracy: 0.7049 - val_loss: 0.5385 - val_accuracy: 0.8353\n",
      "Epoch 15/25\n",
      "32/32 [==============================] - 6s 185ms/step - loss: 0.8122 - accuracy: 0.7353 - val_loss: 0.5832 - val_accuracy: 0.8206\n",
      "Epoch 16/25\n",
      "32/32 [==============================] - 6s 187ms/step - loss: 0.8009 - accuracy: 0.7294 - val_loss: 0.5808 - val_accuracy: 0.8118\n",
      "Epoch 17/25\n",
      "32/32 [==============================] - 6s 186ms/step - loss: 0.7438 - accuracy: 0.7588 - val_loss: 0.5703 - val_accuracy: 0.8235\n",
      "Epoch 18/25\n",
      "32/32 [==============================] - 6s 186ms/step - loss: 0.7647 - accuracy: 0.7471 - val_loss: 0.5174 - val_accuracy: 0.8382\n",
      "Epoch 19/25\n",
      "32/32 [==============================] - 6s 185ms/step - loss: 0.7078 - accuracy: 0.7647 - val_loss: 0.5063 - val_accuracy: 0.8294\n",
      "Epoch 20/25\n",
      "32/32 [==============================] - 6s 186ms/step - loss: 0.7189 - accuracy: 0.7706 - val_loss: 0.6094 - val_accuracy: 0.8206\n",
      "Epoch 21/25\n",
      "32/32 [==============================] - 6s 185ms/step - loss: 0.7197 - accuracy: 0.7608 - val_loss: 0.4894 - val_accuracy: 0.8500\n",
      "Epoch 22/25\n",
      "32/32 [==============================] - 6s 185ms/step - loss: 0.6393 - accuracy: 0.7765 - val_loss: 0.6751 - val_accuracy: 0.8206\n",
      "Epoch 23/25\n",
      "32/32 [==============================] - 6s 185ms/step - loss: 0.6600 - accuracy: 0.7892 - val_loss: 0.5292 - val_accuracy: 0.8324\n",
      "Epoch 24/25\n",
      "32/32 [==============================] - 6s 185ms/step - loss: 0.6120 - accuracy: 0.8098 - val_loss: 0.4617 - val_accuracy: 0.8588\n",
      "Epoch 25/25\n",
      "32/32 [==============================] - 6s 185ms/step - loss: 0.5913 - accuracy: 0.8039 - val_loss: 0.5042 - val_accuracy: 0.8471\n",
      "Evaluating after initialization...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    bluebell       0.93      0.61      0.74        23\n",
      "   buttercup       0.94      0.88      0.91        17\n",
      "   coltsfoot       0.88      0.74      0.80        19\n",
      "     cowslip       0.81      0.68      0.74        25\n",
      "      crocus       0.67      0.94      0.78        17\n",
      "    daffodil       0.73      1.00      0.84        16\n",
      "       daisy       1.00      0.94      0.97        17\n",
      "   dandelion       0.92      0.92      0.92        26\n",
      "  fritillary       0.92      0.69      0.79        16\n",
      "        iris       0.90      1.00      0.95        18\n",
      "  lilyvalley       0.82      0.95      0.88        19\n",
      "       pansy       1.00      0.95      0.98        21\n",
      "    snowdrop       0.90      0.79      0.84        24\n",
      "   sunflower       0.94      1.00      0.97        17\n",
      "   tigerlily       0.83      0.95      0.89        21\n",
      "       tulip       0.56      0.68      0.61        22\n",
      "  windflower       0.90      0.82      0.86        22\n",
      "\n",
      "    accuracy                           0.85       340\n",
      "   macro avg       0.86      0.86      0.85       340\n",
      "weighted avg       0.86      0.85      0.85       340\n",
      "\n",
      "Re-compiling model...\n",
      "Epoch 1/100\n",
      "31/31 [==============================] - 6s 190ms/step - loss: 0.4718 - accuracy: 0.8431 - val_loss: 0.4428 - val_accuracy: 0.8676\n",
      "Epoch 2/100\n",
      "31/31 [==============================] - 6s 186ms/step - loss: 0.4214 - accuracy: 0.8512 - val_loss: 0.4149 - val_accuracy: 0.8765\n",
      "Epoch 3/100\n",
      "31/31 [==============================] - 6s 186ms/step - loss: 0.3462 - accuracy: 0.8846 - val_loss: 0.3685 - val_accuracy: 0.8794\n",
      "Epoch 4/100\n",
      "31/31 [==============================] - 6s 186ms/step - loss: 0.3910 - accuracy: 0.8644 - val_loss: 0.3885 - val_accuracy: 0.8794\n",
      "Epoch 5/100\n",
      "31/31 [==============================] - 6s 186ms/step - loss: 0.3513 - accuracy: 0.8775 - val_loss: 0.3951 - val_accuracy: 0.8853\n",
      "Epoch 6/100\n",
      "31/31 [==============================] - 6s 186ms/step - loss: 0.3581 - accuracy: 0.8907 - val_loss: 0.3771 - val_accuracy: 0.8824\n",
      "Epoch 7/100\n",
      "31/31 [==============================] - 6s 185ms/step - loss: 0.3272 - accuracy: 0.8887 - val_loss: 0.3618 - val_accuracy: 0.8853\n",
      "Epoch 8/100\n",
      "31/31 [==============================] - 6s 186ms/step - loss: 0.3302 - accuracy: 0.8806 - val_loss: 0.3828 - val_accuracy: 0.8824\n",
      "Epoch 9/100\n",
      "31/31 [==============================] - 6s 185ms/step - loss: 0.3294 - accuracy: 0.8887 - val_loss: 0.3543 - val_accuracy: 0.8765\n",
      "Epoch 10/100\n",
      "31/31 [==============================] - 6s 186ms/step - loss: 0.2930 - accuracy: 0.9018 - val_loss: 0.3453 - val_accuracy: 0.8971\n",
      "Epoch 11/100\n",
      "31/31 [==============================] - 6s 186ms/step - loss: 0.3434 - accuracy: 0.8846 - val_loss: 0.3608 - val_accuracy: 0.8824\n",
      "Epoch 12/100\n",
      "31/31 [==============================] - 6s 186ms/step - loss: 0.2915 - accuracy: 0.9099 - val_loss: 0.3583 - val_accuracy: 0.8912\n",
      "Epoch 13/100\n",
      "31/31 [==============================] - 6s 186ms/step - loss: 0.2894 - accuracy: 0.9069 - val_loss: 0.3563 - val_accuracy: 0.8912\n",
      "Epoch 14/100\n",
      "31/31 [==============================] - 6s 186ms/step - loss: 0.2659 - accuracy: 0.8978 - val_loss: 0.3474 - val_accuracy: 0.8971\n",
      "Epoch 15/100\n",
      "31/31 [==============================] - 6s 186ms/step - loss: 0.2605 - accuracy: 0.9200 - val_loss: 0.3514 - val_accuracy: 0.8912\n",
      "Epoch 16/100\n",
      "31/31 [==============================] - 6s 185ms/step - loss: 0.2795 - accuracy: 0.9109 - val_loss: 0.3487 - val_accuracy: 0.8824\n",
      "Epoch 17/100\n",
      "31/31 [==============================] - 6s 186ms/step - loss: 0.2197 - accuracy: 0.9362 - val_loss: 0.3773 - val_accuracy: 0.9029\n",
      "Epoch 18/100\n",
      "31/31 [==============================] - 6s 185ms/step - loss: 0.2600 - accuracy: 0.9190 - val_loss: 0.3323 - val_accuracy: 0.9059\n",
      "Epoch 19/100\n",
      "31/31 [==============================] - 6s 186ms/step - loss: 0.2289 - accuracy: 0.9312 - val_loss: 0.3686 - val_accuracy: 0.8912\n",
      "Epoch 20/100\n",
      "31/31 [==============================] - 6s 185ms/step - loss: 0.2432 - accuracy: 0.9150 - val_loss: 0.3685 - val_accuracy: 0.8912\n",
      "Epoch 21/100\n",
      "31/31 [==============================] - 6s 185ms/step - loss: 0.2377 - accuracy: 0.9140 - val_loss: 0.3397 - val_accuracy: 0.9000\n",
      "Epoch 22/100\n",
      "31/31 [==============================] - 6s 186ms/step - loss: 0.2436 - accuracy: 0.9069 - val_loss: 0.3154 - val_accuracy: 0.9029\n",
      "Epoch 23/100\n",
      "31/31 [==============================] - 6s 185ms/step - loss: 0.2215 - accuracy: 0.9231 - val_loss: 0.3477 - val_accuracy: 0.8912\n",
      "Epoch 24/100\n",
      "31/31 [==============================] - 6s 186ms/step - loss: 0.1975 - accuracy: 0.9342 - val_loss: 0.3177 - val_accuracy: 0.9088\n",
      "Epoch 25/100\n",
      "31/31 [==============================] - 6s 186ms/step - loss: 0.1993 - accuracy: 0.9251 - val_loss: 0.3623 - val_accuracy: 0.8971\n",
      "Epoch 26/100\n",
      "31/31 [==============================] - 6s 185ms/step - loss: 0.2066 - accuracy: 0.9281 - val_loss: 0.3498 - val_accuracy: 0.8941\n",
      "Epoch 27/100\n",
      "31/31 [==============================] - 6s 185ms/step - loss: 0.1950 - accuracy: 0.9433 - val_loss: 0.3593 - val_accuracy: 0.9088\n",
      "Epoch 28/100\n",
      "31/31 [==============================] - 6s 185ms/step - loss: 0.1996 - accuracy: 0.9352 - val_loss: 0.3518 - val_accuracy: 0.9118\n",
      "Epoch 29/100\n",
      "31/31 [==============================] - 6s 185ms/step - loss: 0.2529 - accuracy: 0.9160 - val_loss: 0.3413 - val_accuracy: 0.9059\n",
      "Epoch 30/100\n",
      "31/31 [==============================] - 6s 185ms/step - loss: 0.1913 - accuracy: 0.9443 - val_loss: 0.3407 - val_accuracy: 0.9059\n",
      "Epoch 31/100\n",
      "31/31 [==============================] - 6s 186ms/step - loss: 0.1916 - accuracy: 0.9383 - val_loss: 0.3020 - val_accuracy: 0.9088\n",
      "Epoch 32/100\n",
      "31/31 [==============================] - 6s 185ms/step - loss: 0.2044 - accuracy: 0.9342 - val_loss: 0.3107 - val_accuracy: 0.9088\n",
      "Epoch 33/100\n",
      "31/31 [==============================] - 6s 185ms/step - loss: 0.2066 - accuracy: 0.9372 - val_loss: 0.3305 - val_accuracy: 0.9147\n",
      "Epoch 34/100\n",
      "31/31 [==============================] - 6s 186ms/step - loss: 0.2044 - accuracy: 0.9261 - val_loss: 0.3462 - val_accuracy: 0.9147\n",
      "Epoch 35/100\n",
      "31/31 [==============================] - 6s 186ms/step - loss: 0.1722 - accuracy: 0.9423 - val_loss: 0.3348 - val_accuracy: 0.9088\n",
      "Epoch 36/100\n",
      "31/31 [==============================] - 6s 186ms/step - loss: 0.1716 - accuracy: 0.9413 - val_loss: 0.3205 - val_accuracy: 0.9059\n",
      "Epoch 37/100\n",
      "31/31 [==============================] - 6s 186ms/step - loss: 0.1915 - accuracy: 0.9342 - val_loss: 0.3303 - val_accuracy: 0.9029\n",
      "Epoch 38/100\n",
      "31/31 [==============================] - 6s 185ms/step - loss: 0.2113 - accuracy: 0.9261 - val_loss: 0.3191 - val_accuracy: 0.9029\n",
      "Epoch 39/100\n",
      "31/31 [==============================] - 6s 186ms/step - loss: 0.1692 - accuracy: 0.9413 - val_loss: 0.3476 - val_accuracy: 0.9118\n",
      "Epoch 40/100\n",
      "31/31 [==============================] - 6s 186ms/step - loss: 0.1539 - accuracy: 0.9484 - val_loss: 0.3434 - val_accuracy: 0.9088\n",
      "Epoch 41/100\n",
      "31/31 [==============================] - 6s 185ms/step - loss: 0.2109 - accuracy: 0.9211 - val_loss: 0.3211 - val_accuracy: 0.9059\n",
      "Epoch 42/100\n",
      "31/31 [==============================] - 6s 186ms/step - loss: 0.1689 - accuracy: 0.9474 - val_loss: 0.3650 - val_accuracy: 0.9029\n",
      "Epoch 43/100\n",
      "31/31 [==============================] - 6s 185ms/step - loss: 0.1505 - accuracy: 0.9484 - val_loss: 0.3263 - val_accuracy: 0.9088\n",
      "Epoch 44/100\n",
      "31/31 [==============================] - 6s 186ms/step - loss: 0.1875 - accuracy: 0.9281 - val_loss: 0.3245 - val_accuracy: 0.9088\n",
      "Epoch 45/100\n",
      "31/31 [==============================] - 6s 186ms/step - loss: 0.1695 - accuracy: 0.9443 - val_loss: 0.3476 - val_accuracy: 0.9118\n",
      "Epoch 46/100\n",
      "31/31 [==============================] - 6s 186ms/step - loss: 0.1700 - accuracy: 0.9423 - val_loss: 0.3169 - val_accuracy: 0.9176\n",
      "Epoch 47/100\n",
      "31/31 [==============================] - 6s 186ms/step - loss: 0.1814 - accuracy: 0.9302 - val_loss: 0.3564 - val_accuracy: 0.9118\n",
      "Epoch 48/100\n",
      "31/31 [==============================] - 6s 185ms/step - loss: 0.1845 - accuracy: 0.9393 - val_loss: 0.3256 - val_accuracy: 0.9059\n",
      "Epoch 49/100\n",
      "31/31 [==============================] - 6s 186ms/step - loss: 0.1601 - accuracy: 0.9514 - val_loss: 0.3181 - val_accuracy: 0.9235\n",
      "Epoch 50/100\n",
      "31/31 [==============================] - 6s 186ms/step - loss: 0.1576 - accuracy: 0.9494 - val_loss: 0.3278 - val_accuracy: 0.9088\n",
      "Epoch 51/100\n",
      "31/31 [==============================] - 6s 186ms/step - loss: 0.1852 - accuracy: 0.9403 - val_loss: 0.3275 - val_accuracy: 0.9118\n",
      "Epoch 52/100\n",
      "31/31 [==============================] - 6s 186ms/step - loss: 0.1418 - accuracy: 0.9524 - val_loss: 0.3004 - val_accuracy: 0.9147\n",
      "Epoch 53/100\n",
      "31/31 [==============================] - 6s 186ms/step - loss: 0.1708 - accuracy: 0.9393 - val_loss: 0.3229 - val_accuracy: 0.9029\n",
      "Epoch 54/100\n",
      "31/31 [==============================] - 6s 185ms/step - loss: 0.1676 - accuracy: 0.9433 - val_loss: 0.3553 - val_accuracy: 0.9029\n",
      "Epoch 55/100\n",
      "31/31 [==============================] - 6s 186ms/step - loss: 0.1504 - accuracy: 0.9474 - val_loss: 0.2948 - val_accuracy: 0.9235\n",
      "Epoch 56/100\n",
      "31/31 [==============================] - 6s 187ms/step - loss: 0.1879 - accuracy: 0.9302 - val_loss: 0.3120 - val_accuracy: 0.9118\n",
      "Epoch 57/100\n",
      "31/31 [==============================] - 6s 186ms/step - loss: 0.1493 - accuracy: 0.9494 - val_loss: 0.3169 - val_accuracy: 0.9235\n",
      "Epoch 58/100\n",
      "31/31 [==============================] - 6s 187ms/step - loss: 0.1512 - accuracy: 0.9534 - val_loss: 0.2972 - val_accuracy: 0.9235\n",
      "Epoch 59/100\n",
      "31/31 [==============================] - 6s 186ms/step - loss: 0.1680 - accuracy: 0.9453 - val_loss: 0.3599 - val_accuracy: 0.9147\n",
      "Epoch 60/100\n",
      "31/31 [==============================] - 6s 187ms/step - loss: 0.1551 - accuracy: 0.9456 - val_loss: 0.3501 - val_accuracy: 0.9147\n",
      "Epoch 61/100\n",
      "31/31 [==============================] - 6s 186ms/step - loss: 0.1538 - accuracy: 0.9474 - val_loss: 0.3294 - val_accuracy: 0.9059\n",
      "Epoch 62/100\n",
      "31/31 [==============================] - 6s 186ms/step - loss: 0.1579 - accuracy: 0.9443 - val_loss: 0.2945 - val_accuracy: 0.9235\n",
      "Epoch 63/100\n",
      "31/31 [==============================] - 6s 186ms/step - loss: 0.1623 - accuracy: 0.9393 - val_loss: 0.2857 - val_accuracy: 0.9176\n",
      "Epoch 64/100\n",
      "31/31 [==============================] - 6s 185ms/step - loss: 0.1390 - accuracy: 0.9555 - val_loss: 0.3383 - val_accuracy: 0.9088\n",
      "Epoch 65/100\n",
      "31/31 [==============================] - 6s 186ms/step - loss: 0.1371 - accuracy: 0.9534 - val_loss: 0.3563 - val_accuracy: 0.9206\n",
      "Epoch 66/100\n",
      "31/31 [==============================] - 6s 186ms/step - loss: 0.1521 - accuracy: 0.9494 - val_loss: 0.2950 - val_accuracy: 0.9206\n",
      "Epoch 67/100\n",
      "31/31 [==============================] - 6s 185ms/step - loss: 0.1551 - accuracy: 0.9514 - val_loss: 0.2890 - val_accuracy: 0.9176\n",
      "Epoch 68/100\n",
      "31/31 [==============================] - 6s 186ms/step - loss: 0.1498 - accuracy: 0.9514 - val_loss: 0.3799 - val_accuracy: 0.9088\n",
      "Epoch 69/100\n",
      "31/31 [==============================] - 6s 186ms/step - loss: 0.1787 - accuracy: 0.9413 - val_loss: 0.3133 - val_accuracy: 0.9147\n",
      "Epoch 70/100\n",
      "31/31 [==============================] - 6s 186ms/step - loss: 0.1572 - accuracy: 0.9504 - val_loss: 0.3414 - val_accuracy: 0.9059\n",
      "Epoch 71/100\n",
      "31/31 [==============================] - 6s 186ms/step - loss: 0.1354 - accuracy: 0.9484 - val_loss: 0.3172 - val_accuracy: 0.9176\n",
      "Epoch 72/100\n",
      "31/31 [==============================] - 6s 185ms/step - loss: 0.1580 - accuracy: 0.9464 - val_loss: 0.3464 - val_accuracy: 0.9176\n",
      "Epoch 73/100\n",
      "31/31 [==============================] - 6s 186ms/step - loss: 0.1319 - accuracy: 0.9565 - val_loss: 0.3248 - val_accuracy: 0.9265\n",
      "Epoch 74/100\n",
      "31/31 [==============================] - 6s 185ms/step - loss: 0.1492 - accuracy: 0.9464 - val_loss: 0.3154 - val_accuracy: 0.9206\n",
      "Epoch 75/100\n",
      "31/31 [==============================] - 6s 186ms/step - loss: 0.1498 - accuracy: 0.9464 - val_loss: 0.3368 - val_accuracy: 0.9235\n",
      "Epoch 76/100\n",
      "31/31 [==============================] - 6s 185ms/step - loss: 0.1786 - accuracy: 0.9494 - val_loss: 0.2896 - val_accuracy: 0.9235\n",
      "Epoch 77/100\n",
      "31/31 [==============================] - 6s 186ms/step - loss: 0.1614 - accuracy: 0.9413 - val_loss: 0.2981 - val_accuracy: 0.9294\n",
      "Epoch 78/100\n",
      "31/31 [==============================] - 6s 185ms/step - loss: 0.1595 - accuracy: 0.9413 - val_loss: 0.2786 - val_accuracy: 0.9206\n",
      "Epoch 79/100\n",
      "31/31 [==============================] - 6s 186ms/step - loss: 0.1507 - accuracy: 0.9476 - val_loss: 0.3357 - val_accuracy: 0.9176\n",
      "Epoch 80/100\n",
      "31/31 [==============================] - 6s 185ms/step - loss: 0.1353 - accuracy: 0.9524 - val_loss: 0.3273 - val_accuracy: 0.9147\n",
      "Epoch 81/100\n",
      "31/31 [==============================] - 6s 185ms/step - loss: 0.1319 - accuracy: 0.9464 - val_loss: 0.3459 - val_accuracy: 0.9176\n",
      "Epoch 82/100\n",
      "31/31 [==============================] - 6s 185ms/step - loss: 0.1154 - accuracy: 0.9615 - val_loss: 0.3178 - val_accuracy: 0.9206\n",
      "Epoch 83/100\n",
      "31/31 [==============================] - 6s 185ms/step - loss: 0.1411 - accuracy: 0.9524 - val_loss: 0.2922 - val_accuracy: 0.9206\n",
      "Epoch 84/100\n",
      "31/31 [==============================] - 6s 186ms/step - loss: 0.1431 - accuracy: 0.9514 - val_loss: 0.2832 - val_accuracy: 0.9235\n",
      "Epoch 85/100\n",
      "31/31 [==============================] - 6s 185ms/step - loss: 0.1449 - accuracy: 0.9474 - val_loss: 0.3248 - val_accuracy: 0.9176\n",
      "Epoch 86/100\n",
      "31/31 [==============================] - 6s 186ms/step - loss: 0.0930 - accuracy: 0.9717 - val_loss: 0.3017 - val_accuracy: 0.9206\n",
      "Epoch 87/100\n",
      "31/31 [==============================] - 6s 186ms/step - loss: 0.1403 - accuracy: 0.9524 - val_loss: 0.2973 - val_accuracy: 0.9235\n",
      "Epoch 88/100\n",
      "31/31 [==============================] - 6s 185ms/step - loss: 0.1505 - accuracy: 0.9474 - val_loss: 0.3099 - val_accuracy: 0.9176\n",
      "Epoch 89/100\n",
      "31/31 [==============================] - 6s 185ms/step - loss: 0.1204 - accuracy: 0.9555 - val_loss: 0.3168 - val_accuracy: 0.9176\n",
      "Epoch 90/100\n",
      "31/31 [==============================] - 6s 187ms/step - loss: 0.1145 - accuracy: 0.9636 - val_loss: 0.3025 - val_accuracy: 0.9147\n",
      "Epoch 91/100\n",
      "31/31 [==============================] - 6s 185ms/step - loss: 0.1462 - accuracy: 0.9484 - val_loss: 0.2832 - val_accuracy: 0.9265\n",
      "Epoch 92/100\n",
      "31/31 [==============================] - 6s 186ms/step - loss: 0.1302 - accuracy: 0.9534 - val_loss: 0.3065 - val_accuracy: 0.9147\n",
      "Epoch 93/100\n",
      "31/31 [==============================] - 6s 185ms/step - loss: 0.1295 - accuracy: 0.9575 - val_loss: 0.2949 - val_accuracy: 0.9206\n",
      "Epoch 94/100\n",
      "31/31 [==============================] - 6s 185ms/step - loss: 0.1161 - accuracy: 0.9646 - val_loss: 0.3155 - val_accuracy: 0.9235\n",
      "Epoch 95/100\n",
      "31/31 [==============================] - 6s 187ms/step - loss: 0.1216 - accuracy: 0.9526 - val_loss: 0.3037 - val_accuracy: 0.9206\n",
      "Epoch 96/100\n",
      "31/31 [==============================] - 6s 186ms/step - loss: 0.1147 - accuracy: 0.9626 - val_loss: 0.2902 - val_accuracy: 0.9206\n",
      "Epoch 97/100\n",
      "31/31 [==============================] - 6s 185ms/step - loss: 0.1324 - accuracy: 0.9565 - val_loss: 0.2895 - val_accuracy: 0.9235\n",
      "Epoch 98/100\n",
      "31/31 [==============================] - 6s 185ms/step - loss: 0.1320 - accuracy: 0.9585 - val_loss: 0.3146 - val_accuracy: 0.9206\n",
      "Epoch 99/100\n",
      "31/31 [==============================] - 6s 186ms/step - loss: 0.1295 - accuracy: 0.9565 - val_loss: 0.3085 - val_accuracy: 0.9265\n",
      "Epoch 100/100\n",
      "31/31 [==============================] - 6s 185ms/step - loss: 0.1071 - accuracy: 0.9646 - val_loss: 0.2946 - val_accuracy: 0.9235\n",
      "Evaluating after fine-tuning...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    bluebell       0.92      1.00      0.96        23\n",
      "   buttercup       0.89      0.94      0.91        17\n",
      "   coltsfoot       0.88      0.74      0.80        19\n",
      "     cowslip       0.91      0.80      0.85        25\n",
      "      crocus       1.00      0.82      0.90        17\n",
      "    daffodil       0.82      0.88      0.85        16\n",
      "       daisy       1.00      1.00      1.00        17\n",
      "   dandelion       0.96      0.88      0.92        26\n",
      "  fritillary       0.94      0.94      0.94        16\n",
      "        iris       1.00      1.00      1.00        18\n",
      "  lilyvalley       0.83      1.00      0.90        19\n",
      "       pansy       1.00      1.00      1.00        21\n",
      "    snowdrop       0.82      0.96      0.88        24\n",
      "   sunflower       1.00      1.00      1.00        17\n",
      "   tigerlily       1.00      1.00      1.00        21\n",
      "       tulip       0.82      0.82      0.82        22\n",
      "  windflower       1.00      0.95      0.98        22\n",
      "\n",
      "    accuracy                           0.92       340\n",
      "   macro avg       0.93      0.93      0.92       340\n",
      "weighted avg       0.93      0.92      0.92       340\n",
      "\n",
      "Saving the model to disk...\n",
      "INFO:tensorflow:Assets written to: flowers17.model\\assets\n"
     ]
    }
   ],
   "source": [
    "fineTune('../data/flowers17/images', 'flowers17.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] processed 500/3000\n",
      "[INFO] processed 1000/3000\n",
      "[INFO] processed 1500/3000\n",
      "[INFO] processed 2000/3000\n",
      "[INFO] processed 2500/3000\n",
      "[INFO] processed 3000/3000\n",
      "Compiling model...\n",
      "Training head...\n",
      "Epoch 1/25\n",
      "71/71 [==============================] - 15s 206ms/step - loss: 2.2768 - accuracy: 0.5916 - val_loss: 0.5350 - val_accuracy: 0.7573\n",
      "Epoch 2/25\n",
      "71/71 [==============================] - 13s 183ms/step - loss: 0.6396 - accuracy: 0.7476 - val_loss: 0.2795 - val_accuracy: 0.8920\n",
      "Epoch 3/25\n",
      "71/71 [==============================] - 13s 182ms/step - loss: 0.5362 - accuracy: 0.7809 - val_loss: 0.2844 - val_accuracy: 0.8747\n",
      "Epoch 4/25\n",
      "71/71 [==============================] - 13s 181ms/step - loss: 0.4438 - accuracy: 0.8204 - val_loss: 0.2233 - val_accuracy: 0.9160\n",
      "Epoch 5/25\n",
      "71/71 [==============================] - 13s 182ms/step - loss: 0.4470 - accuracy: 0.8258 - val_loss: 0.2560 - val_accuracy: 0.9093\n",
      "Epoch 6/25\n",
      "71/71 [==============================] - 13s 185ms/step - loss: 0.4037 - accuracy: 0.8364 - val_loss: 0.2200 - val_accuracy: 0.9173\n",
      "Epoch 7/25\n",
      "71/71 [==============================] - 13s 182ms/step - loss: 0.4181 - accuracy: 0.8431 - val_loss: 0.2203 - val_accuracy: 0.9147\n",
      "Epoch 8/25\n",
      "71/71 [==============================] - 13s 182ms/step - loss: 0.3545 - accuracy: 0.8516 - val_loss: 0.2428 - val_accuracy: 0.9067\n",
      "Epoch 9/25\n",
      "71/71 [==============================] - 13s 182ms/step - loss: 0.3661 - accuracy: 0.8622 - val_loss: 0.2319 - val_accuracy: 0.9200\n",
      "Epoch 10/25\n",
      "71/71 [==============================] - 13s 182ms/step - loss: 0.3455 - accuracy: 0.8596 - val_loss: 0.2367 - val_accuracy: 0.9213\n",
      "Epoch 11/25\n",
      "71/71 [==============================] - 13s 182ms/step - loss: 0.3343 - accuracy: 0.8680 - val_loss: 0.4640 - val_accuracy: 0.8480\n",
      "Epoch 12/25\n",
      "71/71 [==============================] - 13s 183ms/step - loss: 0.3298 - accuracy: 0.8756 - val_loss: 0.2491 - val_accuracy: 0.9213\n",
      "Epoch 13/25\n",
      "71/71 [==============================] - 13s 183ms/step - loss: 0.3357 - accuracy: 0.8760 - val_loss: 0.2282 - val_accuracy: 0.9173\n",
      "Epoch 14/25\n",
      "71/71 [==============================] - 13s 183ms/step - loss: 0.3045 - accuracy: 0.8844 - val_loss: 0.2563 - val_accuracy: 0.9000\n",
      "Epoch 15/25\n",
      "71/71 [==============================] - 13s 182ms/step - loss: 0.3082 - accuracy: 0.8818 - val_loss: 0.3116 - val_accuracy: 0.8987\n",
      "Epoch 16/25\n",
      "71/71 [==============================] - 13s 183ms/step - loss: 0.2999 - accuracy: 0.8893 - val_loss: 0.3272 - val_accuracy: 0.9040\n",
      "Epoch 17/25\n",
      "71/71 [==============================] - 13s 182ms/step - loss: 0.2775 - accuracy: 0.8969 - val_loss: 0.2990 - val_accuracy: 0.9027\n",
      "Epoch 18/25\n",
      "71/71 [==============================] - 13s 183ms/step - loss: 0.2654 - accuracy: 0.8964 - val_loss: 0.2580 - val_accuracy: 0.9187\n",
      "Epoch 19/25\n",
      "71/71 [==============================] - 13s 182ms/step - loss: 0.2554 - accuracy: 0.9013 - val_loss: 0.2854 - val_accuracy: 0.9227\n",
      "Epoch 20/25\n",
      "71/71 [==============================] - 13s 182ms/step - loss: 0.2835 - accuracy: 0.8924 - val_loss: 0.2791 - val_accuracy: 0.9133\n",
      "Epoch 21/25\n",
      "71/71 [==============================] - 13s 183ms/step - loss: 0.2749 - accuracy: 0.8942 - val_loss: 0.2671 - val_accuracy: 0.9173\n",
      "Epoch 22/25\n",
      "71/71 [==============================] - 13s 182ms/step - loss: 0.2531 - accuracy: 0.9027 - val_loss: 0.3139 - val_accuracy: 0.9173\n",
      "Epoch 23/25\n",
      "71/71 [==============================] - 13s 183ms/step - loss: 0.2627 - accuracy: 0.9044 - val_loss: 0.3296 - val_accuracy: 0.9240\n",
      "Epoch 24/25\n",
      "71/71 [==============================] - 13s 183ms/step - loss: 0.2386 - accuracy: 0.9124 - val_loss: 0.3330 - val_accuracy: 0.9213\n",
      "Epoch 25/25\n",
      "71/71 [==============================] - 13s 182ms/step - loss: 0.2687 - accuracy: 0.8960 - val_loss: 0.2944 - val_accuracy: 0.9240\n",
      "Evaluating after initialization...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        cats       0.93      0.91      0.92       259\n",
      "        dogs       0.91      0.91      0.91       256\n",
      "       panda       0.93      0.95      0.94       235\n",
      "\n",
      "    accuracy                           0.92       750\n",
      "   macro avg       0.92      0.92      0.92       750\n",
      "weighted avg       0.92      0.92      0.92       750\n",
      "\n",
      "Re-compiling model...\n",
      "Epoch 1/100\n",
      "70/70 [==============================] - 13s 186ms/step - loss: 0.2187 - accuracy: 0.9211 - val_loss: 0.2770 - val_accuracy: 0.9333\n",
      "Epoch 2/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.2215 - accuracy: 0.9247 - val_loss: 0.2540 - val_accuracy: 0.9387\n",
      "Epoch 3/100\n",
      "70/70 [==============================] - 13s 186ms/step - loss: 0.2164 - accuracy: 0.9243 - val_loss: 0.2430 - val_accuracy: 0.9373\n",
      "Epoch 4/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.1480 - accuracy: 0.9427 - val_loss: 0.2591 - val_accuracy: 0.9360\n",
      "Epoch 5/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.1496 - accuracy: 0.9450 - val_loss: 0.2818 - val_accuracy: 0.9347\n",
      "Epoch 6/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.1522 - accuracy: 0.9427 - val_loss: 0.2275 - val_accuracy: 0.9427\n",
      "Epoch 7/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.1468 - accuracy: 0.9504 - val_loss: 0.2235 - val_accuracy: 0.9440\n",
      "Epoch 8/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.1277 - accuracy: 0.9504 - val_loss: 0.2277 - val_accuracy: 0.9467\n",
      "Epoch 9/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.1259 - accuracy: 0.9581 - val_loss: 0.2300 - val_accuracy: 0.9480\n",
      "Epoch 10/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.1315 - accuracy: 0.9554 - val_loss: 0.2554 - val_accuracy: 0.9387\n",
      "Epoch 11/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.0967 - accuracy: 0.9639 - val_loss: 0.2278 - val_accuracy: 0.9453\n",
      "Epoch 12/100\n",
      "70/70 [==============================] - 13s 186ms/step - loss: 0.1035 - accuracy: 0.9599 - val_loss: 0.2076 - val_accuracy: 0.9467\n",
      "Epoch 13/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.0975 - accuracy: 0.9630 - val_loss: 0.2194 - val_accuracy: 0.9480\n",
      "Epoch 14/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.1155 - accuracy: 0.9594 - val_loss: 0.2450 - val_accuracy: 0.9427\n",
      "Epoch 15/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.0903 - accuracy: 0.9702 - val_loss: 0.2157 - val_accuracy: 0.9467\n",
      "Epoch 16/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.1031 - accuracy: 0.9639 - val_loss: 0.2126 - val_accuracy: 0.9507\n",
      "Epoch 17/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.1076 - accuracy: 0.9608 - val_loss: 0.2100 - val_accuracy: 0.9453\n",
      "Epoch 18/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.0795 - accuracy: 0.9662 - val_loss: 0.2322 - val_accuracy: 0.9467\n",
      "Epoch 19/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.0882 - accuracy: 0.9666 - val_loss: 0.2241 - val_accuracy: 0.9440\n",
      "Epoch 20/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.0844 - accuracy: 0.9698 - val_loss: 0.2372 - val_accuracy: 0.9480\n",
      "Epoch 21/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.0808 - accuracy: 0.9725 - val_loss: 0.1919 - val_accuracy: 0.9547\n",
      "Epoch 22/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.0680 - accuracy: 0.9775 - val_loss: 0.2056 - val_accuracy: 0.9520\n",
      "Epoch 23/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.0645 - accuracy: 0.9779 - val_loss: 0.2037 - val_accuracy: 0.9560\n",
      "Epoch 24/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.0768 - accuracy: 0.9770 - val_loss: 0.1864 - val_accuracy: 0.9507\n",
      "Epoch 25/100\n",
      "70/70 [==============================] - 13s 186ms/step - loss: 0.0617 - accuracy: 0.9775 - val_loss: 0.2232 - val_accuracy: 0.9467\n",
      "Epoch 26/100\n",
      "70/70 [==============================] - 13s 186ms/step - loss: 0.0652 - accuracy: 0.9770 - val_loss: 0.1972 - val_accuracy: 0.9547\n",
      "Epoch 27/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.0708 - accuracy: 0.9770 - val_loss: 0.1870 - val_accuracy: 0.9533\n",
      "Epoch 28/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.0695 - accuracy: 0.9748 - val_loss: 0.2268 - val_accuracy: 0.9400\n",
      "Epoch 29/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.0656 - accuracy: 0.9729 - val_loss: 0.2066 - val_accuracy: 0.9520\n",
      "Epoch 30/100\n",
      "70/70 [==============================] - 13s 186ms/step - loss: 0.0704 - accuracy: 0.9793 - val_loss: 0.1985 - val_accuracy: 0.9493\n",
      "Epoch 31/100\n",
      "70/70 [==============================] - 13s 186ms/step - loss: 0.0682 - accuracy: 0.9793 - val_loss: 0.2060 - val_accuracy: 0.9480\n",
      "Epoch 32/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.0656 - accuracy: 0.9793 - val_loss: 0.2057 - val_accuracy: 0.9520\n",
      "Epoch 33/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.0608 - accuracy: 0.9802 - val_loss: 0.1923 - val_accuracy: 0.9560\n",
      "Epoch 34/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.0592 - accuracy: 0.9779 - val_loss: 0.2111 - val_accuracy: 0.9507\n",
      "Epoch 35/100\n",
      "70/70 [==============================] - 13s 186ms/step - loss: 0.0553 - accuracy: 0.9784 - val_loss: 0.2656 - val_accuracy: 0.9293\n",
      "Epoch 36/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.0641 - accuracy: 0.9802 - val_loss: 0.2480 - val_accuracy: 0.9413\n",
      "Epoch 37/100\n",
      "70/70 [==============================] - 13s 184ms/step - loss: 0.0515 - accuracy: 0.9820 - val_loss: 0.2451 - val_accuracy: 0.9427\n",
      "Epoch 38/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.0559 - accuracy: 0.9842 - val_loss: 0.2205 - val_accuracy: 0.9507\n",
      "Epoch 39/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.0447 - accuracy: 0.9838 - val_loss: 0.2266 - val_accuracy: 0.9520\n",
      "Epoch 40/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.0506 - accuracy: 0.9847 - val_loss: 0.2251 - val_accuracy: 0.9493\n",
      "Epoch 41/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.0471 - accuracy: 0.9838 - val_loss: 0.2457 - val_accuracy: 0.9467\n",
      "Epoch 42/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.0527 - accuracy: 0.9824 - val_loss: 0.2426 - val_accuracy: 0.9413\n",
      "Epoch 43/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.0444 - accuracy: 0.9847 - val_loss: 0.2435 - val_accuracy: 0.9480\n",
      "Epoch 44/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.0447 - accuracy: 0.9833 - val_loss: 0.2118 - val_accuracy: 0.9520\n",
      "Epoch 45/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.0412 - accuracy: 0.9883 - val_loss: 0.2236 - val_accuracy: 0.9587\n",
      "Epoch 46/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.0352 - accuracy: 0.9865 - val_loss: 0.2304 - val_accuracy: 0.9533\n",
      "Epoch 47/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.0513 - accuracy: 0.9824 - val_loss: 0.2196 - val_accuracy: 0.9533\n",
      "Epoch 48/100\n",
      "70/70 [==============================] - 13s 186ms/step - loss: 0.0296 - accuracy: 0.9901 - val_loss: 0.2364 - val_accuracy: 0.9493\n",
      "Epoch 49/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.0498 - accuracy: 0.9833 - val_loss: 0.2484 - val_accuracy: 0.9467\n",
      "Epoch 50/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.0447 - accuracy: 0.9856 - val_loss: 0.2492 - val_accuracy: 0.9373\n",
      "Epoch 51/100\n",
      "70/70 [==============================] - 13s 186ms/step - loss: 0.0439 - accuracy: 0.9838 - val_loss: 0.2144 - val_accuracy: 0.9507\n",
      "Epoch 52/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.0422 - accuracy: 0.9874 - val_loss: 0.1903 - val_accuracy: 0.9560\n",
      "Epoch 53/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.0352 - accuracy: 0.9865 - val_loss: 0.2465 - val_accuracy: 0.9480\n",
      "Epoch 54/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.0429 - accuracy: 0.9865 - val_loss: 0.2378 - val_accuracy: 0.9440\n",
      "Epoch 55/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.0318 - accuracy: 0.9901 - val_loss: 0.2120 - val_accuracy: 0.9573\n",
      "Epoch 56/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.0368 - accuracy: 0.9883 - val_loss: 0.2472 - val_accuracy: 0.9387\n",
      "Epoch 57/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.0227 - accuracy: 0.9932 - val_loss: 0.2214 - val_accuracy: 0.9547\n",
      "Epoch 58/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.0428 - accuracy: 0.9856 - val_loss: 0.1947 - val_accuracy: 0.9600\n",
      "Epoch 59/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.0324 - accuracy: 0.9901 - val_loss: 0.1901 - val_accuracy: 0.9640\n",
      "Epoch 60/100\n",
      "70/70 [==============================] - 13s 184ms/step - loss: 0.0283 - accuracy: 0.9896 - val_loss: 0.2045 - val_accuracy: 0.9587\n",
      "Epoch 61/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.0363 - accuracy: 0.9860 - val_loss: 0.2224 - val_accuracy: 0.9520\n",
      "Epoch 62/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.0315 - accuracy: 0.9892 - val_loss: 0.1983 - val_accuracy: 0.9653\n",
      "Epoch 63/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.0236 - accuracy: 0.9937 - val_loss: 0.2137 - val_accuracy: 0.9640\n",
      "Epoch 64/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.0457 - accuracy: 0.9851 - val_loss: 0.1864 - val_accuracy: 0.9627\n",
      "Epoch 65/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.0260 - accuracy: 0.9901 - val_loss: 0.2148 - val_accuracy: 0.9613\n",
      "Epoch 66/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.0282 - accuracy: 0.9914 - val_loss: 0.1861 - val_accuracy: 0.9627\n",
      "Epoch 67/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.0310 - accuracy: 0.9914 - val_loss: 0.2983 - val_accuracy: 0.9347\n",
      "Epoch 68/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.0243 - accuracy: 0.9914 - val_loss: 0.2144 - val_accuracy: 0.9573\n",
      "Epoch 69/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.0256 - accuracy: 0.9901 - val_loss: 0.2257 - val_accuracy: 0.9507\n",
      "Epoch 70/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.0356 - accuracy: 0.9883 - val_loss: 0.2059 - val_accuracy: 0.9560\n",
      "Epoch 71/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.0313 - accuracy: 0.9901 - val_loss: 0.2370 - val_accuracy: 0.9493\n",
      "Epoch 72/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.0250 - accuracy: 0.9923 - val_loss: 0.2295 - val_accuracy: 0.9547\n",
      "Epoch 73/100\n",
      "70/70 [==============================] - 13s 186ms/step - loss: 0.0221 - accuracy: 0.9937 - val_loss: 0.2447 - val_accuracy: 0.9533\n",
      "Epoch 74/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.0278 - accuracy: 0.9892 - val_loss: 0.2137 - val_accuracy: 0.9600\n",
      "Epoch 75/100\n",
      "70/70 [==============================] - 13s 186ms/step - loss: 0.0228 - accuracy: 0.9910 - val_loss: 0.2383 - val_accuracy: 0.9533\n",
      "Epoch 76/100\n",
      "70/70 [==============================] - 13s 184ms/step - loss: 0.0296 - accuracy: 0.9896 - val_loss: 0.2180 - val_accuracy: 0.9573\n",
      "Epoch 77/100\n",
      "70/70 [==============================] - 13s 187ms/step - loss: 0.0126 - accuracy: 0.9969 - val_loss: 0.2284 - val_accuracy: 0.9613\n",
      "Epoch 78/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.0292 - accuracy: 0.9910 - val_loss: 0.2210 - val_accuracy: 0.9547\n",
      "Epoch 79/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.0277 - accuracy: 0.9932 - val_loss: 0.2460 - val_accuracy: 0.9533\n",
      "Epoch 80/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.0233 - accuracy: 0.9928 - val_loss: 0.2077 - val_accuracy: 0.9613\n",
      "Epoch 81/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.0196 - accuracy: 0.9932 - val_loss: 0.2277 - val_accuracy: 0.9600\n",
      "Epoch 82/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.0296 - accuracy: 0.9883 - val_loss: 0.2320 - val_accuracy: 0.9613\n",
      "Epoch 83/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.0257 - accuracy: 0.9887 - val_loss: 0.2844 - val_accuracy: 0.9413\n",
      "Epoch 84/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.0194 - accuracy: 0.9941 - val_loss: 0.2504 - val_accuracy: 0.9493\n",
      "Epoch 85/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.0273 - accuracy: 0.9928 - val_loss: 0.2560 - val_accuracy: 0.9467\n",
      "Epoch 86/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.0185 - accuracy: 0.9946 - val_loss: 0.2577 - val_accuracy: 0.9520\n",
      "Epoch 87/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.0226 - accuracy: 0.9941 - val_loss: 0.2785 - val_accuracy: 0.9493\n",
      "Epoch 88/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.0161 - accuracy: 0.9964 - val_loss: 0.2663 - val_accuracy: 0.9547\n",
      "Epoch 89/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.0246 - accuracy: 0.9919 - val_loss: 0.2411 - val_accuracy: 0.9547\n",
      "Epoch 90/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.0205 - accuracy: 0.9919 - val_loss: 0.2326 - val_accuracy: 0.9613\n",
      "Epoch 91/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.0190 - accuracy: 0.9941 - val_loss: 0.2540 - val_accuracy: 0.9560\n",
      "Epoch 92/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.0195 - accuracy: 0.9919 - val_loss: 0.3052 - val_accuracy: 0.9413\n",
      "Epoch 93/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.0177 - accuracy: 0.9946 - val_loss: 0.2297 - val_accuracy: 0.9600\n",
      "Epoch 94/100\n",
      "70/70 [==============================] - 13s 184ms/step - loss: 0.0227 - accuracy: 0.9932 - val_loss: 0.2321 - val_accuracy: 0.9613\n",
      "Epoch 95/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.0207 - accuracy: 0.9937 - val_loss: 0.2367 - val_accuracy: 0.9573\n",
      "Epoch 96/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.0241 - accuracy: 0.9932 - val_loss: 0.2464 - val_accuracy: 0.9573\n",
      "Epoch 97/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.0210 - accuracy: 0.9928 - val_loss: 0.2272 - val_accuracy: 0.9600\n",
      "Epoch 98/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.0151 - accuracy: 0.9941 - val_loss: 0.2456 - val_accuracy: 0.9600\n",
      "Epoch 99/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.0183 - accuracy: 0.9964 - val_loss: 0.2390 - val_accuracy: 0.9613\n",
      "Epoch 100/100\n",
      "70/70 [==============================] - 13s 185ms/step - loss: 0.0173 - accuracy: 0.9941 - val_loss: 0.2118 - val_accuracy: 0.9640\n",
      "Evaluating after fine-tuning...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        cats       0.96      0.97      0.96       259\n",
      "        dogs       0.96      0.95      0.95       256\n",
      "       panda       0.97      0.98      0.98       235\n",
      "\n",
      "    accuracy                           0.96       750\n",
      "   macro avg       0.96      0.96      0.96       750\n",
      "weighted avg       0.96      0.96      0.96       750\n",
      "\n",
      "Saving the model to disk...\n",
      "INFO:tensorflow:Assets written to: animals.model\\assets\n"
     ]
    }
   ],
   "source": [
    "fineTune('../data/animals/images', 'animals.model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
